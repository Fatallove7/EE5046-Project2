import torch
from torch.utils.data import Dataset
import json
import os
import scipy.io as io
import numpy as np
from scipy.interpolate import interp1d
from transformers import AutoTokenizer
# å‡è®¾ Config.py ä¸­çš„å¸¸é‡å¯ä»¥è¢«å¯¼å…¥
from Config import FIXED_LENGTH, DOWNSAMPLE_RATE, AUGMENT_SETTING


# ------------------------------------------------------------------
# å…³é”®è¾…åŠ©å‡½æ•° (åŸºäº ECG_dataset.py é€»è¾‘)
# ------------------------------------------------------------------

def _get_ecg_data_raw(base_file, file_name):
    """æ ¹æ®æ–‡ä»¶åä» training2017 ç›®å½•åŠ è½½åŸå§‹ .mat ä¿¡å·"""
    # è·¯å¾„: base_file ('../Dataset') + '/training2017/' + file_name + '.mat'
    mat_file = os.path.join(base_file, 'training2017', file_name + '.mat')
    if not os.path.exists(mat_file):
        raise FileNotFoundError(f"ECG .mat file not found: {mat_file}")
    # io.loadmat(mat_file)['val'] è¿”å›çš„æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œæˆ‘ä»¬å–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºä¿¡å·æ•°æ®
    data_raw = io.loadmat(mat_file)['val']
    return data_raw[0]  # è¿”å›ä¸€ç»´çš„ä¿¡å·æ•°ç»„


# å¤åˆ¶ ECG_dataset ä¸­çš„æ•°æ®å¤„ç†é€»è¾‘ï¼Œä½œä¸ºç‹¬ç«‹çš„å‡½æ•°
def _add_noise(data):
    if np.random.rand() < 0.5:
        noise_level = 0.05
        noise = np.random.normal(0, noise_level, data.shape)
        data = data + noise
    return data


def _time_scaling(data):
    if np.random.rand() < 0.5:
        scale_factor = np.random.uniform(0.8, 1.2)
        old_len = data.shape[0]
        new_len = int(old_len * scale_factor)

        x_old = np.linspace(0, 1, old_len)
        x_new = np.linspace(0, 1, new_len)
        f = interp1d(x_old, data, kind='linear')
        data = f(x_new)
    return data


def _crop_padding(data, length, is_train, apply_augment):
    if data.shape[0] <= length:
        pad_len = length - data.shape[0]
        data = np.pad(data, (0, pad_len), 'constant')
    elif data.shape[0] > length:
        if is_train and apply_augment:
            # è®­ç»ƒæ—¶éšæœºè£å‰ª
            start = np.random.randint(0, data.shape[0] - length)
        else:
            # æµ‹è¯•æˆ–ä¸å¢å¼ºæ—¶ä¸­å¿ƒè£å‰ª
            start = (data.shape[0] - length) // 2
        data = data[start:start + length]
    return data


def _data_process_full(data_raw, is_train, apply_augment):
    """æ•´åˆæ‰€æœ‰æ•°æ®é¢„å¤„ç†å’Œå¢å¼ºæ­¥éª¤"""
    data = data_raw.copy()

    # 1. é™é‡‡æ ·
    data = data[::DOWNSAMPLE_RATE]

    # 2. æ—¶é—´ç¼©æ”¾ (å¦‚æœå¼€å¯)
    if is_train and apply_augment:
        data = _time_scaling(data)

    # 3. å½’ä¸€åŒ–
    data = data - data.mean()
    std = data.std()
    data = data / std

    # 4. æ·»åŠ å™ªå£° (å¦‚æœå¼€å¯)
    if is_train and apply_augment:
        data = _add_noise(data)

    # 5. è£å‰ª/å¡«å……åˆ°å›ºå®šé•¿åº¦
    data = _crop_padding(data, FIXED_LENGTH, is_train, apply_augment)

    # è½¬æ¢ä¸º Tensorï¼Œå¹¶æ·»åŠ é€šé“ç»´åº¦ [1, FIXED_LENGTH]
    return torch.tensor(data, dtype=torch.float32).unsqueeze(0)


# ------------------------------------------------------------------
# MultimodalDataset ç±»
# ------------------------------------------------------------------

class MultimodalDataset(Dataset):
    
    # ... (__init__ æ–¹æ³•å‚æ•°ä¿æŒä¸å˜) ...

    def __init__(self, json_path, data_dir, tokenizer, ecg_token="<ECG>", max_len=512, is_train=True,
                 augment=AUGMENT_SETTING):
        self.tokenizer = tokenizer
        self.data_dir = data_dir
        self.ecg_token = ecg_token
        self.max_len = max_len
        self.is_train = is_train
        self.augment = augment

        # ğŸ’¥ ä¿®æ­£ 1ï¼šç§»é™¤ add_tokens å—
        # å¯¹äº Qwenï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨å…¶å†…ç½®çš„ Token (å¦‚ <|extra_0|>)ï¼Œä¸èƒ½æ‰‹åŠ¨æ·»åŠ è¯æ±‡ã€‚
        
        self.ecg_token_id = self.tokenizer.convert_tokens_to_ids(ecg_token)

        if self.ecg_token_id == self.tokenizer.unk_token_id:
            print(f"è­¦å‘Š: ECG token '{ecg_token}' ä¸å­˜åœ¨äºè¯è¡¨ä¸­ï¼")
            # å°è¯•ä½¿ç”¨ <|im_start|> ä½œä¸ºå¤‡ç”¨
            backup_token = "<|im_start|>"
            self.ecg_token = backup_token
            self.ecg_token_id = self.tokenizer.convert_tokens_to_ids(backup_token)
            print(f"ä½¿ç”¨å¤‡ç”¨ token: {backup_token} (ID: {self.ecg_token_id})")
        
        print(f"ECG token: '{self.ecg_token}' (ID: {self.ecg_token_id})")

        with open(json_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)

        print(f"MultimodalDataset åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.metadata)} ä¸ªæ ·æœ¬ã€‚")

    def _find_answer_start(self, input_ids, tokenizer):
        """æ‰¾åˆ°'ç­”æ¡ˆ:'åœ¨input_idsä¸­çš„èµ·å§‹ä½ç½®"""
        # æ–¹æ³•1ï¼šæŸ¥æ‰¾ç‰¹å®šçš„tokenåºåˆ—
        # éœ€è¦çŸ¥é“"ç­”æ¡ˆ:"åœ¨Qwen tokenizerä¸­å¦‚ä½•ç¼–ç 
        answer_tokens = tokenizer.encode("ç­”æ¡ˆ:", add_special_tokens=False)
        
        # åœ¨input_idsä¸­æœç´¢è¿™ä¸ªåºåˆ—
        for i in range(len(input_ids) - len(answer_tokens) + 1):
            if all(input_ids[i+j] == answer_tokens[j] for j in range(len(answer_tokens))):
                return i + len(answer_tokens)  # è¿”å›"ç­”æ¡ˆ:"ä¹‹åçš„ä½ç½®
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›-1
        return -1

    def __len__(self):
        return len(self.metadata)

    def __getitem__(self, idx):
        item = self.metadata[idx]
        file_name = item['file_name']

        # -------------------- A. åŠ è½½å’Œå¤„ç† ECG æ•°æ® --------------------
        data_raw = _get_ecg_data_raw(self.data_dir, file_name)
        ecg_data = _data_process_full(
            data_raw,
            is_train=self.is_train,
            apply_augment=self.augment
        )

        # -------------------- B. Tokenize æ–‡æœ¬ --------------------
        full_text = item['full_text']
        final_input_text = f"{full_text}{self.tokenizer.eos_token}"

        # ä¿®æ”¹è¿™é‡Œï¼šå»æ‰ padding="max_length"ï¼Œåªåšæˆªæ–­
        tokenized = self.tokenizer(
            final_input_text,
            max_length=self.max_len,
            truncation=True,  # åªåšæˆªæ–­ï¼Œä¸åšå¡«å……
            return_tensors="pt",
            add_special_tokens=True
        )

        input_ids = tokenized['input_ids'].squeeze(0)  # [seq_len]
        attention_mask = tokenized['attention_mask'].squeeze(0)

        # -------------------- C. æ ‡ç­¾æ©ç  (Labels Masking) --------------------
        labels = input_ids.clone()
        
        ecg_positions = (input_ids == self.ecg_token_id).nonzero(as_tuple=True)[0]
        
        if len(ecg_positions) > 0:
            # æ­£å¸¸æƒ…å†µä¸‹åªæœ‰ä¸€ä¸ªECG token
            ecg_pos = ecg_positions[0].item()
            
            # æ‰¾åˆ°"ç­”æ¡ˆ:"çš„ä½ç½®ï¼ˆæ›´å¯é çš„æ–¹æ³•ï¼‰
            # åœ¨tokenizedåæŸ¥æ‰¾"ç­”æ¡ˆ"å¯¹åº”çš„token
            # å‡è®¾"ç­”æ¡ˆ:"å¯¹åº”çš„tokenåºåˆ—æ˜¯[token1, token2]
            answer_start_pos = self._find_answer_start(input_ids, self.tokenizer)
            
            if answer_start_pos > ecg_pos:
                # æ©ç ä»å¼€å§‹åˆ°"ç­”æ¡ˆ:"ä¹‹å‰çš„æ‰€æœ‰token
                labels[:answer_start_pos] = -100
            else:
                # å¦‚æœæ‰¾ä¸åˆ°"ç­”æ¡ˆ:"ï¼Œè‡³å°‘æ©ç åˆ°ECG tokenä¹‹å
                labels[:ecg_pos + 1] = -100
        else:
           # å¦‚æœæ²¡æœ‰ECG tokenï¼Œä½¿ç”¨fallbackæ–¹æ³•
            if "\nç­”æ¡ˆ:" in full_text:
                prompt_text = full_text.split("\nç­”æ¡ˆ:")[0] + "\nç­”æ¡ˆ:"
                prompt_ids = self.tokenizer.encode(prompt_text)
                labels[:len(prompt_ids)] = -100

        if idx == 0:  # åªæ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬
            print("\n=== è®­ç»ƒæ•°æ®æ ¼å¼éªŒè¯ ===")
            print(f"æ–‡ä»¶å: {file_name}")
            print(f"ECGæ•°æ®å½¢çŠ¶: {ecg_data.shape}")
            print(f"å®Œæ•´æ–‡æœ¬: {full_text}")
            print(f"\nTokenizedç»“æœ:")
            print(f"input_idsé•¿åº¦: {len(input_ids)}")
            print(f"input_ids: {input_ids.tolist()[:30]}...")
            print(f"\næ ‡ç­¾æ©ç :")
            print(f"labels: {labels.tolist()[:30]}...")
            
            # è§£ç æŸ¥çœ‹
            print(f"\nè§£ç input_ids:")
            decoded_input = self.tokenizer.decode(input_ids, skip_special_tokens=False)
            print(decoded_input[:200])
            
            print(f"\nè§£ç labelsï¼ˆ-100æ›¿æ¢ä¸º[IGN]ï¼‰:")
            labels_text = []
            for i, label in enumerate(labels):
                if label == -100:
                    labels_text.append("[IGN]")
                else:
                    labels_text.append(self.tokenizer.decode([label]))
            print(" ".join(labels_text[:50]))
            
            # æ£€æŸ¥å“ªäº›ä½ç½®è®¡ç®—æŸå¤±
            loss_positions = (labels != -100).nonzero(as_tuple=True)[0]
            print(f"\nè®¡ç®—æŸå¤±çš„ä½ç½®ï¼ˆå‰10ä¸ªï¼‰: {loss_positions[:10].tolist()}")
            print(f"è¿™äº›ä½ç½®çš„token: {[self.tokenizer.decode([input_ids[pos]]) for pos in loss_positions[:10]]}")
        
        return {
            "ecg_data": ecg_data,
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
        }