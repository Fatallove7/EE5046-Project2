from datetime import datetime
import json
import os
import tqdm

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    get_linear_schedule_with_warmup
)
from torch.amp import GradScaler, autocast

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from MultimodelLLM import MultimodalLLM
from MultimodelDataset import MultimodalDataset
from ECGEncoder import ECGEncoder
from Config import CNN_WEIGHTS_PATH, DATASET_PATH, FIXED_LENGTH, JSON_PATH


# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================
LLM_MODEL_NAME = "Qwen/Qwen-7B"
LLM_PATH = "/home/xusi/EE5046_Projects/LLM_Models/Qwen_Qwen-7B"
JSON_BEST_CONFIG = "/home/xusi/Logs/FinalTraining/Results_20251212_121215/final_results.json"

# è®­ç»ƒè¶…å‚æ•°
MAX_LEN = 512
BATCH_SIZE = 8
EPOCHS = 8
LEARNING_RATE = 2e-5
WARMUP_RATIO = 0.1
GRADIENT_CLIP_NORM = 1.0
LOG_INTERVAL = 100  # æ—¥å¿—è®°å½•é—´éš”ï¼ˆæ­¥æ•°ï¼‰

# Qwen ç‰¹æ®Š token IDï¼ˆç¡¬ç¼–ç ï¼‰
QWEN_SPECIAL_TOKENS = {
    '<|endoftext|>': 151643,
    '<|im_start|>': 151644,
    '<|im_end|>': 151645,
    '<|extra_0|>': 151646,
    '<|extra_1|>': 151647,
    '<|extra_2|>': 151648
}


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================
def load_best_cnn_config(json_path):
    """ä»è®­ç»ƒç»“æœ JSON æ–‡ä»¶ä¸­è¯»å–æœ€ä½³çš„ CNN é…ç½®"""
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è®­ç»ƒç»“æœ JSON æ–‡ä»¶: {json_path}")

    with open(json_path, 'r', encoding='utf-8') as f:
        results = json.load(f)

    # æå– JSON ä¸­çš„ kernel_config éƒ¨åˆ†
    kernel_config = results.get("best_config", {}).get("kernel_config", {})

    if not kernel_config:
        raise ValueError("JSON æ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'best_config' æˆ– 'kernel_config' ç»“æ„ã€‚")

    # æ ¹æ® Mscnn çš„æ„é€ å‡½æ•°éœ€æ±‚ï¼Œæ„å»º CNN_CONFIG å­—å…¸
    cnn_config = {
        'ch_in': 1,               # é»˜è®¤å€¼ï¼šå•å¯¼è” ECG
        'ch_out': 1,              # é»˜è®¤å€¼ï¼šè¾“å‡ºé€šé“æ•°
        'use_stream2': True,      # é»˜è®¤å€¼ï¼šåªè¦ stream2_first_kernel å­˜åœ¨ï¼Œé€šå¸¸ä¸º True
        'stream1_kernel': kernel_config.get("stream1_kernel"),
        'stream2_first_kernel': kernel_config.get("stream2_first_kernel"),
    }
    
    # æ£€æŸ¥å…³é”®å‚æ•°æ˜¯å¦æˆåŠŸè¯»å–
    if cnn_config['stream1_kernel'] is None or cnn_config['stream2_first_kernel'] is None:
        raise ValueError("æ— æ³•ä» JSON ä¸­æå– stream1_kernel æˆ– stream2_first_kernelã€‚")

    return cnn_config


def calculate_flat_dim(cnn_config, fixed_length, ECGEncoder_class, cnn_weights_path):
    """é€šè¿‡æ¨¡æ‹Ÿå‰å‘ä¼ æ’­æ¥è®¡ç®— ECGEncoder çš„è¾“å‡ºç»´åº¦"""
    model = ECGEncoder_class(cnn_config, cnn_weights_path) 
    
    dummy_input = torch.randn(1, 1, fixed_length) 
    model.eval()
    with torch.no_grad():
        output_tensor = model(dummy_input)
    
    return output_tensor.size(-1)


def create_model_save_dir(base_path='../Trained_Multimodal_Models', experiment_name=None):
    """
    åˆ›å»ºæœ‰æ„ä¹‰çš„æ¨¡å‹ä¿å­˜ç›®å½•
    
    Args:
        base_path: åŸºç¡€ä¿å­˜è·¯å¾„
        experiment_name: å®éªŒåç§°ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
    
    Returns:
        str: æ–°åˆ›å»ºçš„æ¨¡å‹ä¿å­˜ç›®å½•çš„ç»å¯¹è·¯å¾„
    """
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    absolute_base_path = os.path.abspath(os.path.join(current_script_dir, base_path))
    
    # ç¡®ä¿åŸºç¡€ç›®å½•å­˜åœ¨
    os.makedirs(absolute_base_path, exist_ok=True)
    
    # ç”Ÿæˆæœ‰æ„ä¹‰çš„å®éªŒåç§°
    if experiment_name is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        experiment_name = f"Qwen7B_ECG_B{BATCH_SIZE}_LR{LEARNING_RATE}_E{EPOCHS}_{timestamp}"
    
    save_dir = os.path.join(absolute_base_path, experiment_name)
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"æ¨¡å‹å°†ä¿å­˜åˆ°: {save_dir}")
    return save_dir


def monitor_gpu_memory():
    """ç›‘æ§ GPU å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
        gpu_memory_max = torch.cuda.max_memory_allocated() / 1024**3  # GB
        return gpu_memory, gpu_memory_max
    return None, None


def multimodal_collate_fn(batch, tokenizer):
    """
    è‡ªå®šä¹‰çš„ collate_fn ç”¨äºå¤šæ¨¡æ€æ•°æ®é›†
    å¤„ç†å˜é•¿åºåˆ—çš„å¡«å……
    """
    # ECG æ•°æ®æ˜¯å›ºå®šé•¿åº¦çš„ï¼Œç›´æ¥å †å 
    ecg_data = torch.stack([item['ecg_data'] for item in batch])
    
    # æ–‡æœ¬æ•°æ®éœ€è¦å¡«å……
    input_ids_list = [item['input_ids'] for item in batch]
    attention_mask_list = [item['attention_mask'] for item in batch]
    labels_list = [item['labels'] for item in batch]
    
    # æ‰¾åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—é•¿åº¦
    max_len = max(len(ids) for ids in input_ids_list)
    
    # åˆå§‹åŒ–å¡«å……åçš„å¼ é‡
    batch_size = len(batch)
    padded_input_ids = torch.full(
        (batch_size, max_len), 
        tokenizer.pad_token_id, 
        dtype=torch.long
    )
    padded_attention_mask = torch.zeros((batch_size, max_len), dtype=torch.long)
    padded_labels = torch.full((batch_size, max_len), -100, dtype=torch.long)
    
    # å¡«å……æ¯ä¸ªåºåˆ—
    for i, (ids, mask, lbl) in enumerate(zip(input_ids_list, attention_mask_list, labels_list)):
        seq_len = len(ids)
        padded_input_ids[i, :seq_len] = ids
        padded_attention_mask[i, :seq_len] = mask
        padded_labels[i, :seq_len] = lbl
    
    return {
        'ecg_data': ecg_data,
        'input_ids': padded_input_ids,
        'attention_mask': padded_attention_mask,
        'labels': padded_labels
    }


def setup_qwen_tokenizer(llm_path):
    """ä¸“é—¨ä¸º Qwen è®¾ç½® tokenizerï¼Œç¡®ä¿ pad_token æ­£ç¡®é…ç½®"""
    tokenizer = AutoTokenizer.from_pretrained(
        llm_path, 
        trust_remote_code=True,
        padding_side='right'
    )
    
    print("åŸå§‹ Qwen tokenizer é…ç½®:")
    print(f"  eos_token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    print(f"  bos_token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")
    print(f"  pad_token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    
    # è®¾ç½® pad_token
    if tokenizer.pad_token is None:
        if tokenizer.eos_token is not None and tokenizer.eos_token_id is not None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
            print(f"è®¾ç½® pad_token ä¸º eos_token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
        else:
            # ä½¿ç”¨ <|endoftext|> ä½œä¸ºé»˜è®¤ pad_token
            tokenizer.pad_token = '<|endoftext|>'
            tokenizer.pad_token_id = QWEN_SPECIAL_TOKENS['<|endoftext|>']
            print(f"è®¾ç½® pad_token ä¸º '<|endoftext|>': (ID: {tokenizer.pad_token_id})")
    
    # ç¡®ä¿ pad_token_id ä¸ä¸º None
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = '<|endoftext|>'
        tokenizer.pad_token_id = QWEN_SPECIAL_TOKENS['<|endoftext|>']
        print(f"å¼ºåˆ¶è®¾ç½® pad_token ä¸º '<|endoftext|>': (ID: {tokenizer.pad_token_id})")
    
    print(f"\næœ€ç»ˆ tokenizer é…ç½®:")
    print(f"  pad_token: {tokenizer.pad_token}")
    print(f"  pad_token_id: {tokenizer.pad_token_id}")
    
    return tokenizer


def validate_ecg_token(tokenizer, ecg_token="<|extra_0|>"):
    """éªŒè¯ ECG token æ˜¯å¦å­˜åœ¨"""
    ecg_token_id = tokenizer.convert_tokens_to_ids(ecg_token)
    
    print(f"ECG token éªŒè¯:")
    print(f"  ECG token: {ecg_token}")
    print(f"  ECG token ID: {ecg_token_id}")
    
    if ecg_token_id == tokenizer.unk_token_id:
        raise ValueError(f"ECG token {ecg_token} ä¸å­˜åœ¨äº Qwen è¯è¡¨ä¸­ï¼")
    
    print(f"âœ… ECG token {ecg_token} (ID: {ecg_token_id}) éªŒè¯é€šè¿‡ã€‚")
    
    return ecg_token_id


def log_model_parameters_to_tensorboard(model, writer, global_step):
    """è®°å½•æ¨¡å‹å‚æ•°åˆ°TensorBoard"""
    for name, param in model.named_parameters():
        if param.requires_grad and param.grad is not None:
            # è®°å½•å‚æ•°å€¼åˆ†å¸ƒ
            writer.add_histogram(f'parameters/{name}', param.data.cpu().numpy(), global_step)
            # è®°å½•æ¢¯åº¦åˆ†å¸ƒ
            writer.add_histogram(f'gradients/{name}', param.grad.cpu().numpy(), global_step)
            
            # è®°å½•å‚æ•°èŒƒæ•°
            writer.add_scalar(f'norm/parameters/{name}', param.norm().item(), global_step)
            writer.add_scalar(f'norm/gradients/{name}', param.grad.norm().item(), global_step)


def save_model_with_metadata(model, save_dir, epoch, loss, config=None, is_best=False):
    """
    ä¿å­˜æ¨¡å‹åŠç›¸å…³å…ƒæ•°æ®
    
    Args:
        model: è¦ä¿å­˜çš„æ¨¡å‹
        save_dir: ä¿å­˜ç›®å½•
        epoch: å½“å‰epoch
        loss: å½“å‰æŸå¤±
        config: è®­ç»ƒé…ç½®
        is_best: æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
    """
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹æƒé‡
    model.llm.save_pretrained(os.path.join(save_dir, "lora_adapter"))
    torch.save(model.projector.state_dict(), os.path.join(save_dir, "projector.pth"))
    
    # åˆ›å»ºæ¨¡å‹å¡ä¿¡æ¯
    model_card = {
        "model_name": "ECG-Qwen-LLM",
        "model_type": "multimodal_language_model",
        "task": "ecg_classification",
        "framework": "pytorch",
        "base_model": "Qwen-7B",
        "fine_tuning_method": "lora",
        "ecg_encoder": "Mscnn",
        "training_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "last_epoch": epoch,
        "loss": loss,
        "is_best_model": is_best,
        "hyperparameters": {
            "learning_rate": LEARNING_RATE,
            "batch_size": BATCH_SIZE,
            "epochs": EPOCHS,
            "max_length": MAX_LEN,
            "warmup_ratio": WARMUP_RATIO
        }
    }
    
    if config:
        model_card["model_config"] = config
    
    # ä¿å­˜æ¨¡å‹å¡
    with open(os.path.join(save_dir, "model_card.json"), "w", encoding='utf-8') as f:
        json.dump(model_card, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜è®­ç»ƒçŠ¶æ€
    training_state = {
        "epoch": epoch,
        "loss": loss,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    with open(os.path.join(save_dir, "training_state.json"), "w", encoding='utf-8') as f:
        json.dump(training_state, f, indent=2)
    
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}")


def save_training_report(save_dir, best_loss, final_loss, total_epochs, training_time=None):
    """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
    if training_time is None:
        training_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    report = f"""
    ========================================
    ECG-Qwen-LLM è®­ç»ƒæŠ¥å‘Š
    ========================================
    
    è®­ç»ƒå®Œæˆæ—¶é—´: {training_time}
    
    è®­ç»ƒç»Ÿè®¡:
    - æ€»epochæ•°: {total_epochs}
    - æœ€ä½³è®­ç»ƒæŸå¤±: {best_loss:.4f}
    - æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_loss:.4f}
    - æ€§èƒ½æå‡: {((final_loss - best_loss) / best_loss * 100):+.2f}% (æœ€ä½³ vs æœ€ç»ˆ)
    
    æ¨¡å‹ä¿¡æ¯:
    - åŸºç¡€æ¨¡å‹: Qwen-7B
    - å¾®è°ƒæ–¹æ³•: LoRA
    - ECGç¼–ç å™¨: Mscnn
    - ä»»åŠ¡: ECGä¿¡å·åˆ†ç±»
    
    è¶…å‚æ•°:
    - å­¦ä¹ ç‡: {LEARNING_RATE}
    - æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}
    - æœ€å¤§åºåˆ—é•¿åº¦: {MAX_LEN}
    - Warmupæ¯”ä¾‹: {WARMUP_RATIO}
    - æ¢¯åº¦è£å‰ª: {GRADIENT_CLIP_NORM}
    
    ========================================
    """
    
    report_path = os.path.join(save_dir, "training_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“‹ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    return report_path


# ============================================================================
# è®­ç»ƒå‡½æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ - åªä¿å­˜æœ€ä½³æ¨¡å‹ï¼‰
# ============================================================================
def train_multimodal_model(cnn_config, json_path, data_dir, model_save_path, experiment_name=None):
    """
    å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒä¸»å‡½æ•°
    - æ¯ä¸ªepochè®­ç»ƒå®Œæˆåï¼Œå¦‚æœæ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œåˆ™ä¿å­˜ä¸ºæœ€ä½³æ¨¡å‹
    - è®­ç»ƒç»“æŸåï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
    - ä¸ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹ï¼Œåªä¿ç•™æœ€ä½³å’Œæœ€ç»ˆæ¨¡å‹
    """
    # ------------------ 1. åˆå§‹åŒ– ------------------
    start_time = datetime.now()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"è®­ç»ƒè®¾å¤‡: {device}")
    
    # ------------------ 2. åˆ›å»ºTensorBoardå†™å…¥å™¨ ------------------
    tb_log_dir = os.path.join(model_save_path, "tensorboard_logs")
    os.makedirs(tb_log_dir, exist_ok=True)
    writer = SummaryWriter(log_dir=tb_log_dir)
    print(f"TensorBoardæ—¥å¿—ç›®å½•: {tb_log_dir}")
    
    # ------------------ 3. Tokenizer é…ç½® ------------------
    tokenizer = setup_qwen_tokenizer(LLM_PATH)
    ECG_TOKEN = "<|extra_0|>"
    ecg_token_id = validate_ecg_token(tokenizer, ECG_TOKEN)
    
    # ------------------ 4. æ•°æ®å‡†å¤‡ ------------------
    print("\næ­£åœ¨åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨...")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = MultimodalDataset(
        json_path=json_path,
        data_dir=data_dir,
        tokenizer=tokenizer,
        ecg_token=ECG_TOKEN,
        max_len=MAX_LEN,
        is_train=True
    )
    
    # åˆ›å»º DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        collate_fn=lambda batch: multimodal_collate_fn(batch, tokenizer)
    )
    
    print(f"æ•°æ®é›†å¤§å°: {len(train_dataset)}")
    print(f"DataLoader æ‰¹æ¬¡æ•°é‡: {len(train_loader)}")
    
    # æµ‹è¯•æ‰¹æ¬¡æ•°æ®
    test_batch = next(iter(train_loader))
    print(f"\næµ‹è¯•æ‰¹æ¬¡å½¢çŠ¶:")
    print(f"  ecg_data: {test_batch['ecg_data'].shape}")
    print(f"  input_ids: {test_batch['input_ids'].shape}")
    print(f"  attention_mask: {test_batch['attention_mask'].shape}")
    print(f"  labels: {test_batch['labels'].shape}")
    
    # ------------------ 5. æ¨¡å‹åˆå§‹åŒ– ------------------
    print("\næ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    
    # è®¡ç®— FLAT_DIM å’Œ LLM_EMBEDDING_DIM
    FLAT_DIM = calculate_flat_dim(
        cnn_config, 
        FIXED_LENGTH, 
        ECGEncoder_class=ECGEncoder,
        cnn_weights_path=CNN_WEIGHTS_PATH
    )
    LLM_EMBEDDING_DIM = 4096  # Qwen-7B çš„åµŒå…¥ç»´åº¦
    
    model = MultimodalLLM(
        llm_path=LLM_PATH,
        cnn_config=cnn_config,
        cnn_weights_path=CNN_WEIGHTS_PATH,
        ecg_token_id=ecg_token_id,
        flat_dim=FLAT_DIM,
        llm_embed_dim=LLM_EMBEDDING_DIM,
        device=device
    )
    
    # è®°å½•è¶…å‚æ•°åˆ°TensorBoard
    hparams = {
        "learning_rate": LEARNING_RATE,
        "batch_size": BATCH_SIZE,
        "epochs": EPOCHS,
        "max_len": MAX_LEN,
        "warmup_ratio": WARMUP_RATIO,
        "gradient_clip_norm": GRADIENT_CLIP_NORM,
        "flat_dim": FLAT_DIM,
        "llm_embed_dim": LLM_EMBEDDING_DIM,
        "cnn_stream1_kernel": cnn_config.get('stream1_kernel'),
        "cnn_stream2_first_kernel": cnn_config.get('stream2_first_kernel'),
    }
    writer.add_hparams(hparams, {})
    
    # ------------------ 6. ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ------------------
    print("\næ­£åœ¨é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨...")
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    
    total_steps = len(train_loader) * EPOCHS
    warmup_steps = int(total_steps * WARMUP_RATIO)
    
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    print(f"æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
    print(f"Warmup æ­¥æ•°: {warmup_steps}")
    print(f"åˆå§‹å­¦ä¹ ç‡: {LEARNING_RATE}")
    
    # ------------------ 7. ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒ ------------------
    # ç”±äº Qwen æ¨¡å‹æœ‰æ•°æ®ç±»å‹é—®é¢˜ï¼Œæš‚æ—¶ç¦ç”¨æ··åˆç²¾åº¦
    print("æ··åˆç²¾åº¦è®­ç»ƒå·²ç¦ç”¨")
    scaler = None
    
    # è®­ç»ƒç»Ÿè®¡
    global_step = 0
    best_loss = float('inf')
    best_epoch = 0
    
    # ------------------ 8. è®­ç»ƒå¾ªç¯ ------------------
    print("\n" + "="*50)
    print("å¼€å§‹å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒ...")
    print("="*50)
    
    for epoch in range(EPOCHS):
        model.train()
        epoch_loss = 0
        epoch_steps = 0
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = tqdm.tqdm(
            enumerate(train_loader), 
            total=len(train_loader), 
            desc=f"Epoch {epoch+1}/{EPOCHS}"
        )
        
        for step, batch in progress_bar:
            # å‡†å¤‡æ•°æ®
            ecg_data = batch['ecg_data'].to(device)
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(
                ecg_data=ecg_data,
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            loss = outputs.loss
            
            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_NORM)
            optimizer.step()
            optimizer.zero_grad()
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            # æ›´æ–°ç»Ÿè®¡
            epoch_loss += loss.item()
            epoch_steps += 1
            global_step += 1
            avg_loss = epoch_loss / epoch_steps
            current_lr = scheduler.get_last_lr()[0]
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'avg_loss': f"{avg_loss:.4f}",
                'lr': f"{current_lr:.2e}"
            })
            
            # è®°å½•åˆ°TensorBoard
            if global_step % LOG_INTERVAL == 0:
                # è®°å½•æŸå¤±å’Œå­¦ä¹ ç‡
                writer.add_scalar('train/loss_step', loss.item(), global_step)
                writer.add_scalar('train/loss_avg', avg_loss, global_step)
                writer.add_scalar('train/learning_rate', current_lr, global_step)
                
                # è®°å½•æ¢¯åº¦èŒƒæ•°
                total_grad_norm = 0
                for p in model.parameters():
                    if p.grad is not None:
                        param_grad_norm = p.grad.data.norm(2).item()
                        total_grad_norm += param_grad_norm ** 2
                total_grad_norm = total_grad_norm ** 0.5
                writer.add_scalar('train/grad_norm', total_grad_norm, global_step)
            
            # ç›‘æ§ GPU å†…å­˜
            if step % 100 == 0 and device.type == 'cuda':
                gpu_memory, gpu_memory_max = monitor_gpu_memory()
                if gpu_memory:
                    # è®°å½•GPUå†…å­˜ä½¿ç”¨
                    writer.add_scalar('system/gpu_memory', gpu_memory, global_step)
                    writer.add_scalar('system/gpu_memory_max', gpu_memory_max, global_step)
        
        # ------------------ 9. æ¯ä¸ªepochç»“æŸ ------------------
        epoch_avg_loss = epoch_loss / len(train_loader)
        print(f"\nEpoch {epoch+1} ç»“æŸ, å¹³å‡è®­ç»ƒæŸå¤±: {epoch_avg_loss:.4f}")
        
        # è®°å½•epochçº§åˆ«çš„æŒ‡æ ‡
        writer.add_scalar('train/loss_epoch', epoch_avg_loss, epoch+1)
        writer.add_scalar('train/learning_rate_epoch', scheduler.get_last_lr()[0], epoch+1)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        if epoch_avg_loss < best_loss:
            best_loss = epoch_avg_loss
            best_epoch = epoch + 1
            print(f"ğŸ‰ å‘ç°æ–°çš„æœ€ä½³æ¨¡å‹! Epoch: {best_epoch}, æŸå¤±: {best_loss:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            best_model_dir = os.path.join(model_save_path, "best_model")
            save_model_with_metadata(
                model=model,
                save_dir=best_model_dir,
                epoch=best_epoch,
                loss=best_loss,
                config={
                    "cnn_config": cnn_config,
                    "flat_dim": FLAT_DIM,
                    "llm_embed_dim": LLM_EMBEDDING_DIM
                },
                is_best=True
            )
    
    # ------------------ 10. è®­ç»ƒå®Œæˆ ------------------
    end_time = datetime.now()
    training_duration = end_time - start_time
    
    print("\n" + "="*50)
    print("è®­ç»ƒå®Œæˆ!")
    print(f"è®­ç»ƒæ—¶é•¿: {training_duration}")
    print(f"æœ€ä½³æ¨¡å‹: Epoch {best_epoch}, æŸå¤±: {best_loss:.4f}")
    print(f"æœ€ç»ˆæ¨¡å‹: Epoch {EPOCHS}, æŸå¤±: {epoch_avg_loss:.4f}")
    print("="*50)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_dir = os.path.join(model_save_path, "final_model")
    save_model_with_metadata(
        model=model,
        save_dir=final_model_dir,
        epoch=EPOCHS,
        loss=epoch_avg_loss,
        config={
            "cnn_config": cnn_config,
            "flat_dim": FLAT_DIM,
            "llm_embed_dim": LLM_EMBEDDING_DIM,
            "final_epoch": EPOCHS,
            "final_loss": epoch_avg_loss
        },
        is_best=False
    )
    
    # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
    save_training_report(
        save_dir=model_save_path,
        best_loss=best_loss,
        final_loss=epoch_avg_loss,
        total_epochs=EPOCHS,
        training_time=end_time.strftime('%Y-%m-%d %H:%M:%S')
    )
    
    # å…³é—­TensorBoard writer
    writer.close()
    
    print(f"\nğŸ“ è®­ç»ƒç»“æœä¿å­˜åœ¨: {model_save_path}")
    print(f"ğŸ† æœ€ä½³æ¨¡å‹: {os.path.join(model_save_path, 'best_model')}")
    print(f"âœ… æœ€ç»ˆæ¨¡å‹: {os.path.join(model_save_path, 'final_model')}")
    
    # æç¤ºå¦‚ä½•å¯åŠ¨TensorBoard
    print(f"\nè¦å¯åŠ¨TensorBoardæŸ¥çœ‹è®­ç»ƒæ›²çº¿ï¼Œè¯·è¿è¡Œ:")
    print(f"tensorboard --logdir={tb_log_dir} --port=6006")
    print("ç„¶ååœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: http://localhost:6006")
    
    return {
        "best_loss": best_loss,
        "final_loss": epoch_avg_loss,
        "best_epoch": best_epoch,
        "training_duration": str(training_duration),
        "model_save_path": model_save_path,
        "best_model_path": os.path.join(model_save_path, "best_model"),
        "final_model_path": os.path.join(model_save_path, "final_model")
    }


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("="*50)
    print("å¤šæ¨¡æ€ ECG-LLM æ¨¡å‹è®­ç»ƒï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    print("="*50)
    
    # 1. åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆä½¿ç”¨æœ‰æ„ä¹‰çš„å®éªŒåç§°ï¼‰
    EXPERIMENT_NAME = f"Qwen7B_ECG_B{BATCH_SIZE}_LR{LEARNING_RATE}_E{EPOCHS}"
    MODEL_SAVE_DIR = create_model_save_dir(
        base_path='../Trained_Multimodal_Models',
        experiment_name=EXPERIMENT_NAME
    )
    
    # 2. åŠ è½½æœ€ä½³ CNN é…ç½®
    print("\næ­£åœ¨åŠ è½½ CNN é…ç½®...")
    try:
        CNN_CONFIG = load_best_cnn_config(JSON_BEST_CONFIG)
        print(f"âœ… æˆåŠŸä» JSON æ–‡ä»¶åŠ è½½æœ€ä½³ CNN é…ç½®")
        print(f"  é…ç½®å†…å®¹: {CNN_CONFIG}")
    except Exception as e:
        print(f"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}")
        return
    
    # 3. å¼€å§‹è®­ç»ƒ
    print("\nå¼€å§‹è®­ç»ƒè¿‡ç¨‹...")
    training_results = train_multimodal_model(
        cnn_config=CNN_CONFIG,
        json_path=JSON_PATH,
        data_dir=DATASET_PATH,
        model_save_path=MODEL_SAVE_DIR
    )
    
    # 4. æ‰“å°æœ€ç»ˆç»“æœ
    print("\n" + "="*50)
    print("è®­ç»ƒæ€»ç»“:")
    print(f"  å®éªŒåç§°: {EXPERIMENT_NAME}")
    print(f"  æœ€ä½³Epoch: {training_results['best_epoch']}")
    print(f"  æœ€ä½³æŸå¤±: {training_results['best_loss']:.4f}")
    print(f"  æœ€ç»ˆæŸå¤±: {training_results['final_loss']:.4f}")
    print(f"  è®­ç»ƒæ—¶é•¿: {training_results['training_duration']}")
    print(f"  ä¿å­˜è·¯å¾„: {training_results['model_save_path']}")
    print("="*50)
    
    print("\nâœ… è®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæˆï¼")


if __name__ == '__main__':
    main()