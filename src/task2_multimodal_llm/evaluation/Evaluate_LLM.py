from peft import PeftModel
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import json
import os
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer
import scipy.io as sio
from datetime import datetime
from pathlib import Path
import re

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ï¼ˆæ ¹æ®æ‚¨çš„å®é™…è·¯å¾„è°ƒæ•´ï¼‰
from src.task2_multimodal_llm.models.MultimodelLLM import MultimodalLLM
from EE5046_Projects.src.task2_multimodal_llm.models.ECGEncoder import ECGEncoder
from src.common.Config import CNN_WEIGHTS_PATH, DATASET_PATH, FIXED_LENGTH

# ä¿®æ”¹åçš„æ ‡ç­¾æ˜ å°„å­—å…¸
# æ ¹æ®CSVæ–‡ä»¶æ ¼å¼ï¼šAè¡¨ç¤ºæˆ¿é¢¤ï¼Œå…¶ä»–(0, N, O, ~ç­‰)è¡¨ç¤ºéæˆ¿é¢¤
LABEL_MAPPING = {
    "A": 1,  # æˆ¿é¢¤
    "N": 0,  # å™ªå£°ï¼ˆè§†ä¸ºæ­£å¸¸ï¼‰
    "O": 0,  # å…¶ä»–ï¼ˆè§†ä¸ºæ­£å¸¸ï¼‰
    "~": 0,  # æ— æ³•åˆ†ç±»ï¼ˆè§†ä¸ºæ­£å¸¸ï¼‰
}

class ResultSaver:
    """ç»Ÿä¸€çš„è¯„ä¼°ç»“æœä¿å­˜å™¨"""
    
    def __init__(self, base_dir="Evaluation_Results", experiment_name=None):
        """
        åˆå§‹åŒ–ç»“æœä¿å­˜å™¨
        
        Args:
            base_dir: åŸºç¡€ç›®å½•
            experiment_name: å®éªŒåç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ—¶é—´æˆ³
        """
        self.base_dir = os.path.abspath(base_dir)
        self.experiment_name = experiment_name or f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self.run_dir = self._create_directory_structure()
        
        # è®¾ç½®æ—¥å¿—è·¯å¾„
        self._log_path = os.path.join(self.run_dir, "logs", "evaluation.log")
        
        # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
        self._init_log_file()
        
        print(f"âœ“ ResultSaveråˆå§‹åŒ–å®Œæˆ")
        print(f"  å®éªŒç›®å½•: {self.run_dir}")
        print(f"  æ—¥å¿—æ–‡ä»¶: {self._log_path}")
    
    def _create_directory_structure(self):
        """åˆ›å»ºæ ‡å‡†åŒ–çš„ç›®å½•ç»“æ„"""
        run_dir = os.path.join(self.base_dir, "runs", self.experiment_name)
        
        # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„å­ç›®å½•
        directories = [
            os.path.join(run_dir, "results"),
            os.path.join(run_dir, "logs"),
            os.path.join(run_dir, "models"),
            os.path.join(run_dir, "visualizations"),
            os.path.join(run_dir, "responses"),
            os.path.join(run_dir, "confusion_matrices"),
            os.path.join(run_dir, "detailed_results")
        ]
        
        for dir_path in directories:
            os.makedirs(dir_path, exist_ok=True)
            
        return run_dir
    
    def _init_log_file(self):
        """åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶"""
        try:
            with open(self._log_path, "w", encoding="utf-8") as f:
                f.write(f"ECGè¯„ä¼°å®éªŒæ—¥å¿— - {self.experiment_name}\n")
                f.write(f"åˆ›å»ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("="*80 + "\n\n")
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºæ—¥å¿—æ–‡ä»¶ {self._log_path}: {e}")
            # ä½¿ç”¨å¤‡ç”¨æ—¥å¿—è·¯å¾„
            self._log_path = os.path.join(os.getcwd(), "evaluation_fallback.log")
            print(f"ä½¿ç”¨å¤‡ç”¨æ—¥å¿—è·¯å¾„: {self._log_path}")
    
    def log(self, message, level="INFO"):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] [{level}] {message}"
        
        try:
            # ç¡®ä¿æ—¥å¿—æ–‡ä»¶å­˜åœ¨
            log_dir = os.path.dirname(self._log_path)
            if not os.path.exists(log_dir):
                os.makedirs(log_dir, exist_ok=True)
            
            with open(self._log_path, "a", encoding="utf-8") as f:
                f.write(log_message + "\n")
        except Exception as e:
            # å¦‚æœå†™å…¥å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨è·¯å¾„
            print(f"æ— æ³•å†™å…¥æ—¥å¿—æ–‡ä»¶ {self._log_path}: {e}")
            # å°è¯•ç›´æ¥æ‰“å°
            print(f"æ—¥å¿—å†…å®¹: {log_message}")
            
            # åˆ›å»ºå¤‡ç”¨æ—¥å¿—
            backup_log = os.path.join(os.getcwd(), "evaluation_error.log")
            try:
                with open(backup_log, "a", encoding="utf-8") as f:
                    f.write(f"[{timestamp}] [ERROR] æ— æ³•å†™å…¥ä¸»æ—¥å¿—: {e}\n")
                    f.write(f"åŸå§‹æ—¥å¿—å†…å®¹: {log_message}\n")
            except:
                pass
        
        # æ§åˆ¶å°è¾“å‡º
        if level == "INFO":
            print(message)
        elif level == "WARNING":
            print(f"è­¦å‘Š: {message}")
        elif level == "ERROR":
            print(f"é”™è¯¯: {message}")
    
    def save_json(self, data, filepath):
        """ä¿å­˜JSONæ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            self.log(f"JSONæ–‡ä»¶å·²ä¿å­˜: {filepath}")
            return True
        except Exception as e:
            self.log(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥ {filepath}: {e}", level="ERROR")
            return False
    
    def save_evaluation_results(self, results, dataset_name, save_detailed=True):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ
        
        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
            dataset_name: æ•°æ®é›†åç§°ï¼ˆå¦‚cv0, cv1ç­‰ï¼‰
            save_detailed: æ˜¯å¦ä¿å­˜è¯¦ç»†ç»“æœ
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # 1. ä¿å­˜ä¸»ç»“æœæ–‡ä»¶
            main_result_path = os.path.join(self.run_dir, "results", f"evaluation_summary_{dataset_name}.json")
            self.save_json(results, main_result_path)
            
            # 2. ä¿å­˜è¯¦ç»†ç»“æœï¼ˆCSVæ ¼å¼ï¼‰
            if save_detailed and 'detailed_results' in results:
                detailed_csv_path = os.path.join(self.run_dir, "detailed_results", f"detailed_results_{dataset_name}.csv")
                df = pd.DataFrame(results['detailed_results'])
                df.to_csv(detailed_csv_path, index=False, encoding='utf-8-sig')
                self.log(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {detailed_csv_path}")
            
            # 3. ä¿å­˜æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
            metrics = {
                'dataset': dataset_name,
                'timestamp': timestamp,
                'metrics': {
                    'accuracy': results.get('accuracy', 0),
                    'precision': results.get('precision', 0),
                    'recall': results.get('recall', 0),
                    'f1': results.get('f1', 0)
                },
                'sample_stats': {
                    'total_samples': results.get('total_samples', 0),
                    'valid_samples': results.get('valid_samples', 0),
                    'invalid_samples': results.get('invalid_samples', 0),
                    'invalid_rate': results.get('invalid_rate', 0)
                },
                'confusion_matrix': results.get('confusion_matrix', []),
                'classification_report': results.get('classification_report', '')
            }
            
            metrics_path = os.path.join(self.run_dir, "results", f"metrics_{dataset_name}.json")
            self.save_json(metrics, metrics_path)
            
            # 4. ä¿å­˜å“åº”ç¤ºä¾‹
            if 'response_examples' in results:
                examples_path = os.path.join(self.run_dir, "responses", f"response_examples_{dataset_name}.json")
                self.save_json(results['response_examples'], examples_path)
            
            # 5. ä¿å­˜æ··æ·†çŸ©é˜µ
            if 'confusion_matrix' in results:
                cm_path = os.path.join(self.run_dir, "confusion_matrices", f"confusion_matrix_{dataset_name}.json")
                self.save_json({'confusion_matrix': results['confusion_matrix']}, cm_path)
                
                # åŒæ—¶ä¿å­˜ä¸ºCSVä¾¿äºåˆ†æ
                cm_df = pd.DataFrame(results['confusion_matrix'], 
                                    index=['å®é™…:æ­£å¸¸', 'å®é™…:æˆ¿é¢¤'], 
                                    columns=['é¢„æµ‹:æ­£å¸¸', 'é¢„æµ‹:æˆ¿é¢¤'])
                cm_csv_path = os.path.join(self.run_dir, "confusion_matrices", f"confusion_matrix_{dataset_name}.csv")
                cm_df.to_csv(cm_csv_path, encoding='utf-8-sig')
            
            self.log(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {self.run_dir}")
            return self.run_dir
            
        except Exception as e:
            self.log(f"ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}", level="ERROR")
            return None
    
    def save_comparison_results(self, comparison, dataset_name):
        """
        ä¿å­˜å¯¹æ¯”ç»“æœ
        
        Args:
            comparison: å¯¹æ¯”ç»“æœå­—å…¸
            dataset_name: æ•°æ®é›†åç§°
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # 1. ä¿å­˜JSONæ ¼å¼
            comparison_json_path = os.path.join(self.run_dir, "results", f"comparison_{dataset_name}.json")
            self.save_json(comparison, comparison_json_path)
            
            # 2. ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
            report_text = self._generate_comparison_report(comparison, dataset_name, timestamp)
            report_path = os.path.join(self.run_dir, "results", f"comparison_report_{dataset_name}.txt")
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            
            # 3. ä¿å­˜Markdownæ ¼å¼
            markdown_path = os.path.join(self.run_dir, "results", f"comparison_{dataset_name}.md")
            self._save_markdown_report(comparison, dataset_name, markdown_path)
            
            self.log(f"å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {comparison_json_path}")
            
        except Exception as e:
            self.log(f"ä¿å­˜å¯¹æ¯”ç»“æœå¤±è´¥: {e}", level="ERROR")
    
    def save_cross_validation_summary(self, cv_results):
        """
        ä¿å­˜äº¤å‰éªŒè¯æ±‡æ€»ç»“æœ
        
        Args:
            cv_results: äº¤å‰éªŒè¯ç»“æœå­—å…¸
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # 1. ä¿å­˜æ±‡æ€»ç»“æœ
            summary_path = os.path.join(self.run_dir, "results", "cross_validation_summary.json")
            self.save_json(cv_results, summary_path)
            
            # 2. åˆ›å»ºæ¯ä¸ªfoldçš„å•ç‹¬æ–‡ä»¶
            if 'fold_results' in cv_results:
                for fold_name, fold_result in cv_results['fold_results'].items():
                    fold_path = os.path.join(self.run_dir, "results", f"fold_{fold_name}.json")
                    self.save_json(fold_result, fold_path)
            
            # 3. åˆ›å»ºCSVæ±‡æ€»è¡¨æ ¼
            self._create_cv_summary_table(cv_results)
            
            self.log(f"äº¤å‰éªŒè¯æ±‡æ€»å·²ä¿å­˜åˆ°: {summary_path}")
            
        except Exception as e:
            self.log(f"ä¿å­˜äº¤å‰éªŒè¯æ±‡æ€»å¤±è´¥: {e}", level="ERROR")
    
    def log_experiment_info(self, config):
        """
        ä¿å­˜å®éªŒé…ç½®ä¿¡æ¯
        
        Args:
            config: å®éªŒé…ç½®å­—å…¸
        """
        try:
            info = {
                'experiment_name': self.experiment_name,
                'start_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'config': config,
                'system_info': {
                    'python_version': os.sys.version,
                    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
                    'torch_version': torch.__version__,
                    'cuda_available': torch.cuda.is_available(),
                    'cuda_version': torch.version.cuda if torch.cuda.is_available() else 'N/A'
                }
            }
            
            info_path = os.path.join(self.run_dir, "logs", "experiment_info.json")
            self.save_json(info, info_path)
            
            self.log(f"å®éªŒä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
            
        except Exception as e:
            self.log(f"ä¿å­˜å®éªŒä¿¡æ¯å¤±è´¥: {e}", level="ERROR")
    
    def save_error_report(self, error_info, context=""):
        """
        ä¿å­˜é”™è¯¯æŠ¥å‘Š
        
        Args:
            error_info: é”™è¯¯ä¿¡æ¯
            context: é”™è¯¯ä¸Šä¸‹æ–‡
        """
        try:
            error_report = {
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'error': str(error_info),
                'context': context,
                'traceback': self._get_traceback()
            }
            
            error_path = os.path.join(self.run_dir, "logs", "error_reports.json")
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿½åŠ é”™è¯¯
            if os.path.exists(error_path):
                with open(error_path, 'r', encoding='utf-8') as f:
                    existing_errors = json.load(f)
                existing_errors.append(error_report)
                errors_to_save = existing_errors
            else:
                errors_to_save = [error_report]
            
            self.save_json(errors_to_save, error_path)
            self.log(f"é”™è¯¯æŠ¥å‘Šå·²ä¿å­˜: {error_path}", level="ERROR")
            
        except Exception as e:
            print(f"ä¿å­˜é”™è¯¯æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _generate_comparison_report(self, comparison, dataset_name, timestamp):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šæ–‡æœ¬"""
        report = f"""
{'='*80}
ECGåˆ†ç±»æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š
{'='*80}

æ•°æ®é›†: {dataset_name}
ç”Ÿæˆæ—¶é—´: {timestamp}

æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:
{'='*80}

| æŒ‡æ ‡       | LLMæ¨¡å‹  | CNNåŸºçº¿  | å·®å¼‚      | æ€§èƒ½æå‡  |
|------------|----------|----------|----------|-----------|
"""
        
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        for metric in metrics:
            llm_val = comparison['LLM'].get(metric, 0) * 100
            cnn_val = comparison['CNN_Baseline'].get(metric, 0) * 100
            diff = comparison['Difference'].get(metric, 0) * 100
            
            if diff > 0:
                conclusion = f"â†‘{diff:.2f}%"
                diff_str = f"+{diff:.2f}%"
            elif diff < 0:
                conclusion = f"â†“{-diff:.2f}%"
                diff_str = f"{diff:.2f}%"
            else:
                conclusion = "æŒå¹³"
                diff_str = "0.00%"
            
            report += f"| {metric.capitalize():10} | {llm_val:.2f}% | {cnn_val:.2f}% | {diff_str:9} | {conclusion:10} |\n"
        
        # æ·»åŠ æ€»ç»“
        avg_diff = sum([abs(comparison['Difference'][m]) for m in metrics]) / len(metrics) * 100
        overall_diff = sum([comparison['Difference'][m] for m in metrics]) / len(metrics) * 100
        
        if overall_diff > 5:
            summary = "LLMæ¨¡å‹æ•´ä½“æ€§èƒ½æ˜¾è‘—ä¼˜äºCNNåŸºçº¿"
        elif overall_diff > 0:
            summary = "LLMæ¨¡å‹æ•´ä½“æ€§èƒ½ç•¥ä¼˜äºCNNåŸºçº¿"
        elif overall_diff < -5:
            summary = "CNNåŸºçº¿æ•´ä½“æ€§èƒ½æ˜¾è‘—ä¼˜äºLLMæ¨¡å‹"
        elif overall_diff < 0:
            summary = "CNNåŸºçº¿æ•´ä½“æ€§èƒ½ç•¥ä¼˜äºLLMæ¨¡å‹"
        else:
            summary = "ä¸¤ä¸ªæ¨¡å‹æ€§èƒ½ç›¸å½“"
        
        report += f"""
{'='*80}
æ€»ç»“: {summary}
å¹³å‡å·®å¼‚: {overall_diff:.2f}%
æœ€å¤§å·®å¼‚: {max([abs(comparison['Difference'][m]) for m in metrics])*100:.2f}%
{'='*80}
"""
        
        return report
    
    def _save_markdown_report(self, comparison, dataset_name, filepath):
        """ä¿å­˜Markdownæ ¼å¼æŠ¥å‘Š"""
        md_content = f"""# ECGåˆ†ç±»æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **æ•°æ®é›†**: {dataset_name}
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **å®éªŒç›®å½•**: {self.run_dir}

## æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | LLMæ¨¡å‹ | CNNåŸºçº¿ | å·®å¼‚ | ç»“è®º |
|------|---------|---------|------|------|
"""
        
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        for metric in metrics:
            llm_val = comparison['LLM'].get(metric, 0) * 100
            cnn_val = comparison['CNN_Baseline'].get(metric, 0) * 100
            diff = comparison['Difference'].get(metric, 0) * 100
            
            if diff > 0:
                conclusion = "âœ… LLMæ›´ä¼˜"
                diff_str = f"+{diff:.2f}%"
            elif diff < 0:
                conclusion = "ğŸ”µ CNNæ›´ä¼˜"
                diff_str = f"{diff:.2f}%"
            else:
                conclusion = "âšª æŒå¹³"
                diff_str = "0.00%"
            
            md_content += f"| {metric.capitalize()} | {llm_val:.2f}% | {cnn_val:.2f}% | {diff_str} | {conclusion} |\n"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(md_content)
    
    def _create_cv_summary_table(self, cv_results):
        """åˆ›å»ºäº¤å‰éªŒè¯æ±‡æ€»è¡¨æ ¼"""
        if 'fold_results' not in cv_results:
            return
        
        rows = []
        for fold_name, fold_data in cv_results['fold_results'].items():
            rows.append({
                'Fold': fold_name,
                'Accuracy': fold_data.get('accuracy', 0),
                'Precision': fold_data.get('precision', 0),
                'Recall': fold_data.get('recall', 0),
                'F1': fold_data.get('f1', 0),
                'Valid_Samples': fold_data.get('valid_samples', 0),
                'Invalid_Samples': fold_data.get('invalid_samples', 0),
                'Invalid_Rate': fold_data.get('invalid_rate', 0)
            })
        
        # æ·»åŠ å¹³å‡å€¼è¡Œ
        if rows:
            avg_row = {
                'Fold': 'Average',
                'Accuracy': cv_results.get('averages', {}).get('accuracy', 0),
                'Precision': cv_results.get('averages', {}).get('precision', 0),
                'Recall': cv_results.get('averages', {}).get('recall', 0),
                'F1': cv_results.get('averages', {}).get('f1', 0),
                'Valid_Samples': sum(r['Valid_Samples'] for r in rows) // len(rows),
                'Invalid_Samples': sum(r['Invalid_Samples'] for r in rows) // len(rows),
                'Invalid_Rate': sum(r['Invalid_Rate'] for r in rows) / len(rows)
            }
            rows.append(avg_row)
        
        df = pd.DataFrame(rows)
        csv_path = os.path.join(self.run_dir, "results", "cv_summary_table.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    
    def _get_traceback(self):
        """è·å–å½“å‰tracebackä¿¡æ¯"""
        import traceback
        return traceback.format_exc()


class SimpleResultSaver:
    """ç®€åŒ–ç‰ˆç»“æœä¿å­˜å™¨ï¼Œç”¨äºåº”æ€¥æƒ…å†µ"""
    
    def __init__(self, base_dir="Evaluation_Results", experiment_name=None):
        self.base_dir = os.path.abspath(base_dir)
        self.experiment_name = experiment_name or f"simple_exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.run_dir = os.path.join(self.base_dir, self.experiment_name)
        os.makedirs(self.run_dir, exist_ok=True)
        
        # åˆ›å»ºåŸºæœ¬å­ç›®å½•
        for subdir in ["results", "logs", "comparisons", "errors"]:
            os.makedirs(os.path.join(self.run_dir, subdir), exist_ok=True)
        
        print(f"ç®€æ˜“ä¿å­˜å™¨åˆå§‹åŒ–: {self.run_dir}")
    
    def log(self, message, level="INFO"):
        """ç®€åŒ–æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_msg = f"[{timestamp}] [{level}] {message}"
        print(log_msg)
        
        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        log_file = os.path.join(self.run_dir, "logs", "log.txt")
        try:
            with open(log_file, "a", encoding="utf-8") as f:
                f.write(log_msg + "\n")
        except:
            pass
    
    def save_json(self, data, filepath):
        """ç®€åŒ–JSONä¿å­˜"""
        try:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            self.log(f"æ–‡ä»¶å·²ä¿å­˜: {filepath}")
            return True
        except Exception as e:
            self.log(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}", level="ERROR")
            return False
    
    def save_evaluation_results(self, results, dataset_name, save_detailed=True):
        """ç®€åŒ–ç‰ˆä¿å­˜è¯„ä¼°ç»“æœ"""
        try:
            # åªä¿å­˜æœ€åŸºæœ¬çš„ç»“æœ
            result_file = os.path.join(self.run_dir, "results", f"results_{dataset_name}.json")
            self.save_json(results, result_file)
            
            # å¦‚æœæœ‰å¯èƒ½ï¼Œä¿å­˜CSV
            if save_detailed and 'detailed_results' in results:
                csv_file = os.path.join(self.run_dir, "results", f"detailed_{dataset_name}.csv")
                df = pd.DataFrame(results['detailed_results'])
                df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                
            return self.run_dir
        except Exception as e:
            self.log(f"ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}", level="ERROR")
            return None
    
    def save_comparison_results(self, comparison, dataset_name):
        """ç®€åŒ–ç‰ˆä¿å­˜å¯¹æ¯”ç»“æœ"""
        try:
            comparison_file = os.path.join(self.run_dir, "comparisons", f"comparison_{dataset_name}.json")
            self.save_json(comparison, comparison_file)
            
            # åŒæ—¶ä¿å­˜ä¸ºæ–‡æœ¬æ ¼å¼
            text_file = os.path.join(self.run_dir, "comparisons", f"comparison_{dataset_name}.txt")
            with open(text_file, "w", encoding="utf-8") as f:
                f.write(f"æ¨¡å‹å¯¹æ¯”ç»“æœ - {dataset_name}\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("="*60 + "\n")
                
                metrics = ['accuracy', 'precision', 'recall', 'f1']
                for metric in metrics:
                    llm_val = comparison.get('LLM', {}).get(metric, 0) * 100
                    cnn_val = comparison.get('CNN_Baseline', {}).get(metric, 0) * 100
                    diff = comparison.get('Difference', {}).get(metric, 0) * 100
                    
                    f.write(f"\n{metric.upper()}:\n")
                    f.write(f"  LLMæ¨¡å‹: {llm_val:.2f}%\n")
                    f.write(f"  CNNåŸºçº¿: {cnn_val:.2f}%\n")
                    f.write(f"  å·®å¼‚: {diff:+.2f}%\n")
            
            self.log(f"å¯¹æ¯”ç»“æœå·²ä¿å­˜: {comparison_file}")
        except Exception as e:
            self.log(f"ä¿å­˜å¯¹æ¯”ç»“æœå¤±è´¥: {e}", level="ERROR")
    
    def save_error_report(self, error_info, context=""):
        """ç®€åŒ–ç‰ˆä¿å­˜é”™è¯¯æŠ¥å‘Š"""
        try:
            error_data = {
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'error': str(error_info),
                'context': context
            }
            
            error_file = os.path.join(self.run_dir, "errors", f"error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            self.save_json(error_data, error_file)
            
            # åŒæ—¶è®°å½•åˆ°æ—¥å¿—
            self.log(f"é”™è¯¯æŠ¥å‘Šå·²ä¿å­˜: {error_file}", level="ERROR")
        except Exception as e:
            print(f"ä¿å­˜é”™è¯¯æŠ¥å‘Šå¤±è´¥: {e}")
    
    def save_cross_validation_summary(self, cv_results):
        """ç®€åŒ–ç‰ˆä¿å­˜äº¤å‰éªŒè¯æ±‡æ€»"""
        try:
            cv_file = os.path.join(self.run_dir, "results", "cross_validation_summary.json")
            self.save_json(cv_results, cv_file)
            self.log(f"äº¤å‰éªŒè¯æ±‡æ€»å·²ä¿å­˜: {cv_file}")
        except Exception as e:
            self.log(f"ä¿å­˜äº¤å‰éªŒè¯æ±‡æ€»å¤±è´¥: {e}", level="ERROR")
    
    def log_experiment_info(self, config):
        """ç®€åŒ–ç‰ˆè®°å½•å®éªŒä¿¡æ¯"""
        try:
            info_file = os.path.join(self.run_dir, "logs", "experiment_info.json")
            self.save_json(config, info_file)
            self.log(f"å®éªŒä¿¡æ¯å·²ä¿å­˜: {info_file}")
        except Exception as e:
            self.log(f"ä¿å­˜å®éªŒä¿¡æ¯å¤±è´¥: {e}", level="ERROR")

# ============================================================================
# LLMEvaluatorç±»
# ============================================================================
    def save_evaluation_results(self, results, dataset_name, save_detailed=True):
        """ç®€åŒ–ç‰ˆä¿å­˜è¯„ä¼°ç»“æœ"""
        try:
            # åªä¿å­˜æœ€åŸºæœ¬çš„ç»“æœ
            result_file = os.path.join(self.run_dir, f"results_{dataset_name}.json")
            self.save_json(results, result_file)
            
            # å¦‚æœæœ‰å¯èƒ½ï¼Œä¿å­˜CSV
            if save_detailed and 'detailed_results' in results:
                csv_file = os.path.join(self.run_dir, f"detailed_{dataset_name}.csv")
                df = pd.DataFrame(results['detailed_results'])
                df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                
            return self.run_dir
        except Exception as e:
            self.log(f"ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}", level="ERROR")
            return None


class LLMEvaluator:
    def __init__(self, model_path, cnn_config, ecg_token_id, llm_embed_dim, device=None, log_dir=None):
        """
        å…¼å®¹æ€§è¯„ä¼°å™¨ï¼Œè‡ªåŠ¨å¤„ç†ç»´åº¦é—®é¢˜
        Args:
            model_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            cnn_config: CNNé…ç½®
            ecg_token_id: ECG tokençš„ID
            llm_embed_dim: LLMåµŒå…¥ç»´åº¦
            device: è®¾å¤‡
        """
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device
            
        print(f"è¯„ä¼°è®¾å¤‡: {self.device}")

        self.result_saver = None
        self.ecg_token_id = ecg_token_id
        self.llm_embed_dim = llm_embed_dim
        self.log_dir = log_dir
        
        # 1. é¦–å…ˆåˆ›å»ºECGç¼–ç å™¨å¹¶è®¡ç®—å®é™…è¾“å‡ºç»´åº¦
        print("è®¡ç®—ECGç¼–ç å™¨è¾“å‡ºç»´åº¦...")
        self.ecg_encoder = ECGEncoder(cnn_config, CNN_WEIGHTS_PATH, device=self.device)
        self.actual_flat_dim = self._calculate_ecg_output_dim(FIXED_LENGTH)
        print(f"ECGç¼–ç å™¨å®é™…è¾“å‡ºç»´åº¦: {self.actual_flat_dim}")
        
        # 2. åŠ è½½æ¨¡å‹
        self.model = self._load_model_compatible(
            model_path, cnn_config, ecg_token_id, 
            self.actual_flat_dim, llm_embed_dim
        )
        
        # 3. è®¾ç½®tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            "/home/xusi/EE5046_Projects/LLM_Models/Qwen_Qwen-7B",
            trust_remote_code=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # 4. åˆå§‹åŒ–ç»“æœä¿å­˜å™¨
        self._init_result_saver(model_path)

    def _init_result_saver(self, model_path):
        """åˆå§‹åŒ–ç»“æœä¿å­˜å™¨"""
        try:
            # ä»è·¯å¾„æå–æ¨¡å‹åç§°
            model_name = os.path.basename(os.path.dirname(model_path))
            if not model_name or model_name == ".":
                model_name = os.path.basename(model_path)
            
            # æ¸…ç†æ¨¡å‹åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            model_name = re.sub(r'[^\w\-_]', '_', model_name)
            
            exp_name = f"llm_eval_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            print(f"åˆ›å»ºResultSaver: {exp_name}")
            
            # åˆ›å»ºResultSaver
            self.result_saver = ResultSaver(
                base_dir=self.log_dir or "Evaluation_Results",
                experiment_name=exp_name
            )
            
            print(f"âœ“ ResultSaveråˆ›å»ºæˆåŠŸ")
            print(f"  è¿è¡Œç›®å½•: {self.result_saver.run_dir}")
            
            # è®°å½•åŸºæœ¬æ¨¡å‹ä¿¡æ¯
            model_info = {
                'model_path': model_path,
                'device': str(self.device),
                'ecg_output_dim': self.actual_flat_dim,
                'ecg_token_id': self.ecg_token_id,
                'llm_embed_dim': self.llm_embed_dim,
                'evaluation_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'cnn_weights_path': CNN_WEIGHTS_PATH,
                'fixed_length': FIXED_LENGTH,
                'dataset_path': DATASET_PATH
            }
            
            # ç›´æ¥ä¿å­˜å®éªŒä¿¡æ¯ï¼Œé¿å…åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è°ƒç”¨log
            info_path = os.path.join(self.result_saver.run_dir, "logs", "experiment_info.json")
            os.makedirs(os.path.dirname(info_path), exist_ok=True)
            
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, indent=2, ensure_ascii=False)
            
            print(f"âœ“ å®éªŒä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
            
            # ç°åœ¨å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨logæ–¹æ³•
            self.result_saver.log(f"æ¨¡å‹è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
            self.result_saver.log(f"æ¨¡å‹è·¯å¾„: {model_path}")
            self.result_saver.log(f"ECGè¾“å‡ºç»´åº¦: {self.actual_flat_dim}")
            
        except Exception as e:
            print(f"åˆå§‹åŒ–ç»“æœä¿å­˜å™¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # åˆ›å»ºç®€æ˜“åå¤‡
            self.result_saver = SimpleResultSaver(self.log_dir or "Evaluation_Results", "fallback_eval")
            print(f"ä½¿ç”¨ç®€æ˜“ResultSaver: {self.result_saver.run_dir}")
            


    def generate_response(self, ecg_data, prompt_template="è¯·åˆ†æè¿™ä¸ªECGä¿¡å·ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰æˆ¿é¢¤ã€‚"):
        """
        ç”ŸæˆLLMçš„å“åº” - æ‰‹åŠ¨å®ç°ç”Ÿæˆ
        """
        with torch.no_grad():
            # å‡†å¤‡è¾“å…¥
            ecg_data = ecg_data.to(self.device)
            
            # è°ƒæ•´å½¢çŠ¶
            if ecg_data.dim() == 1:
                ecg_data = ecg_data.unsqueeze(0).unsqueeze(0)
            elif ecg_data.dim() == 2:
                ecg_data = ecg_data.unsqueeze(1)
            
            # æ„å»ºæç¤ºè¯
            prompt = f"æŒ‡ä»¤: <|extra_0|>{prompt_template}\nç­”æ¡ˆ:"
            input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)
            attention_mask = torch.ones_like(input_ids).to(self.device)
            
            # æ‰‹åŠ¨ç”Ÿæˆ
            generated_ids = self._simple_generate(
                ecg_data=ecg_data,
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=50
            )
            
            # è§£ç 
            full_response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            
            # æå–ç­”æ¡ˆ
            if "ç­”æ¡ˆ:" in full_response:
                answer = full_response.split("ç­”æ¡ˆ:")[1].strip()
            else:
                answer = full_response.replace(prompt, "").strip()
                
            return answer
        
    def _simple_generate(self, ecg_data, input_ids, attention_mask, max_new_tokens=50):
        """
        ç®€å•çš„è´ªå©ªè§£ç ç”Ÿæˆ
        """
        generated_ids = input_ids.clone()
        
        for i in range(max_new_tokens):
            # å‰å‘ä¼ æ’­
            outputs = self.model(
                ecg_data=ecg_data,
                input_ids=generated_ids,
                attention_mask=attention_mask,
                labels=None
            )
            
            # è·å–ä¸‹ä¸€ä¸ªtokençš„logits
            next_token_logits = outputs.logits[:, -1, :]
            
            # è´ªå©ªè§£ç ï¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„token
            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
            
            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆç»“æŸç¬¦
            if next_token.item() == self.tokenizer.eos_token_id:
                break
            
            # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
            generated_ids = torch.cat([generated_ids, next_token], dim=-1)
            
            # æ›´æ–°attention mask
            attention_mask = torch.cat([
                attention_mask, 
                torch.ones((1, 1), device=self.device, dtype=attention_mask.dtype)
            ], dim=-1)
        
        return generated_ids
    
    
    def _calculate_ecg_output_dim(self, fixed_length):
        """è®¡ç®—ECGç¼–ç å™¨çš„è¾“å‡ºç»´åº¦"""
        # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
        dummy_input = torch.randn(1, 1, fixed_length).to(self.device)
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.ecg_encoder.eval()
        
        with torch.no_grad():
            output = self.ecg_encoder(dummy_input)
        
        # å±•å¹³åè®¡ç®—ç»´åº¦
        output_dim = output.view(1, -1).size(1)
        return output_dim
    
    def _load_model_compatible(self, model_path, cnn_config, ecg_token_id, flat_dim, llm_embed_dim):
        """åŠ è½½æ¨¡å‹ï¼Œè‡ªåŠ¨å¤„ç†ç»´åº¦ä¸åŒ¹é…"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        
        try: 
            # 1. åˆå§‹åŒ–æ¨¡å‹ç»“æ„
            model = MultimodalLLM(
                llm_path="/home/xusi/EE5046_Projects/LLM_Models/Qwen_Qwen-7B",
                cnn_config=cnn_config,
                cnn_weights_path=CNN_WEIGHTS_PATH,
                ecg_token_id=ecg_token_id,
                flat_dim=flat_dim,  # ä½¿ç”¨è®¡ç®—å¾—åˆ°çš„å®é™…ç»´åº¦
                llm_embed_dim=llm_embed_dim,
                device=self.device
            )
            
            # 2. åŠ è½½LoRAé€‚é…å™¨
            lora_path = os.path.join(model_path, "lora_adapter")
            if os.path.exists(lora_path):
                print(f"åŠ è½½LoRAé€‚é…å™¨: {lora_path}")
                try:
                    # å…ˆä¿å­˜åŸå§‹LLM
                    original_llm = model.llm
                    
                    # ä½¿ç”¨PeftModelåŠ è½½é€‚é…å™¨
                    model.llm = PeftModel.from_pretrained(original_llm, lora_path)
                    print("LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
                except Exception as e:
                    print(f"åŠ è½½LoRAå¤±è´¥: {e}")
                    print("å°è¯•ç›´æ¥ä½¿ç”¨åŸºç¡€æ¨¡å‹...")
                    # å¦‚æœå¤±è´¥ï¼Œä¿æŒåŸå§‹æ¨¡å‹
            else:
                print(f"è­¦å‘Š: æœªæ‰¾åˆ°LoRAé€‚é…å™¨: {lora_path}")
            
            # 3. åŠ è½½projectoræƒé‡
            projector_path = os.path.join(model_path, "projector.pth")
            if os.path.exists(projector_path):
                print(f"åŠ è½½projectoræƒé‡: {projector_path}")
                projector_state = torch.load(projector_path, map_location=self.device)
                
                # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
                weight_shape = projector_state['weight'].shape
                expected_input_dim = weight_shape[1]  # æƒé‡å½¢çŠ¶: [output_dim, input_dim]
                actual_input_dim = flat_dim
                
                if expected_input_dim != actual_input_dim:
                    print(f"ç»´åº¦ä¸åŒ¹é…: æŠ•å½±å±‚æœŸæœ›è¾“å…¥ç»´åº¦={expected_input_dim}, å®é™…ç»´åº¦={actual_input_dim}")
                    print("æ­£åœ¨è°ƒæ•´æŠ•å½±å±‚æƒé‡...")
                    
                    # è°ƒæ•´æŠ•å½±å±‚æƒé‡ä»¥é€‚åº”å®é™…ç»´åº¦
                    adjusted_projector = self._adjust_projector_weights(
                        projector_state, expected_input_dim, actual_input_dim, llm_embed_dim
                    )
                    model.projector.load_state_dict(adjusted_projector)
                    print("æŠ•å½±å±‚æƒé‡è°ƒæ•´å®Œæˆ")
                else:
                    model.projector.load_state_dict(projector_state)
                    print("æŠ•å½±å±‚æƒé‡åŠ è½½æˆåŠŸ")
            else:
                print(f"è­¦å‘Š: æœªæ‰¾åˆ°projectoræƒé‡: {projector_path}")
                print("ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æŠ•å½±å±‚")
            
            # 4. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            model.eval()
            
            # 5. æ‰“å°æ¨¡å‹ä¿¡æ¯
            print(f"æ¨¡å‹åŠ è½½å®Œæˆ")
            print(f"  è®¾å¤‡: {self.device}")
            print(f"  ECGç¼–ç å™¨è¾“å‡ºç»´åº¦: {flat_dim}")
            print(f"  æŠ•å½±å±‚è¾“å…¥ç»´åº¦: {flat_dim}, è¾“å‡ºç»´åº¦: {llm_embed_dim}")
            
            return model
        
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            if self.result_saver:
                self.result_saver.save_error_report(e, "æ¨¡å‹åŠ è½½å¤±è´¥")
            raise
    
    def _adjust_projector_weights(self, original_state, expected_dim, actual_dim, llm_embed_dim):
        """
        è°ƒæ•´æŠ•å½±å±‚æƒé‡ä»¥é€‚åº”ä¸åŒçš„è¾“å…¥ç»´åº¦
        
        Args:
            original_state: åŸå§‹æŠ•å½±å±‚çŠ¶æ€å­—å…¸
            expected_dim: æœŸæœ›çš„è¾“å…¥ç»´åº¦ï¼ˆè®­ç»ƒæ—¶çš„ç»´åº¦ï¼‰
            actual_dim: å®é™…çš„è¾“å…¥ç»´åº¦ï¼ˆå½“å‰ECGç¼–ç å™¨çš„è¾“å‡ºç»´åº¦ï¼‰
            llm_embed_dim: LLMåµŒå…¥ç»´åº¦ï¼ˆè¾“å‡ºç»´åº¦ï¼‰
            
        Returns:
            è°ƒæ•´åçš„æŠ•å½±å±‚çŠ¶æ€å­—å…¸
        """
        print(f"è°ƒæ•´æŠ•å½±å±‚æƒé‡: {expected_dim} -> {actual_dim}")
        
        # åŸå§‹æƒé‡å½¢çŠ¶: [llm_embed_dim, expected_dim]
        original_weight = original_state['weight'].cpu()
        original_bias = original_state['bias'].cpu()
        
        if actual_dim > expected_dim:
            # å®é™…ç»´åº¦æ›´å¤§ï¼Œéœ€è¦æ‰©å±•æƒé‡çŸ©é˜µ
            new_weight = torch.zeros(llm_embed_dim, actual_dim)
            new_weight[:, :expected_dim] = original_weight  # å¤åˆ¶åŸå§‹æƒé‡
            # å‰©ä½™éƒ¨åˆ†ä¿æŒä¸ºé›¶ï¼ˆç›¸å½“äºä¸¢å¼ƒé¢å¤–ç‰¹å¾ï¼‰
            print(f"  æ‰©å±•æƒé‡çŸ©é˜µ: {original_weight.shape} -> {new_weight.shape}")
            
            # åç½®ä¸å˜
            new_bias = original_bias
            
        elif actual_dim < expected_dim:
            # å®é™…ç»´åº¦æ›´å°ï¼Œéœ€è¦æˆªæ–­æƒé‡çŸ©é˜µ
            new_weight = original_weight[:, :actual_dim]
            print(f"  æˆªæ–­æƒé‡çŸ©é˜µ: {original_weight.shape} -> {new_weight.shape}")
            
            # åç½®ä¸å˜
            new_bias = original_bias
            
        else:
            # ç»´åº¦ç›¸åŒï¼Œæ— éœ€è°ƒæ•´
            new_weight = original_weight
            new_bias = original_bias
            print(f"  ç»´åº¦ç›¸åŒï¼Œæ— éœ€è°ƒæ•´")
        
        # åˆ›å»ºæ–°çš„çŠ¶æ€å­—å…¸
        new_state = {
            'weight': new_weight.to(self.device),
            'bias': new_bias.to(self.device)
        }
        
        return new_state
    
    def parse_label_from_text(self, text):
        """
        ä»LLMç”Ÿæˆçš„æ–‡æœ¬ä¸­è§£ææ ‡ç­¾
        """
        text = text.lower().strip()
        
        # æŸ¥æ‰¾å…³é”®è¯
        for keyword, label in LABEL_MAPPING.items():
            if keyword.lower() in text:
                return label
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å…³é”®è¯ï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™
        negative_keywords = ["æ— ", "ä¸", "é", "æ­£å¸¸", "å¦", "negative", "normal", "æ­£å¸¸", "çª¦æ€§"]
        positive_keywords = ["æœ‰", "æ˜¯", "å¼‚å¸¸", "æˆ¿é¢¤", "af", "abnormal", "å¿ƒæˆ¿é¢¤åŠ¨", "å¿ƒæˆ¿çº¤é¢¤"]
        
        negative_count = sum(1 for word in negative_keywords if word in text)
        positive_count = sum(1 for word in positive_keywords if word in text)
        
        if positive_count > negative_count:
            return 1
        elif negative_count > positive_count:
            return 0
        else:
            # å¦‚æœæ— æ³•ç¡®å®šï¼Œè¿”å›-1è¡¨ç¤ºæ— æ³•è§£æ
            return -1
    
    def evaluate_on_dataset(self, dataset, batch_size=8,save_dir=None,dataset_name="unknown"):
        """
        åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹
        """
        try:
            dataloader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=False,
                collate_fn=lambda batch: self._collate_fn(batch)
            )
            
            all_true_labels = []
            all_pred_labels = []
            all_responses = []
            all_filenames = []
            all_confidences = []  # ä¿å­˜ç½®ä¿¡åº¦ä¿¡æ¯
            
            print(f"æ­£åœ¨è¯„ä¼°æ•°æ®é›† (å¤§å°: {len(dataset)})...")
            self.result_saver.log(f"å¼€å§‹è¯„ä¼°æ•°æ®é›†: {dataset_name}, æ ·æœ¬æ•°: {len(dataset)}")
            
            progress_bar = tqdm(dataloader, desc=f"è¯„ä¼° {dataset_name}")
            for batch_idx, batch in enumerate(progress_bar):
                ecg_data = batch['ecg_data'].to(self.device)
                true_labels = batch['labels'].cpu().numpy()
                filenames = batch['file_names']
                
                # æ‰¹é‡ç”Ÿæˆå“åº”
                batch_responses = []
                for i in range(ecg_data.size(0)):
                    try:
                        response = self.generate_response(ecg_data[i])
                        batch_responses.append(response)
                    except Exception as e:
                        batch_responses.append(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
                        self.result_saver.log(f"æ ·æœ¬ {filenames[i]} ç”Ÿæˆå¤±è´¥: {e}", level="WARNING")
                
                # è§£ææ ‡ç­¾
                batch_pred_labels = []
                batch_confidences = []
                for response in batch_responses:
                    pred_label = self.parse_label_from_text(response)
                    batch_pred_labels.append(pred_label)
                    
                    # ç®€å•çš„ç½®ä¿¡åº¦ä¼°è®¡ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ç¨‹åº¦ï¼‰
                    confidence = self._estimate_confidence(response, pred_label)
                    batch_confidences.append(confidence)
                
                all_true_labels.extend(true_labels)
                all_pred_labels.extend(batch_pred_labels)
                all_responses.extend(batch_responses)
                all_filenames.extend(filenames)
                all_confidences.extend(batch_confidences)
                
                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({
                    'å‡†ç¡®ç‡': f"{accuracy_score([l for l in all_pred_labels if l != -1], [all_true_labels[i] for i, l in enumerate(all_pred_labels) if l != -1]):.3f}" 
                    if len([l for l in all_pred_labels if l != -1]) > 0 else "N/A"
                })
            
            # è¿‡æ»¤æ‰æ— æ³•è§£æçš„æ ·æœ¬
            valid_indices = [i for i, label in enumerate(all_pred_labels) if label != -1]
            valid_true = [all_true_labels[i] for i in valid_indices]
            valid_pred = [all_pred_labels[i] for i in valid_indices]
            valid_responses = [all_responses[i] for i in valid_indices]
            valid_filenames = [all_filenames[i] for i in valid_indices]
            valid_confidences = [all_confidences[i] for i in valid_indices]
            
            # è®¡ç®—æŒ‡æ ‡
            results = self._calculate_metrics(
                valid_true, valid_pred, valid_responses, 
                valid_filenames, valid_confidences
            )
            
            # æ·»åŠ æ— æ³•è§£æçš„æ ·æœ¬ä¿¡æ¯
            results['total_samples'] = len(all_true_labels)
            results['valid_samples'] = len(valid_true)
            results['invalid_samples'] = len(all_true_labels) - len(valid_true)
            results['invalid_rate'] = results['invalid_samples'] / results['total_samples'] if results['total_samples'] > 0 else 0
            
            # æ·»åŠ æ•°æ®é›†ä¿¡æ¯
            results['dataset_name'] = dataset_name
            results['evaluation_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # ä½¿ç”¨ResultSaverä¿å­˜ç»“æœ
            self.result_saver.save_evaluation_results(results, dataset_name, save_detailed=True)
            
            # æ‰“å°ç®€è¦ç»“æœ
            self._print_summary(results, dataset_name)
            
            return results
            
        except Exception as e:
            self.result_saver.save_error_report(e, f"æ•°æ®é›†è¯„ä¼°å¤±è´¥: {dataset_name}")
            raise

    def _estimate_confidence(self, response, predicted_label):
        """ä¼°è®¡é¢„æµ‹ç½®ä¿¡åº¦"""
        response_lower = response.lower()
        
        # åŸºäºå…³é”®è¯åŒ¹é…çš„ç½®ä¿¡åº¦
        if predicted_label == 1:  # æˆ¿é¢¤
            strong_indicators = ["ç¡®è¯Š", "æ˜ç¡®", "è‚¯å®š", "ä¸€å®šæ˜¯", "ç¡®å®š", "æ¯«æ— ç–‘é—®"]
            weak_indicators = ["å¯èƒ½", "ç–‘ä¼¼", "æ€€ç–‘", "å€¾å‘", "è€ƒè™‘"]
            
            confidence = 0.8  # åŸºç¡€ç½®ä¿¡åº¦
            for indicator in strong_indicators:
                if indicator in response_lower:
                    confidence = min(confidence + 0.15, 1.0)
            for indicator in weak_indicators:
                if indicator in response_lower:
                    confidence = max(confidence - 0.2, 0.5)
                    
        elif predicted_label == 0:  # æ­£å¸¸
            strong_indicators = ["å®Œå…¨æ­£å¸¸", "æœªè§å¼‚å¸¸", "æ­£å¸¸å¿ƒç‡", "çª¦æ€§å¿ƒå¾‹", "æ— å¼‚å¸¸"]
            weak_indicators = ["åŸºæœ¬æ­£å¸¸", "å¤§è‡´æ­£å¸¸", "æœªè§æ˜æ˜¾å¼‚å¸¸"]
            
            confidence = 0.8  # åŸºç¡€ç½®ä¿¡åº¦
            for indicator in strong_indicators:
                if indicator in response_lower:
                    confidence = min(confidence + 0.15, 1.0)
            for indicator in weak_indicators:
                if indicator in response_lower:
                    confidence = max(confidence - 0.2, 0.5)
        else:
            confidence = 0.0
            
        return round(confidence, 2)
    
    def _collate_fn(self, batch):
        """è¯„ä¼°æ—¶çš„collateå‡½æ•°"""
        # å¤„ç†ECGæ•°æ®
        ecg_data_list = []
        for item in batch:
            ecg = item['ecg_data']
            # ç¡®ä¿æ˜¯1Dï¼Œç„¶åæ·»åŠ é€šé“ç»´åº¦
            if ecg.dim() == 1:
                ecg = ecg.unsqueeze(0)  # [length] -> [1, length]
            ecg_data_list.append(ecg)
        
        # å †å : [batch, 1, length]
        ecg_data = torch.stack(ecg_data_list)
        
        labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)
        file_names = [item['file_name'] for item in batch]
        
        return {
            'ecg_data': ecg_data,  # [batch, 1, length]
            'labels': labels,
            'file_names': file_names
        }
    
    def _calculate_metrics(self, true_labels, pred_labels, responses, filenames=None,confidences=None):
        """è®¡ç®—åˆ†ç±»æŒ‡æ ‡"""
        if len(true_labels) == 0:
            return {
                'accuracy': 0,
                'precision': 0,
                'recall': 0,
                'f1': 0,
                'confusion_matrix': [[0, 0], [0, 0]],
                'classification_report': '',
                'detailed_results': [],
                'class_distribution': {'normal': 0, 'af': 0},
                'average_confidence': 0
            }
        
        accuracy = accuracy_score(true_labels, pred_labels)
        precision = precision_score(true_labels, pred_labels, average='binary', zero_division=0)
        recall = recall_score(true_labels, pred_labels, average='binary', zero_division=0)
        f1 = f1_score(true_labels, pred_labels, average='binary', zero_division=0)
        cm = confusion_matrix(true_labels, pred_labels).tolist()
        
        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        report = classification_report(true_labels, pred_labels, target_names=['æ­£å¸¸', 'æˆ¿é¢¤'], zero_division=0,output_dict=True)
        
        # åˆ›å»ºè¯¦ç»†ç»“æœåˆ—è¡¨
        detailed_results = []
        for i in range(len(true_labels)):
            result = {
                'filename': filenames[i] if filenames else f"sample_{i}",
                'true_label': int(true_labels[i]),
                'true_label_str': 'æˆ¿é¢¤' if true_labels[i] == 1 else 'æ­£å¸¸',
                'pred_label': int(pred_labels[i]),
                'pred_label_str': 'æˆ¿é¢¤' if pred_labels[i] == 1 else 'æ­£å¸¸',
                'response': responses[i],
                'confidence': confidences[i] if confidences else None,
                'correct': true_labels[i] == pred_labels[i],
                'prediction_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            detailed_results.append(result)
        
        # ç»Ÿè®¡å“åº”ç¤ºä¾‹
        response_examples = []
        normal_examples = []
        af_examples = []

        for i in range(min(10, len(responses))):
            example = {
                'true_label': int(true_labels[i]),
                'true_label_str': 'æˆ¿é¢¤' if true_labels[i] == 1 else 'æ­£å¸¸',
                'pred_label': int(pred_labels[i]),
                'pred_label_str': 'æˆ¿é¢¤' if pred_labels[i] == 1 else 'æ­£å¸¸',
                'response': responses[i][:200] + "..." if len(responses[i]) > 200 else responses[i],
                'filename': filenames[i] if filenames else f"sample_{i}",
                'confidence': confidences[i] if confidences else None
            }
            response_examples.append(example)
            
            # åˆ†ç±»åˆ«æ”¶é›†ç¤ºä¾‹
            if true_labels[i] == 0 and len(normal_examples) < 3:
                normal_examples.append(example)
            elif true_labels[i] == 1 and len(af_examples) < 3:
                af_examples.append(example)
        
        # è®¡ç®—ç±»åˆ«åˆ†å¸ƒ
        normal_count = sum(1 for label in true_labels if label == 0)
        af_count = sum(1 for label in true_labels if label == 1)
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = np.mean(confidences) if confidences and len(confidences) > 0 else 0
        
        return {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1': float(f1),
            'confusion_matrix': cm,
            'classification_report': report,
            'classification_report_str': classification_report(true_labels, pred_labels, target_names=['æ­£å¸¸', 'æˆ¿é¢¤'], zero_division=0),
            'response_examples': response_examples,
            'detailed_results': detailed_results,
            'class_distribution': {
                'normal': normal_count,
                'af': af_count,
                'normal_percentage': normal_count / len(true_labels) * 100 if len(true_labels) > 0 else 0,
                'af_percentage': af_count / len(true_labels) * 100 if len(true_labels) > 0 else 0
            },
            'average_confidence': float(avg_confidence),
            'normal_examples': normal_examples,
            'af_examples': af_examples,
            'metrics_by_class': {
                'normal': {
                    'precision': report.get('æ­£å¸¸', {}).get('precision', 0),
                    'recall': report.get('æ­£å¸¸', {}).get('recall', 0),
                    'f1': report.get('æ­£å¸¸', {}).get('f1-score', 0)
                },
                'af': {
                    'precision': report.get('æˆ¿é¢¤', {}).get('precision', 0),
                    'recall': report.get('æˆ¿é¢¤', {}).get('recall', 0),
                    'f1': report.get('æˆ¿é¢¤', {}).get('f1-score', 0)
                }
            }
        }
    
    def _print_summary(self, results, dataset_name):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*60)
        print(f"LLMæ¨¡å‹è¯„ä¼°ç»“æœ - {dataset_name}")
        print("="*60)
        print(f"æ€»æ ·æœ¬æ•°: {results['total_samples']}")
        print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {results['valid_samples']}")
        print(f"æ— æ³•è§£ææ ·æœ¬æ•°: {results['invalid_samples']} ({results['invalid_rate']*100:.2f}%)")
        print(f"ç±»åˆ«åˆ†å¸ƒ: æ­£å¸¸={results['class_distribution']['normal']} ({results['class_distribution']['normal_percentage']:.1f}%), "
              f"æˆ¿é¢¤={results['class_distribution']['af']} ({results['class_distribution']['af_percentage']:.1f}%)")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {results['average_confidence']:.2f}")
        print()
        print(f"å‡†ç¡®ç‡: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
        print(f"ç²¾ç¡®ç‡: {results['precision']:.4f} ({results['precision']*100:.2f}%)")
        print(f"å¬å›ç‡: {results['recall']:.4f} ({results['recall']*100:.2f}%)")
        print(f"F1åˆ†æ•°: {results['f1']:.4f} ({results['f1']*100:.2f}%)")
        
        print("\næ··æ·†çŸ©é˜µ:")
        cm = results['confusion_matrix']
        print(f"         é¢„æµ‹æ­£å¸¸   é¢„æµ‹æˆ¿é¢¤")
        print(f"å®é™…æ­£å¸¸   {cm[0][0]:^10}   {cm[0][1]:^10}")
        print(f"å®é™…æˆ¿é¢¤   {cm[1][0]:^10}   {cm[1][1]:^10}")
        
        # è®°å½•åˆ°æ—¥å¿—
        self.result_saver.log(f"è¯„ä¼°å®Œæˆ: {dataset_name} - å‡†ç¡®ç‡: {results['accuracy']:.4f}, F1: {results['f1']:.4f}")
    

    def compare_with_cnn_baseline(self, llm_results, cnn_results_path,dataset_name):
        """
        ä¸CNNåŸºçº¿æ¨¡å‹å¯¹æ¯”
        """
        try:
            # åŠ è½½CNNåŸºçº¿ç»“æœ
            if os.path.exists(cnn_results_path):
                with open(cnn_results_path, 'r', encoding='utf-8') as f:
                    cnn_results_data = json.load(f)
                
                # å°è¯•ä»ä¸åŒæ ¼å¼ä¸­æå–CNNç»“æœ
                if 'metrics' in cnn_results_data:
                    cnn_results = cnn_results_data['metrics']
                elif 'final_results' in cnn_results_data:
                    cnn_results = cnn_results_data['final_results']
                else:
                    cnn_results = cnn_results_data
            else:
                self.result_saver.log(f"è­¦å‘Š: æœªæ‰¾åˆ°CNNåŸºçº¿ç»“æœ: {cnn_results_path}", level="WARNING")
                # ä½¿ç”¨é»˜è®¤å€¼
                cnn_results = {
                    'accuracy': 0.85,
                    'precision': 0.86,
                    'recall': 0.84,
                    'f1': 0.85
                }
            
            comparison = {
                'dataset': dataset_name,
                'comparison_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'LLM': {
                    'accuracy': llm_results['accuracy'],
                    'precision': llm_results['precision'],
                    'recall': llm_results['recall'],
                    'f1': llm_results['f1']
                },
                'CNN_Baseline': {
                    'accuracy': cnn_results.get('accuracy', cnn_results.get('Accuracy', 0)),
                    'precision': cnn_results.get('precision', cnn_results.get('Precision', 0)),
                    'recall': cnn_results.get('recall', cnn_results.get('Recall', 0)),
                    'f1': cnn_results.get('f1', cnn_results.get('F1', 0))
                }
            }
            
            # è®¡ç®—å·®å¼‚
            metrics = ['accuracy', 'precision', 'recall', 'f1']
            comparison['Difference'] = {}
            for metric in metrics:
                llm_val = comparison['LLM'][metric]
                cnn_val = comparison['CNN_Baseline'][metric]
                comparison['Difference'][metric] = llm_val - cnn_val
            
            # ä¿å­˜å¯¹æ¯”ç»“æœ
            self.result_saver.save_comparison_results(comparison, dataset_name)
            
            # æ‰“å°å¯¹æ¯”ç»“æœ
            self._print_comparison(comparison)
            
            return comparison
            
        except Exception as e:
            self.result_saver.save_error_report(e, "CNNåŸºçº¿å¯¹æ¯”å¤±è´¥")
            raise

    def _print_comparison(self, comparison):
        """æ‰“å°å¯¹æ¯”ç»“æœ"""
        print("\n" + "="*60)
        print("ä¸CNNåŸºçº¿æ¨¡å‹å¯¹æ¯”")
        print("="*60)
        
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        for metric in metrics:
            llm_val = comparison['LLM'][metric] * 100
            cnn_val = comparison['CNN_Baseline'][metric] * 100
            diff = comparison['Difference'][metric] * 100
            
            print(f"\n{metric.capitalize()}:")
            print(f"  LLM: {llm_val:.2f}%")
            print(f"  CNN: {cnn_val:.2f}%")
            
            if diff > 0:
                print(f"  LLMä¼˜äºCNN: +{diff:.2f}%")
            elif diff < 0:
                print(f"  CNNä¼˜äºLLM: {-diff:.2f}%")
            else:
                print(f"  ä¸¤è€…ç›¸åŒ")
    
    def test_single_sample(self, dataset):
        """æµ‹è¯•å•ä¸ªæ ·æœ¬çš„å¤„ç†"""
        print("æµ‹è¯•å•ä¸ªæ ·æœ¬...")
        
        if len(dataset) == 0:
            print("æ•°æ®é›†ä¸ºç©º")
            return False
        
        # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        sample = dataset[0]
        print(f"æ ·æœ¬ECGå½¢çŠ¶: {sample['ecg_data'].shape}")
        print(f"æ ·æœ¬æ ‡ç­¾: {sample['label']} ({'æˆ¿é¢¤' if sample['label'] == 1 else 'æ­£å¸¸'})")
        
        # æµ‹è¯•ç”Ÿæˆ
        try:
            response = self.generate_response(sample['ecg_data'])
            print(f"ç”ŸæˆæˆåŠŸ!")
            print(f"å“åº”: {response}")

            # è§£ææ ‡ç­¾
            pred_label = self.parse_label_from_text(response)
            print(f"è§£ææ ‡ç­¾: {pred_label} ({'æˆ¿é¢¤' if pred_label == 1 else 'æ­£å¸¸' if pred_label == 0 else 'æ— æ³•è§£æ'})")
            
            # æ£€æŸ¥æ˜¯å¦æ­£ç¡®
            if pred_label == sample['label']:
                print("âœ“ é¢„æµ‹æ­£ç¡®")
            elif pred_label == -1:
                print("âš  æ— æ³•è§£æå“åº”")
            else:
                print("âœ— é¢„æµ‹é”™è¯¯")
            
            return True
        except Exception as e:
            print(f"ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False


def load_cnn_config(config_path):
    """åŠ è½½CNNé…ç½®"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # å°è¯•ä»ä¸åŒæ ¼å¼ä¸­æå–é…ç½®
        if 'best_config' in config:
            return config['best_config'].get('kernel_config', {})
        elif 'kernel_config' in config:
            return config['kernel_config']
        else:
            return config
    except Exception as e:
        print(f"åŠ è½½CNNé…ç½®å¤±è´¥: {e}")
        return {}


def load_ecg_from_mat(file_name, data_dir, ecg_key='val'):
    """
    ä».matæ–‡ä»¶åŠ è½½ECGæ•°æ®
    
    Args:
        file_name: .matæ–‡ä»¶åï¼ˆä¸å¸¦è·¯å¾„ï¼‰
        data_dir: æ•°æ®ç›®å½•
        ecg_key: .matæ–‡ä»¶ä¸­ECGæ•°æ®çš„é”®å
    
    Returns:
        ecg_data: ECGä¿¡å·æ•°æ®
    """
    # æ„å»ºå®Œæ•´è·¯å¾„
    full_path = os.path.join(data_dir, file_name)
    
    if not os.path.exists(full_path):
        # å°è¯•æ·»åŠ æ‰©å±•å
        if not file_name.endswith('.mat'):
            full_path = os.path.join(data_dir, f"{file_name}.mat")
        
        if not os.path.exists(full_path):
            print(f"è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
            return None
    
    try:
        # åŠ è½½.matæ–‡ä»¶
        mat_data = sio.loadmat(full_path)
        
        # æŸ¥æ‰¾ECGæ•°æ®
        # å¸¸è§é”®å: 'val', 'ECG', 'ecg', 'data'
        if ecg_key in mat_data:
            ecg_data = mat_data[ecg_key]
        elif 'ECG' in mat_data:
            ecg_data = mat_data['ECG']
        elif 'ecg' in mat_data:
            ecg_data = mat_data['ecg']
        elif 'data' in mat_data:
            ecg_data = mat_data['data']
        else:
            # å¦‚æœæ‰¾ä¸åˆ°å¸¸è§çš„é”®ï¼Œå°è¯•ç¬¬ä¸€ä¸ªæ•°å€¼æ•°ç»„
            for key in mat_data.keys():
                if not key.startswith('__') and isinstance(mat_data[key], np.ndarray):
                    ecg_data = mat_data[key]
                    print(f"ä½¿ç”¨é”® '{key}' ä½œä¸ºECGæ•°æ®")
                    break
            else:
                raise ValueError(f"åœ¨æ–‡ä»¶ {file_name} ä¸­æ‰¾ä¸åˆ°ECGæ•°æ®")
        
        # ç¡®ä¿æ˜¯1ç»´æˆ–2ç»´æ•°ç»„ï¼Œå¹¶è½¬æ¢ä¸º1ç»´
        if ecg_data.ndim > 1:
            ecg_data = ecg_data.flatten()
        
        return ecg_data
        
    except Exception as e:
        print(f"åŠ è½½ECGæ–‡ä»¶å¤±è´¥ {file_name}: {e}")
        return None


def preprocess_ecg_data(ecg_data, fixed_length, normalize=True):
    """
    é¢„å¤„ç†ECGæ•°æ®
    
    Args:
        ecg_data: åŸå§‹ECGæ•°æ®
        fixed_length: å›ºå®šé•¿åº¦
        normalize: æ˜¯å¦æ ‡å‡†åŒ–
    
    Returns:
        processed_data: é¢„å¤„ç†åçš„ECGæ•°æ®
    """
    if ecg_data is None:
        return None
    
    # 1. æˆªæ–­æˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦
    if len(ecg_data) > fixed_length:
        # æˆªæ–­ä¸­é—´éƒ¨åˆ†
        start_idx = (len(ecg_data) - fixed_length) // 2
        ecg_data = ecg_data[start_idx:start_idx + fixed_length]
    elif len(ecg_data) < fixed_length:
        # å¡«å……ä¸¤ä¾§
        pad_left = (fixed_length - len(ecg_data)) // 2
        pad_right = fixed_length - len(ecg_data) - pad_left
        ecg_data = np.pad(ecg_data, (pad_left, pad_right), mode='constant')
    
    # 2. å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
    if normalize:
        ecg_data = (ecg_data - np.mean(ecg_data)) / (np.std(ecg_data) + 1e-8)
    
    # 3. è½¬æ¢ä¸ºtensorå¹¶è°ƒæ•´ç»´åº¦ [1, 1, FIXED_LENGTH]
    ecg_tensor = torch.tensor(ecg_data, dtype=torch.float32)
    
    return ecg_tensor


def create_evaluation_dataset_from_csv(csv_path, data_dir, fixed_length, test_mode=True):
    """
    ä»CSVæ–‡ä»¶åˆ›å»ºè¯„ä¼°æ•°æ®é›†
    
    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        data_dir: ECGæ•°æ®æ–‡ä»¶ç›®å½•ï¼ˆåŒ…å«.matæ–‡ä»¶ï¼‰
        fixed_length: ECGæ•°æ®å›ºå®šé•¿åº¦
        test_mode: æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼ï¼ˆç¦ç”¨æ•°æ®å¢å¼ºï¼‰
    
    Returns:
        dataset: è¯„ä¼°æ•°æ®é›†
    """
    class EvalDataset(torch.utils.data.Dataset):
        def __init__(self, csv_path, data_dir, fixed_length, test_mode=True):
            # åŠ è½½CSVæ–‡ä»¶
            self.df = pd.read_csv(csv_path)
            
            # é‡å‘½ååˆ—ä»¥ç»Ÿä¸€å¤„ç†
            if 'file_name' not in self.df.columns:
                # å°è¯•æ‰¾åˆ°åŒ…å«æ–‡ä»¶åçš„åˆ—
                if 'record_name' in self.df.columns:
                    self.df = self.df.rename(columns={'record_name': 'file_name'})
                elif 'filename' in self.df.columns:
                    self.df = self.df.rename(columns={'filename': 'file_name'})
                elif 'name' in self.df.columns:
                    self.df = self.df.rename(columns={'name': 'file_name'})
                else:
                    # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ–‡ä»¶å
                    self.df = self.df.rename(columns={self.df.columns[0]: 'file_name'})
            
            if 'label' not in self.df.columns:
                # å°è¯•æ‰¾åˆ°åŒ…å«æ ‡ç­¾çš„åˆ—
                if 'symbol' in self.df.columns:
                    self.df = self.df.rename(columns={'symbol': 'label'})
                elif 'class' in self.df.columns:
                    self.df = self.df.rename(columns={'class': 'label'})
                elif 'target' in self.df.columns:
                    self.df = self.df.rename(columns={'target': 'label'})
                else:
                    # å‡è®¾ç¬¬äºŒåˆ—æ˜¯æ ‡ç­¾
                    self.df = self.df.rename(columns={self.df.columns[1]: 'label'})
            
            # ç¡®ä¿æ ‡ç­¾æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä¾¿äºå¤„ç†
            self.df['label'] = self.df['label'].astype(str).str.strip()
            
            # è½¬æ¢æ ‡ç­¾ï¼šA -> 1 (æˆ¿é¢¤), å…¶ä»– -> 0 (éæˆ¿é¢¤)
            self.df['label_int'] = self.df['label'].apply(
                lambda x: 1 if x.upper() == 'A' else 0
            )
            
            self.data_dir = data_dir
            self.fixed_length = fixed_length
            self.test_mode = test_mode
            
            print(f"ä»CSVåŠ è½½æ•°æ®é›†å®Œæˆï¼Œå…± {len(self.df)} ä¸ªæ ·æœ¬")
            print(f"æ ‡ç­¾åˆ†å¸ƒ: æˆ¿é¢¤(A)={sum(self.df['label_int'] == 1)}, éæˆ¿é¢¤={sum(self.df['label_int'] == 0)}")
        
        def __len__(self):
            return len(self.df)
        
        def __getitem__(self, idx):
            row = self.df.iloc[idx]
            file_name = row['file_name']
            
            # åŠ è½½ECGæ•°æ®
            ecg_raw = load_ecg_from_mat(file_name, self.data_dir)
            
            if ecg_raw is None:
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ›å»ºé›¶æ•°æ®ï¼ˆä½†ä¼šæ ‡è®°ä¸ºæ— æ•ˆï¼‰
                print(f"è­¦å‘Š: æ— æ³•åŠ è½½ECGæ•°æ®: {file_name}")
                ecg_raw = np.zeros(self.fixed_length)
            
            # é¢„å¤„ç†ECGæ•°æ®
            ecg_data = preprocess_ecg_data(ecg_raw, self.fixed_length, normalize=True)
            
            # è·å–æ ‡ç­¾ï¼ˆæ•´æ•°å½¢å¼ï¼‰
            label = int(row['label_int'])
            
            return {
                'ecg_data': ecg_data,
                'label': label,
                'file_name': file_name
            }
    
    return EvalDataset(csv_path, data_dir, fixed_length, test_mode)


def create_evaluation_dataset_from_multiple_csv(csv_paths, data_dir, fixed_length, test_mode=True):
    """
    ä»å¤šä¸ªCSVæ–‡ä»¶åˆ›å»ºè¯„ä¼°æ•°æ®é›†
    
    Args:
        csv_paths: CSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        data_dir: ECGæ•°æ®æ–‡ä»¶ç›®å½•
        fixed_length: ECGæ•°æ®å›ºå®šé•¿åº¦
        test_mode: æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼
    
    Returns:
        dataset: åˆå¹¶çš„è¯„ä¼°æ•°æ®é›†
    """
    from torch.utils.data import ConcatDataset
    
    datasets = []
    for csv_path in csv_paths:
        print(f"åŠ è½½CSVæ–‡ä»¶: {csv_path}")
        dataset = create_evaluation_dataset_from_csv(csv_path, data_dir, fixed_length, test_mode)
        datasets.append(dataset)
    
    return ConcatDataset(datasets)


def main():
    """ä¸»è¯„ä¼°å‡½æ•°"""
    try: 
        # é…ç½®å‚æ•° - éœ€è¦æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
        MODEL_PATH = "/home/xusi/EE5046_Projects/Trained_Multimodal_Models/Qwen7B_ECG_B8_LR2e-05_E8/final_model"
        
        # æ•°æ®é›†è·¯å¾„ - æ ¹æ®æ‚¨çš„ç›®å½•ç»“æ„è°ƒæ•´
        DATASET_BASE = DATASET_PATH  # ä»Config.pyå¯¼å…¥ï¼Œåº”è¯¥æ˜¯Datasetç›®å½•çš„çˆ¶ç›®å½•
        TRAINING2017_DIR = os.path.join(DATASET_BASE, "training2017")
        CV_DIR = os.path.join(DATASET_BASE, "cv")
        
        # é€‰æ‹©è¦è¯„ä¼°çš„CSVæ–‡ä»¶ï¼ˆå¯ä»¥è¯„ä¼°å•ä¸ªæˆ–å¤šä¸ªï¼‰
        # æ–¹æ¡ˆ1ï¼šè¯„ä¼°å•ä¸ªCSVæ–‡ä»¶
        TEST_CSV_PATH = os.path.join(CV_DIR, "cv1.csv")  # ä½¿ç”¨cv1.csvä½œä¸ºæµ‹è¯•é›†
        
        # æ–¹æ¡ˆ2ï¼šè¯„ä¼°æ‰€æœ‰CSVæ–‡ä»¶ï¼ˆäº¤å‰éªŒè¯ï¼‰
        # all_csv_paths = [os.path.join(CV_DIR, f"cv{i}.csv") for i in range(5)]
        
        # åŸºçº¿ç»“æœè·¯å¾„
        CNN_BASELINE_RESULTS = "/home/xusi/Logs/FinalTraining/Results_20251217_115456/cnn_evaluation_results.json"
        OUTPUT_DIR = "/home/xusi/EE5046_Projects/Evaluation_Results"
        
        # åŠ è½½CNNé…ç½®
        cnn_config_path = "/home/xusi/Logs/FinalTraining/Results_20251217_115456/cnn_evaluation_results.json"
        cnn_config = load_cnn_config(cnn_config_path).get("best_config", {}).get("kernel_config", {})
        
        # è®¾ç½®æ¨¡å‹å‚æ•°ï¼ˆéœ€è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        cnn_config_full = {
            'ch_in': 1,
            'ch_out': 1,
            'use_stream2': True,
            'stream1_kernel': 3,
            'stream2_first_kernel': 7,
        }
        
        # è®¾ç½®ECG token IDï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
        ECG_TOKEN_ID = 151646  # <|extra_0|>çš„ID
        
        # è®¾ç½®ç»´åº¦å‚æ•°ï¼ˆéœ€è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        LLM_EMBED_DIM = 4096  # Qwen-7BåµŒå…¥ç»´åº¦
        
        # åˆ›å»ºè¯„ä¼°å™¨
        print("åˆå§‹åŒ–è¯„ä¼°å™¨...")
        evaluator = LLMEvaluator(
            model_path=MODEL_PATH,
            cnn_config=cnn_config_full,
            ecg_token_id=ECG_TOKEN_ID,
            llm_embed_dim=LLM_EMBED_DIM,
            log_dir=OUTPUT_DIR
        )
        
        # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(TRAINING2017_DIR):
            print(f"é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {TRAINING2017_DIR}")
            print("è¯·æ£€æŸ¥DATASET_PATHé…ç½®æ˜¯å¦æ­£ç¡®")
            return
        
        if not os.path.exists(TEST_CSV_PATH):
            print(f"é”™è¯¯: CSVæ–‡ä»¶ä¸å­˜åœ¨: {TEST_CSV_PATH}")
            print(f"CVç›®å½•å†…å®¹: {os.listdir(CV_DIR) if os.path.exists(CV_DIR) else 'ç›®å½•ä¸å­˜åœ¨'}")
            return
        
        # åˆ›å»ºè¯„ä¼°æ•°æ®é›†ï¼ˆä»CSVæ–‡ä»¶ï¼‰
        print(f"åˆ›å»ºè¯„ä¼°æ•°æ®é›†...")
        
        eval_dataset = create_evaluation_dataset_from_csv(
            csv_path=TEST_CSV_PATH,
            data_dir=TRAINING2017_DIR,
            fixed_length=FIXED_LENGTH,
            test_mode=True
        )
        
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
        if len(eval_dataset) == 0:
            print("é”™è¯¯: è¯„ä¼°æ•°æ®é›†ä¸ºç©ºï¼")
            return
        
        # æµ‹è¯•å•ä¸ªæ ·æœ¬
        evaluator.test_single_sample(eval_dataset)

        # è¯„ä¼°æ¨¡å‹
        print("å¼€å§‹è¯„ä¼°LLMæ¨¡å‹...")
        dataset_name = os.path.basename(TEST_CSV_PATH).replace('.csv', '')
        results = evaluator.evaluate_on_dataset(eval_dataset, batch_size=4,dataset_name=dataset_name)

        # ä¸CNNåŸºçº¿å¯¹æ¯”ï¼ˆå¦‚æœæœ‰ï¼‰
        if os.path.exists(CNN_BASELINE_RESULTS):
            comparison = evaluator.compare_with_cnn_baseline(results, CNN_BASELINE_RESULTS, dataset_name)

        print(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {evaluator.result_saver.run_dir}")

    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    

def generate_comparison_report(comparison, llm_results, output_dir, timestamp, dataset_name):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    report = f"""
    ============================================================================
    ECGåˆ†ç±»æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š - {dataset_name}
    ============================================================================
    
    è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    
    1. æ•°æ®é›†ä¿¡æ¯
    -------------
    æ€»æ ·æœ¬æ•°: {llm_results['total_samples']}
    æœ‰æ•ˆæ ·æœ¬æ•°: {llm_results['valid_samples']}
    æ— æ³•è§£ææ ·æœ¬æ•°: {llm_results['invalid_samples']} ({llm_results['invalid_rate']*100:.2f}%)
    
    2. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    --------------
    """
    
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    for metric in metrics:
        llm_val = comparison['LLM'][metric]
        cnn_val = comparison['CNN_Baseline'][metric]
        diff = comparison['Difference'][metric]
        
        report += f"""
    {metric.upper()}:
      - LLMæ¨¡å‹: {llm_val:.4f}
      - CNNåŸºçº¿: {cnn_val:.4f}
      - å·®å¼‚: {diff:+.4f} ({'LLMæ›´ä¼˜' if diff > 0 else 'CNNæ›´ä¼˜' if diff < 0 else 'ç›¸åŒ'})"""
    
    report += f"""
    
    3. LLMæ¨¡å‹è¯¦ç»†ç»“æœ
    -----------------
    æ··æ·†çŸ©é˜µ: {llm_results['confusion_matrix']}
    
    åˆ†ç±»æŠ¥å‘Š:
    {llm_results['classification_report']}
    
    4. ç»“è®º
    -------
    """
    
    # è®¡ç®—å¹³å‡å·®å¼‚
    avg_diff = sum(comparison['Difference'].values()) / len(comparison['Difference'])
    
    if avg_diff > 0.05:
        conclusion = "LLMæ¨¡å‹åœ¨æ•´ä½“æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºCNNåŸºçº¿æ¨¡å‹ã€‚"
    elif avg_diff > 0:
        conclusion = "LLMæ¨¡å‹åœ¨æ•´ä½“æ€§èƒ½ä¸Šç•¥ä¼˜äºCNNåŸºçº¿æ¨¡å‹ã€‚"
    elif avg_diff < -0.05:
        conclusion = "CNNåŸºçº¿æ¨¡å‹åœ¨æ•´ä½“æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºLLMæ¨¡å‹ã€‚"
    elif avg_diff < 0:
        conclusion = "CNNåŸºçº¿æ¨¡å‹åœ¨æ•´ä½“æ€§èƒ½ä¸Šç•¥ä¼˜äºLLMæ¨¡å‹ã€‚"
    else:
        conclusion = "LLMæ¨¡å‹å’ŒCNNåŸºçº¿æ¨¡å‹æ€§èƒ½ç›¸å½“ã€‚"
    
    report += conclusion + "\n"
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(output_dir, f"comparison_report_{dataset_name}_{timestamp}.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


def run_cross_validation_evaluation():
    """
    è¿è¡Œäº¤å‰éªŒè¯è¯„ä¼°ï¼ˆè¯„ä¼°æ‰€æœ‰cv0-cv4ï¼‰
    """
    try: 
        # é…ç½®å‚æ•°
        MODEL_PATH = "/home/xusi/EE5046_Projects/Trained_Multimodal_Models/Qwen7B_ECG_B8_LR2e-05_E8/final_model"
        DATASET_BASE = DATASET_PATH
        TRAINING2017_DIR = os.path.join(DATASET_BASE, "training2017")
        CV_DIR = os.path.join(DATASET_BASE, "cv")
        OUTPUT_DIR = "/home/xusi/EE5046_Projects/Evaluation_Results"
        
        # åŠ è½½CNNé…ç½®
        cnn_config_path = "/home/xusi/Logs/FinalTraining/Results_20251217_115456/cnn_evaluation_results.json"
        cnn_config = load_cnn_config(cnn_config_path).get("best_config", {}).get("kernel_config", {})
        
        # è®¾ç½®æ¨¡å‹å‚æ•°
        cnn_config_full = {
            'ch_in': 1,
            'ch_out': 1,
            'use_stream2': True,
            'stream1_kernel': 3,
            'stream2_first_kernel': 7
        }
        
        ECG_TOKEN_ID = 151646
        LLM_EMBED_DIM = 4096
        
        # åˆ›å»ºè¯„ä¼°å™¨
        print("åˆå§‹åŒ–è¯„ä¼°å™¨...")
        evaluator = LLMEvaluator(
            model_path=MODEL_PATH,
            cnn_config=cnn_config_full,
            ecg_token_id=ECG_TOKEN_ID,
            llm_embed_dim=LLM_EMBED_DIM,
            log_dir=OUTPUT_DIR
        )
        
        # è¯„ä¼°æ‰€æœ‰CSVæ–‡ä»¶
        all_results = {}
        
        for i in range(5):
            csv_path = os.path.join(CV_DIR, f"cv{i}.csv")
            
            if not os.path.exists(csv_path):
                print(f"è·³è¿‡ä¸å­˜åœ¨çš„CSVæ–‡ä»¶: {csv_path}")
                continue
            
            print(f"\n{'='*60}")
            print(f"è¯„ä¼°æ•°æ®é›†: cv{i}.csv")
            print(f"{'='*60}")
            
            # åˆ›å»ºè¯„ä¼°æ•°æ®é›†
            eval_dataset = create_evaluation_dataset_from_csv(
                csv_path=csv_path,
                data_dir=TRAINING2017_DIR,
                fixed_length=FIXED_LENGTH,
                test_mode=True
            )
            
            if len(eval_dataset) == 0:
                print(f"æ•°æ®é›† cv{i}.csv ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            # è¯„ä¼°æ¨¡å‹
            dataset_name = f"cv{i}"
            results = evaluator.evaluate_on_dataset(eval_dataset, batch_size=4,dataset_name=dataset_name)
            
            # ä¿å­˜åˆ°æ±‡æ€»ç»“æœ
            all_results[f"cv{i}"] = {
                'accuracy': results['accuracy'],
                'precision': results['precision'],
                'recall': results['recall'],
                'f1': results['f1'],
                'total_samples': results['total_samples'],
                'valid_samples': results['valid_samples'],
                'invalid_samples': results['invalid_samples'],
                'invalid_rate': results['invalid_rate'],
                'class_distribution': results['class_distribution']
            }
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if all_results:
            print(f"\n{'='*60}")
            print("äº¤å‰éªŒè¯æ±‡æ€»ç»“æœ")
            print(f"{'='*60}")
            
            avg_accuracy = np.mean([r['accuracy'] for r in all_results.values()])
            avg_precision = np.mean([r['precision'] for r in all_results.values()])
            avg_recall = np.mean([r['recall'] for r in all_results.values()])
            avg_f1 = np.mean([r['f1'] for r in all_results.values()])
            avg_invalid_rate = np.mean([r['invalid_rate'] for r in all_results.values()])
            
            print(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
            print(f"å¹³å‡ç²¾ç¡®ç‡: {avg_precision:.4f}")
            print(f"å¹³å‡å¬å›ç‡: {avg_recall:.4f}")
            print(f"å¹³å‡F1åˆ†æ•°: {avg_f1:.4f}")
            print(f"å¹³å‡æ— æ•ˆæ ·æœ¬ç‡: {avg_invalid_rate:.4f} ({avg_invalid_rate*100:.2f}%)")
            
            # ä¿å­˜æ±‡æ€»ç»“æœ
            cv_summary = {
                    'cross_validation_results': all_results,
                    'averages': {
                        'accuracy': float(avg_accuracy),
                        'precision': float(avg_precision),
                        'recall': float(avg_recall),
                        'f1': float(avg_f1),
                        'invalid_rate': float(avg_invalid_rate)
                    },
                    'total_samples': sum(r['total_samples'] for r in all_results.values()),
                    'valid_samples': sum(r['valid_samples'] for r in all_results.values()),
                    'invalid_samples': sum(r['invalid_samples'] for r in all_results.values()),
                    'evaluation_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                
            evaluator.result_saver.save_cross_validation_summary(cv_summary)

    except Exception as e:
        print(f"äº¤å‰éªŒè¯è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    import sys
    
    print("="*60)
    print("ECGåˆ†ç±»æ¨¡å‹è¯„ä¼°ç³»ç»Ÿ")
    print("="*60)
    
    # æ£€æŸ¥å¿…è¦çš„å¯¼å…¥
    try:
        import scipy.io
    except ImportError:
        print("é”™è¯¯: éœ€è¦å®‰è£…scipyåº“")
        print("è¯·è¿è¡Œ: pip install scipy")
        sys.exit(1)
    
    try:
        import sklearn
    except ImportError:
        print("é”™è¯¯: éœ€è¦å®‰è£…scikit-learnåº“")
        print("è¯·è¿è¡Œ: pip install scikit-learn")
        sys.exit(1)
    
    # é€‰æ‹©è¿è¡Œæ–¹å¼
    print("\nè¯·é€‰æ‹©è¯„ä¼°æ¨¡å¼:")
    print("1. è¯„ä¼°å•ä¸ªCSVæ–‡ä»¶ (cv0.csv)")
    print("2. è¿è¡Œäº¤å‰éªŒè¯ (è¯„ä¼°æ‰€æœ‰cv0-cv4.csv)")
    print("3. é€€å‡º")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
    
    if choice == '1':
        main()
    elif choice == '2':
        run_cross_validation_evaluation()
    elif choice == '3':
        print("é€€å‡ºç¨‹åº")
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œé€€å‡ºç¨‹åº")