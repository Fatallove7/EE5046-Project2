"""
ECGæˆ¿é¢¤æ£€æµ‹è®­ç»ƒä¸»ç¨‹åº
æ”¯æŒè¶…å‚æ•°æœç´¢ã€é»˜è®¤è®­ç»ƒã€å®Œæ•´è®­ç»ƒå’Œå¯¹æ¯”å®éªŒå››ç§æ¨¡å¼
"""

# ==================== å¯¼å…¥éƒ¨åˆ† ====================
import argparse
import json
import os
import sys
from datetime import datetime
from glob import glob
from sklearn.model_selection import KFold
import numpy as np

import matplotlib
matplotlib.use('Agg')  # è®¾ç½®ä¸ºéäº¤äº’å¼åç«¯ï¼Œé¿å…GUIé—®é¢˜
import matplotlib.pyplot as plt
import pandas as pd
import torch
from sklearn.metrics import (accuracy_score, roc_auc_score, roc_curve, 
                           precision_score, recall_score, f1_score, 
                           confusion_matrix, classification_report)
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import seaborn as sns
import scipy.io as scio
from ECGDataset import ECG_dataset

# è‡ªå®šä¹‰æ¨¡å—
from Config import (AUGMENT_SETTING, BATCH_SIZE, EARLY_STOP_PATIENCE,
                    EXPERIMENT_MODE, FIXED_LENGTH, INPUT_CHANNELS,
                    LEARNING_RATE, MIN_DELTA, NUM_EPOCHS, OUTPUT_CLASSES,
                    USE_STREAM2_SETTING, COMPARISON_MODE,
                    STREAM_COMPARISON_CONFIGS, AUGMENTATION_COMPARISON_CONFIGS,
                    DEFAULT_KERNEL_CONFIG, DOWNSAMPLE_RATE)
from TrainModel import Mscnn


# è®¾ç½®å­—ä½“æ ¼å¼
matplotlib.use('Agg')
# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams.update({
    'font.size': 11,
    'axes.titlesize': 12,
    'axes.labelsize': 11,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'legend.fontsize': 10,
    'figure.titlesize': 12
})

# è®¾å¤‡è®¾ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
plt.style.use('seaborn-v0_8-darkgrid')

# =========================æ•°æ®ç®¡ç†å™¨ï¼Œç®¡ç†æ•°æ®é›†çš„åŠ è½½å’Œåˆ’åˆ†=================================
class DataManager:
    """ç®¡ç†æ•°æ®é›†çš„åŠ è½½å’Œåˆ’åˆ†"""
    
    def __init__(self, base_path):
        self.base_path = base_path
        self.cv_path = os.path.join(base_path, 'cv')
        
    def load_cv_file(self, cv_index):
        """åŠ è½½å•ä¸ªCVæ–‡ä»¶"""
        csv_file = os.path.join(self.cv_path, f'cv{cv_index}.csv')
        if os.path.exists(csv_file):
            df = pd.read_csv(csv_file)
            return df
        else:
            print(f"Warning: {csv_file} not found.")
            return pd.DataFrame()
        
    def load_multiple_cv_files(self, cv_indices):
        """åŠ è½½å¤šä¸ªCVæ–‡ä»¶å¹¶åˆå¹¶"""
        all_dfs = []
        for idx in cv_indices:
            df = self.load_cv_file(idx)
            if not df.empty:
                all_dfs.append(df)
        
        if all_dfs:
            combined_df = pd.concat(all_dfs, ignore_index=True)
            print(f"Loaded {len(combined_df)} samples from CV files: {cv_indices}")
            return combined_df
        else:
            print("No data loaded.")
            return pd.DataFrame()
    
    def create_kfold_splits(self, data_df, k=5, random_seed=42):
        """
        åˆ›å»ºKæŠ˜äº¤å‰éªŒè¯åˆ’åˆ†
        Returns: list of (train_indices, val_indices) tuples
        """
        if data_df.empty:
            return []
        
        kf = KFold(n_splits=k, shuffle=True, random_state=random_seed)
        
        # è·å–æ–‡ä»¶ååˆ—è¡¨ç”¨äºåˆ’åˆ†
        filenames = data_df.iloc[:, 1].values  # å‡è®¾ç¬¬äºŒåˆ—æ˜¯æ–‡ä»¶å
        
        splits = []
        for train_idx, val_idx in kf.split(filenames):
            splits.append((train_idx, val_idx))
        
        return splits
    
    def get_data_by_indices(self, data_df, indices):
        """æ ¹æ®ç´¢å¼•è·å–æ•°æ®å­é›†"""
        if data_df.empty or len(indices) == 0:
            return pd.DataFrame()
        return data_df.iloc[indices].reset_index(drop=True)
    

# ==================== è‡ªå®šä¹‰æ•°æ®é›†ç±» ====================
class TemporaryECGDataset(Dataset):
    """ä¸´æ—¶æ•°æ®é›†ç±»ï¼Œç”¨äºä»DataFrameåŠ è½½æ•°æ®"""
    
    def __init__(self, data_df, base_path, is_train=True, augment=False):
        """
        Args:
            data_df: DataFrameåŒ…å«æ•°æ®
            base_path: æ•°æ®é›†æ ¹ç›®å½•
            is_train: æ˜¯å¦ä¸ºè®­ç»ƒé›†
            augment: æ˜¯å¦å¯ç”¨æ•°æ®å¢å¼º
        """
        self.data_df = data_df
        self.base_path = base_path
        self.is_train = is_train
        self.augment = augment
        
    def __len__(self):
        return len(self.data_df)
    
    def _load_mat_data(self, filename):
        """åŠ è½½.matæ–‡ä»¶æ•°æ®"""
        mat_path = os.path.join(self.base_path, 'training2017', f'{filename}.mat')
        try:
            data = scio.loadmat(mat_path)['val'][0]
            return data
        except Exception as e:
            print(f"Error loading {mat_path}: {e}")
            return np.zeros(1000)  # è¿”å›é»˜è®¤å€¼
    
    def _add_noise(self, data):
        """æ·»åŠ é«˜æ–¯å™ªå£°"""
        if np.random.rand() < 0.5:
            noise_level = 0.05
            noise = np.random.normal(0, noise_level, data.shape)
            data = data + noise
        return data
    
    def _time_scaling(self, data):
        """æ—¶é—´å°ºåº¦ç¼©æ”¾"""
        if np.random.rand() < 0.5:
            scale_factor = np.random.uniform(0.8, 1.2)
            old_len = len(data)
            new_len = int(old_len * scale_factor)
            
            # ä½¿ç”¨çº¿æ€§æ’å€¼
            x_old = np.linspace(0, 1, old_len)
            x_new = np.linspace(0, 1, new_len)
            data = np.interp(x_new, x_old, data)
        return data
    
    def _crop_padding(self, data, length, apply_augment):
        """è£å‰ªæˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦"""
        L_raw = len(data)
        
        if L_raw <= length:
            # å¡«å……
            pad_len = length - L_raw
            data = np.pad(data, (0, pad_len), 'constant')
        elif L_raw > length:
            # è£å‰ª
            max_start = L_raw - length
            
            if self.is_train and apply_augment:
                # éšæœºè£å‰ª
                start = np.random.randint(0, max_start + 1)
            else:
                # ä¸­å¿ƒè£å‰ª
                start = max_start // 2
                
            data = data[start:start + length]
        
        return data
    
    def _preprocess_data(self, data, apply_augment):
        """æ•°æ®é¢„å¤„ç†æµç¨‹"""
        # 1. é™é‡‡æ ·
        if DOWNSAMPLE_RATE > 1:
            data = data[::DOWNSAMPLE_RATE]
        
        # 2. æ—¶é—´ç¼©æ”¾
        if self.is_train and apply_augment:
            data = self._time_scaling(data)
        
        # 3. å½’ä¸€åŒ–
        data = data - data.mean()
        std = data.std()
        if std > 0:
            data = data / std
        
        # 4. æ·»åŠ å™ªå£°
        if self.is_train and apply_augment:
            data = self._add_noise(data)
        
        # 5. è£å‰ª/å¡«å……
        data = self._crop_padding(data, FIXED_LENGTH, apply_augment)
        
        return data
    
    def __getitem__(self, idx):
        # è·å–æ–‡ä»¶åå’Œæ ‡ç­¾
        row = self.data_df.iloc[idx]
        
        # å‡è®¾CSVæ ¼å¼ä¸º: index, filename, label
        if len(row) >= 3:
            filename = row[1]
            label = row[2]
        else:
            # å¦‚æœæ²¡æœ‰indexåˆ—
            filename = row[0]
            label = row[1]
        
        # åŠ è½½æ•°æ®
        data = self._load_mat_data(filename)
        
        # é¢„å¤„ç†
        if self.is_train and self.augment:
            data = self._preprocess_data(data, apply_augment=True)
        else:
            data = self._preprocess_data(data, apply_augment=False)
        
        # è½¬æ¢ä¸ºtensor
        data_tensor = torch.FloatTensor(data).unsqueeze(0)  # æ·»åŠ é€šé“ç»´åº¦
        
        # æ ‡ç­¾è½¬æ¢
        if label == 'A':
            label_tensor = torch.tensor([1.0])
        else:
            label_tensor = torch.tensor([0.0])
        
        return data_tensor, label_tensor, filename
    
# ==================== æ¨¡å‹éªŒè¯å™¨ ====================
class ModelValidator:
    """æ¨¡å‹éªŒè¯ç›¸å…³åŠŸèƒ½"""
    
    @staticmethod
    def validate(model, criterion, dataloader, device):
        """
        éªŒè¯å‡½æ•°ï¼šè®¡ç®— Loss, Accuracy, AUC å¹¶è¿”å›æ‰€æœ‰é¢„æµ‹å€¼
        è¿”å›: (avg_loss, accuracy, auc, labels, probabilities)
        """
        model.eval()
        running_loss = 0.0
        all_probs = []
        all_labels = []

        with torch.no_grad():
            for x, y, _ in dataloader:
                x = x.to(device).float()
                y = y.to(device).float()
                x = torch.reshape(x, (-1, 1, FIXED_LENGTH))
                
                probs = model(x)
                loss = criterion(probs, y)
                running_loss += loss.item()
                
                all_probs.extend(probs.cpu().numpy().flatten())
                all_labels.extend(y.cpu().numpy().flatten())

        # è®¡ç®—æŒ‡æ ‡
        avg_loss = running_loss / len(dataloader)
        all_probs = np.array(all_probs)
        all_labels = np.array(all_labels)
        
        # é¢„æµ‹ç±»åˆ«
        preds = (all_probs >= 0.5).astype(int)
        acc = accuracy_score(all_labels, preds)
        
        try:
            auc = roc_auc_score(all_labels, all_probs)
        except ValueError:
            auc = 0.5  # é˜²æ­¢åªæœ‰ä¸€ä¸ªç±»åˆ«æŠ¥é”™

        # è®¡ç®—precision, recall, f1
        try:
            precision = precision_score(all_labels, preds, average='binary', zero_division=0)
            recall = recall_score(all_labels, preds, average='binary', zero_division=0)
            f1 = f1_score(all_labels, preds, average='binary', zero_division=0)
        except:
            precision = 0.0
            recall = 0.0
            f1 = 0.0
        
        model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼
        return avg_loss, acc, auc, all_labels, all_probs, precision, recall, f1

# =========================æ¨¡å‹æ–‡ä»¶ç®¡ç†å™¨ï¼Œè´Ÿè´£ç®¡ç†æ¨¡å‹æ–‡ä»¶çš„ä¿å­˜å’Œæ¸…ç†=================================
class ModelManager:
    @staticmethod
    def organize_model_files(experiment_base_dir, fold_results):
        """ç»„ç»‡æ¨¡å‹æ–‡ä»¶ï¼Œåˆ›å»ºç»Ÿä¸€çš„ç›®å½•ç»“æ„"""
        print("\n" + "="*80)
        print("ğŸ“ æ•´ç†æ¨¡å‹æ–‡ä»¶")
        print("="*80)
        
        # 1. åˆ›å»ºç»Ÿä¸€ç›®å½•ç»“æ„
        models_base_dir = os.path.join(experiment_base_dir, "Models")
        dirs_to_create = [
            os.path.join(models_base_dir, "Best_Models"),
            os.path.join(models_base_dir, "Checkpoints"),
            os.path.join(models_base_dir, "Final_Models"),
            os.path.join(models_base_dir, "Logs")
        ]
        
        for dir_path in dirs_to_create:
            os.makedirs(dir_path, exist_ok=True)
        
        # 2. å¤åˆ¶æ‰€æœ‰æŠ˜çš„æœ€ä½³æ¨¡å‹åˆ°ç»Ÿä¸€ç›®å½•
        print("å¤åˆ¶æœ€ä½³æ¨¡å‹åˆ°ç»Ÿä¸€ç›®å½•...")
        best_models_summary = []
        
        for fold_result in fold_results:
            fold = fold_result['fold']
            model_path = fold_result.get('best_model_path')
            
            if model_path and os.path.exists(model_path):
                # å¤åˆ¶åˆ°ç»Ÿä¸€ç›®å½•
                target_dir = os.path.join(models_base_dir, "Best_Models")
                target_path = os.path.join(target_dir, os.path.basename(model_path))
                
                try:
                    import shutil
                    shutil.copy2(model_path, target_path)
                    
                    best_models_summary.append({
                        'fold': fold,
                        'model_file': os.path.basename(model_path),
                        'loss': fold_result['best_loss'],
                        'accuracy': fold_result['final_val_acc']
                    })
                    
                    print(f"  âœ… ç¬¬{fold}æŠ˜: {os.path.basename(model_path)}")
                except Exception as e:
                    print(f"  âŒ å¤åˆ¶ç¬¬{fold}æŠ˜æ¨¡å‹å¤±è´¥: {e}")
        
        # 3. åˆ›å»ºæ¨¡å‹æ‘˜è¦æ–‡ä»¶
        summary_path = os.path.join(models_base_dir, "Logs", "model_summary.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump({
                'total_folds': len(best_models_summary),
                'models': best_models_summary,
                'organized_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }, f, indent=2)
        
        print(f"ğŸ“‹ æ¨¡å‹æ‘˜è¦å·²ä¿å­˜: {summary_path}")
        
        return models_base_dir
    
    @staticmethod
    def create_final_ensemble_model(fold_results, experiment_base_dir, kernel_config):
        """åˆ›å»ºæœ€ç»ˆé›†æˆæ¨¡å‹ï¼ˆå¯é€‰ï¼‰"""
        try:
            print("\nåˆ›å»ºæœ€ç»ˆé›†æˆæ¨¡å‹...")
            
            models_base_dir = os.path.join(experiment_base_dir, "Models", "Best_Models")
            model_files = glob(os.path.join(models_base_dir, "*.pth"))
            
            if len(model_files) < 3:  # è‡³å°‘éœ€è¦3ä¸ªæ¨¡å‹
                print("æ¨¡å‹æ•°é‡ä¸è¶³ï¼Œè·³è¿‡é›†æˆæ¨¡å‹åˆ›å»º")
                return None
            
            # åˆ›å»ºæ¨¡å‹é…ç½®æ‘˜è¦
            ensemble_config = {
                'type': 'ensemble',
                'num_models': len(model_files),
                'models': [os.path.basename(f) for f in model_files],
                'kernel_config': kernel_config,
                'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            config_path = os.path.join(experiment_base_dir, "Models", "ensemble_config.json")
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(ensemble_config, f, indent=2)
            
            print(f"âœ… é›†æˆæ¨¡å‹é…ç½®å·²ä¿å­˜: {config_path}")
            
            # åˆ›å»ºä¸€ä¸ªè½»é‡çº§çš„é›†æˆæ¨¡å‹æ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰æ¨¡å‹è·¯å¾„ï¼‰
            ensemble_model = {
                'ensemble_type': 'majority_voting',
                'model_paths': model_files,
                'weights': [1.0] * len(model_files),  # å¹³ç­‰æƒé‡
                'config': ensemble_config
            }
            
            ensemble_path = os.path.join(experiment_base_dir, "Models", "Final_Models", "ensemble_model.pth")
            torch.save(ensemble_model, ensemble_path)
            
            print(f"âœ… é›†æˆæ¨¡å‹å·²ä¿å­˜: {ensemble_path}")
            
            return ensemble_path
            
        except Exception as e:
            print(f"åˆ›å»ºé›†æˆæ¨¡å‹å¤±è´¥: {e}")
            return None

# ==================== å·¥å…·å‡½æ•°æ¨¡å— ====================
class TrainingUtils:
    """è®­ç»ƒå·¥å…·å‡½æ•°é›†åˆ"""
    
    @staticmethod
    def save_loss(fold, value):
        """ä¿å­˜æŸå¤±å€¼åˆ°æ–‡ä»¶"""
        path = f'loss{fold}.txt'
        with open(path, mode='a+') as file:
            file.write(str(value) + '\n')
    
    @staticmethod
    def plot_roc_curve(labels, probs, epoch, title=None):
        """ç»˜åˆ¶ROCæ›²çº¿å¹¶è¿”å›figureå¯¹è±¡ - ä½¿ç”¨è‹±æ–‡"""
        fpr, tpr, _ = roc_curve(labels, probs)
        roc_auc = roc_auc_score(labels, probs)

        fig = plt.figure(figsize=(6, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2, 
                label=f'ROC curve (AUC = {roc_auc:.4f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')  # æ”¹ä¸ºè‹±æ–‡
        plt.ylabel('True Positive Rate')   # æ”¹ä¸ºè‹±æ–‡
        plt.title(title if title else f'ROC Curve (Epoch {epoch})')  # æ”¹ä¸ºè‹±æ–‡
        plt.legend(loc="lower right")
        return fig, roc_auc
    
    @staticmethod
    def _generate_standard_model_name(self, fold, epoch, loss, is_best=False):
        """ç”Ÿæˆæ ‡å‡†åŒ–çš„æ¨¡å‹æ–‡ä»¶å"""
        # åŸºç¡€ç»„ä»¶
        kernel_tag = f"K{self.kernel_config['stream1_kernel']}{self.kernel_config['stream2_first_kernel']}"
        stream_tag = "S2" if self.use_stream2 else "S1"
        augment_tag = "Aug" if self.augment else "NoAug"
        config_tag = f"{self.config_name}" if self.config_name else ""
        
        # æ„å»ºæ–‡ä»¶åï¼ˆæŒ‰é€»è¾‘é¡ºåºï¼‰
        parts = [
            "Mscnn",
            kernel_tag,
            stream_tag,
            augment_tag,
            f"F{fold}",
            f"E{epoch:03d}",
            f"L{loss:.4f}".replace('.', 'p'),
            "BEST" if is_best else "",
            config_tag
        ]
        
        # è¿‡æ»¤ç©ºéƒ¨åˆ†å¹¶è¿æ¥
        filename = "_".join(filter(None, parts)) + ".pth"
        
        return filename
    
    
    @staticmethod
    def create_visualization_directory(base_dir, experiment_name):
        """åˆ›å»ºå¯è§†åŒ–ç›®å½•ç»“æ„"""
        vis_dir = os.path.join(base_dir, "Visualizations", experiment_name)
        subdirs = ["Metrics", "ROC_Curves", "Comparison_Plots", "Confusion_Matrices"]
        
        for subdir in subdirs:
            os.makedirs(os.path.join(vis_dir, subdir), exist_ok=True)
        
        return vis_dir


# ==================== éªŒè¯æ¨¡å— ====================
class ModelValidator:
    """æ¨¡å‹éªŒè¯ç›¸å…³åŠŸèƒ½"""
    
    @staticmethod
    def validate(model, criterion, dataloader, device):
        """
        éªŒè¯å‡½æ•°ï¼šè®¡ç®— Loss, Accuracy, AUC å¹¶è¿”å›æ‰€æœ‰é¢„æµ‹å€¼
        è¿”å›: (avg_loss, accuracy, auc, labels, probabilities)
        """
        model.eval()
        running_loss = 0.0
        all_probs = []
        all_labels = []

        with torch.no_grad():
            for x, y, _ in dataloader:
                x = x.to(device).float()
                y = y.to(device).float()
                x = torch.reshape(x, (-1, 1, FIXED_LENGTH))
                
                probs = model(x)
                loss = criterion(probs, y)
                running_loss += loss.item()
                
                all_probs.extend(probs.cpu().numpy().flatten())
                all_labels.extend(y.cpu().numpy().flatten())

        # è®¡ç®—æŒ‡æ ‡
        avg_loss = running_loss / len(dataloader)
        all_probs = np.array(all_probs)
        all_labels = np.array(all_labels)
        
        # é¢„æµ‹ç±»åˆ«
        preds = (all_probs >= 0.5).astype(int)
        acc = accuracy_score(all_labels, preds)
        
        try:
            auc = roc_auc_score(all_labels, all_probs)
        except ValueError:
            auc = 0.5  # é˜²æ­¢åªæœ‰ä¸€ä¸ªç±»åˆ«æŠ¥é”™

        # è®¡ç®—precision, recall, f1
        try:
            precision = precision_score(all_labels, preds, average='binary', zero_division=0)
            recall = recall_score(all_labels, preds, average='binary', zero_division=0)
            f1 = f1_score(all_labels, preds, average='binary', zero_division=0)
        except:
            precision = 0.0
            recall = 0.0
            f1 = 0.0
        
        model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼
        return avg_loss, acc, auc, all_labels, all_probs, precision, recall, f1
    
    @staticmethod
    def calculate_confusion_matrix(labels, probs, threshold=0.5):
        """è®¡ç®—æ··æ·†çŸ©é˜µå’Œè¯¦ç»†æŒ‡æ ‡"""
        
        preds = (probs >= threshold).astype(int)
        cm = confusion_matrix(labels, preds)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        tn, fp, fn, tp = cm.ravel()
        
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        f1_score = 2 * precision * sensitivity / (precision + sensitivity) if (precision + sensitivity) > 0 else 0
        
        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        report = classification_report(labels, preds, target_names=['æ­£å¸¸', 'æˆ¿é¢¤'], output_dict=True, zero_division=0)

        return cm, {
            'sensitivity': sensitivity,
            'specificity': specificity,
            'precision': precision,
            'f1_score': f1_score,
            'classification_report': report
        }


# ==================== æ¨¡å‹è®­ç»ƒå™¨æ¨¡å— ====================
class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨ï¼Œè´Ÿè´£å•æ¬¡è®­ç»ƒéªŒè¯è¿‡ç¨‹"""
    
    def __init__(self, base_path, kernel_config, batch_size, lr, 
                 use_stream2, augment, experiment_base_dir, config_name=None):
        self.base_path = base_path
        self.kernel_config = kernel_config
        self.batch_size = batch_size
        self.lr = lr
        self.use_stream2 = use_stream2
        self.augment = augment
        self.experiment_base_dir = experiment_base_dir
        self.config_name = config_name
        self.models_saved = 0
        self.best_model_path = None

        # æ•°æ®ç®¡ç†å™¨
        self.data_manager = DataManager(base_path)
        
        # è®­ç»ƒçŠ¶æ€
        self.best_f1 = 0.0  # F1åˆ†æ•°è¶Šé«˜è¶Šå¥½
        self.best_loss = float('inf')
        self.best_epoch = 0
        self.current_epoch = 0
        self.patience_counter = 0
        self.patience_counter_f1 = 0
        self.validation_fold = None  # å½“å‰éªŒè¯çš„æŠ˜
        self.training_folds = None   # å½“å‰è®­ç»ƒçš„æŠ˜åˆ—è¡¨
        
    def train_fold(self, train_folds, test_fold, num_epochs):
        """è®­ç»ƒå•ä¸ªæŠ˜"""
        self.validation_fold = test_fold
        self.training_folds = train_folds

        config_name_str = f" ({self.config_name})" if self.config_name else ""
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒé…ç½®{config_name_str}:")
        print(f"  Streamé…ç½®: {'Stream1+2' if self.use_stream2 else 'Stream1 Only'}")
        print(f"  å·ç§¯æ ¸: Stream1={self.kernel_config['stream1_kernel']}, "
              f"Stream2å‰4å±‚={self.kernel_config['stream2_first_kernel']}")
        print(f"  æ•°æ®å¢å¼º: {'å¯ç”¨' if self.augment else 'ç¦ç”¨'}")
        print(f"  æ‰¹å¤§å°: {self.batch_size}, å­¦ä¹ ç‡: {self.lr}")
        print(f"  æµ‹è¯•æŠ˜: cv{test_fold}")
        print(f"{'='*60}")
        
        # 1. å‡†å¤‡æ•°æ®
        train_dataset = ECG_dataset(
            self.base_path, is_train=True, augment=self.augment, cv=train_folds
        )
        test_dataset = ECG_dataset(
            self.base_path, is_train=False, augment=False, cv=test_fold
        )
        
        train_loader = DataLoader(
            train_dataset, batch_size=self.batch_size, 
            shuffle=True, num_workers=0, drop_last=True
        )
        test_loader = DataLoader(test_dataset, batch_size=1)
        
        # 2. åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
        model = Mscnn(
            INPUT_CHANNELS,
            OUTPUT_CLASSES,
            use_stream2=self.use_stream2,
            stream1_kernel=self.kernel_config['stream1_kernel'],
            stream2_first_kernel=self.kernel_config['stream2_first_kernel']
        ).to(device)
        
        criterion = torch.nn.BCELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)
        
        # 3. å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=10,
            min_lr=1e-6
        )
        
        # 4. è®¾ç½®TensorBoardæ—¥å¿—
        log_dir = self._setup_logging(test_fold)
        writer = SummaryWriter(log_dir)
        
        # 5. è®­ç»ƒå¾ªç¯
        fold_results = self._training_loop(
            model, criterion, optimizer, scheduler, 
            train_loader, test_loader, writer, 
            test_fold, num_epochs
        )

        # æ·»åŠ æ¨¡å‹ä¿å­˜ä¿¡æ¯
        fold_results['best_model_path'] = self.best_model_path
        fold_results['total_models_saved'] = self.models_saved
        
        writer.close()
        return fold_results

    def cross_validate_on_train_set(self, train_indices, num_epochs, k_folds=5):
        """
        åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯
        Args:
            train_indices: è®­ç»ƒé›†çš„CVç´¢å¼•åˆ—è¡¨ï¼Œå¦‚[0,1,2,3]
            num_epochs: æ¯ä¸ªæŠ˜çš„è®­ç»ƒepochæ•°
            k_folds: äº¤å‰éªŒè¯çš„æŠ˜æ•°
        Returns:
            å¹³å‡éªŒè¯å‡†ç¡®ç‡ï¼Œå„æŠ˜ç»“æœ
        """
        print(f"\n{'='*60}")
        print(f"åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ {k_folds} æŠ˜äº¤å‰éªŒè¯")
        print(f"è®­ç»ƒé›†æ¥æº: CV{', '.join(map(str, train_indices))}")
        print(f"{'='*60}")
        
        # åˆ›å»ºKæŠ˜åˆ’åˆ†
        kfold_splits = self.data_manager.create_kfold_splits(train_indices, k=k_folds)
        
        fold_results = []
        
        for fold_idx, (train_fold_data, val_fold_data) in enumerate(kfold_splits):
            print(f"\n--- è®­ç»ƒæŠ˜ {fold_idx + 1}/{k_folds} ---")
            
            # ä¸ºè¿™ä¸ªæŠ˜åˆ›å»ºä¸´æ—¶æ•°æ®é›†
            train_dataset = self._create_dataset_from_data(train_fold_data, is_train=True)
            val_dataset = self._create_dataset_from_data(val_fold_data, is_train=False)
            
            # è®­ç»ƒå’ŒéªŒè¯è¿™ä¸ªæŠ˜
            fold_result = self._train_single_fold(
                train_dataset, val_dataset, 
                fold_idx, num_epochs
            )
            
            fold_results.append(fold_result)
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_metrics = self._compute_cv_metrics(fold_results)
        
        return avg_metrics, fold_results    
    
    def _train_single_fold(self, train_df, val_df, fold_idx, num_epochs):
        """è®­ç»ƒå•ä¸ªæŠ˜"""
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TemporaryECGDataset(
            train_df, self.base_path, is_train=True, augment=self.augment
        )
        val_dataset = TemporaryECGDataset(
            val_df, self.base_path, is_train=False, augment=False
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset, batch_size=self.batch_size, 
            shuffle=True, num_workers=0
        )
        val_loader = DataLoader(
            val_dataset, batch_size=1, shuffle=False, num_workers=0
        )
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = Mscnn(
            INPUT_CHANNELS,
            OUTPUT_CLASSES,
            use_stream2=self.use_stream2,
            stream1_kernel=self.kernel_config['stream1_kernel'],
            stream2_first_kernel=self.kernel_config['stream2_first_kernel']
        ).to(device)
        
        criterion = torch.nn.BCELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)
        
        # è®­ç»ƒå¾ªç¯
        best_val_acc = 0
        best_val_auc = 0
        best_val_f1 = 0
        best_epoch = 0
        patience_counter = 0
        
        for epoch in range(1, num_epochs + 1):
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = self._train_epoch(
                model, criterion, optimizer, train_loader
            )
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc, val_auc, _, _, _, val_f1 = ModelValidator.validate(
                model, criterion, val_loader, device
            )
            
            # æ—©åœæ£€æŸ¥
            if val_acc > best_val_acc + MIN_DELTA:
                best_val_acc = val_acc
                best_val_auc = val_auc
                best_val_f1 = val_f1
                best_epoch = epoch
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
                if self.experiment_base_dir:
                    self._save_fold_model(model, fold_idx, epoch, val_acc)
            else:
                patience_counter += 1
            
            # æ‰“å°è¿›åº¦
            if epoch % 10 == 0 or epoch == 1 or epoch == num_epochs:
                print(f"  Epoch {epoch}/{num_epochs}: "
                      f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | "
                      f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
            
            # æ—©åœ
            if patience_counter >= EARLY_STOP_PATIENCE:
                print(f"  æ—©åœè§¦å‘äºç¬¬ {epoch} è½®")
                break
        
        return {
            'fold': fold_idx,
            'best_val_acc': best_val_acc,
            'best_val_auc': best_val_auc,
            'best_val_f1': best_val_f1,
            'best_epoch': best_epoch
        }
    
    def _setup_logging(self, test_fold):
        """è®¾ç½®TensorBoardæ—¥å¿—ç›®å½•"""
        config_name_suffix = f"_{self.config_name}" if self.config_name else ""
        log_dir_relative = (
            f"K{self.kernel_config['stream1_kernel']}_{self.kernel_config['stream2_first_kernel']}_"
            f"BS{self.batch_size}_LR{self.lr}_S{'2' if self.use_stream2 else '1'}_"
            f"{'Aug' if self.augment else 'NoAug'}{config_name_suffix}/Fold_{test_fold}"
        )
        log_dir = os.path.join(
            self.experiment_base_dir, "TensorBoard_Logs", log_dir_relative
        )
        os.makedirs(log_dir, exist_ok=True)
        return log_dir
    
    def _training_loop(self, model, criterion, optimizer, scheduler,
                      train_loader, test_loader, writer, test_fold, num_epochs):
        """è®­ç»ƒå¾ªç¯ä¸»é€»è¾‘"""  

        print(f"\nğŸ›¡ï¸ åŒé‡æ—©åœç­–ç•¥å·²å¯ç”¨:")
        print(f"  - MIN_DELTA: {MIN_DELTA}")
        print(f"  - EARLY_STOP_PATIENCE: {EARLY_STOP_PATIENCE}")
        print(f"  - ç›‘æ§æŒ‡æ ‡: F1åˆ†æ•° + æŸå¤±")

        for epoch in range(1, num_epochs + 1):
            self.current_epoch = epoch  # æ›´æ–°å½“å‰epoch
            print(f'\nFold {test_fold} - Epoch {epoch}/{num_epochs}')
            print(f'ğŸ“Š æ—©åœè®¡æ•°å™¨: {self.patience_counter}/{EARLY_STOP_PATIENCE}')
            print(f'ğŸ† æœ€ä½³F1åˆ†æ•°: {self.best_f1:.4f} (Epoch {self.best_epoch})')
            
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = self._train_epoch(
                model, criterion, optimizer, train_loader, epoch
            )
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc, val_auc, val_labels, val_probs, val_precision, val_recall, val_f1 = ModelValidator.validate(
            model, criterion, test_loader, device
        )
            
            print(f"Fold {test_fold} - Train Loss: {train_loss:.4f} | "
                  f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | "
                  f"Val AUC: {val_auc:.4f} | Val F1: {val_f1:.4f}")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_loss)
            current_lr = optimizer.param_groups[0]['lr']
            writer.add_scalar('Learning_Rate', current_lr, epoch)
            
            # è®°å½•åˆ°TensorBoard
            self._log_to_tensorboard(
            writer, epoch, train_loss, train_acc, 
            val_loss, val_acc, val_auc, val_precision, val_recall, val_f1, val_labels, val_probs
        )
            
           # æ£€æŸ¥æ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹ - ä½¿ç”¨F1åˆ†æ•°
            if self._should_save_model(val_loss, val_f1):
                self.best_f1 = val_f1
                self.best_loss = val_loss
                self.best_epoch = epoch
                self.patience_counter_f1 = 0  # é‡ç½®F1è€å¿ƒè®¡æ•°å™¨
                self._save_best_model(
                    model, val_loss, val_f1, epoch, test_fold, 
                    self.batch_size, self.lr
                )
            else:
                self.patience_counter_f1 += 1

            # æ—©åœæ£€æŸ¥
            if self._should_early_stop(current_val_f1=val_f1):
                break
        
        return {
            'fold': test_fold,
            'best_loss': self.best_loss,
            'best_epoch': self.best_epoch,
            'final_val_acc': val_acc,
            'final_val_auc': val_auc,
            'final_val_precision': val_precision,
            'final_val_recall': val_recall,
            'final_val_f1': val_f1,
            'val_labels': val_labels,
            'val_probs': val_probs
    }
    
    def _train_epoch(self, model, criterion, optimizer, train_loader):
        """è®­ç»ƒå•ä¸ªepoch"""
        model.train()
        train_loss = 0.0
        all_preds = []
        all_labels = []
        
        for x, y, _ in train_loader:
            x = x.to(device).float()
            x = torch.reshape(x, (-1, 1, FIXED_LENGTH))
            y = y.to(device).float()
            
            optimizer.zero_grad()
            outputs = model(x)
            loss = criterion(outputs, y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            preds = (outputs.detach().cpu().numpy() >= 0.5).astype(int)
            all_preds.extend(preds.flatten())
            all_labels.extend(y.detach().cpu().numpy().flatten())
        
        # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
        train_acc = accuracy_score(all_labels, all_preds)
        avg_loss = train_loss / len(train_loader)
        
        return avg_loss, train_acc
    
    def _log_to_tensorboard(self, writer, epoch, train_loss, train_acc,
                           val_loss, val_acc,val_auc, val_precision, val_recall, val_f1, val_labels, val_probs):
        """è®°å½•è®­ç»ƒä¿¡æ¯åˆ°TensorBoard - ä¿®æ”¹åç‰ˆæœ¬"""
        # æ ‡é‡è®°å½•
        writer.add_scalar('Loss/Train', train_loss, epoch)
        writer.add_scalar('Loss/Val', val_loss, epoch)
        writer.add_scalar('Accuracy/Train', train_acc, epoch)
        writer.add_scalar('Accuracy/Val', val_acc, epoch)
        writer.add_scalar('Metric/AUC', val_auc, epoch)
        writer.add_scalar('Metric/Precision', val_precision, epoch)
        writer.add_scalar('Metric/Recall', val_recall, epoch)
        writer.add_scalar('Metric/F1', val_f1, epoch)
    
        # ROCæ›²çº¿è®°å½•ï¼ˆæ¯10ä¸ªepochè®°å½•ä¸€æ¬¡ä»¥èŠ‚çœç©ºé—´ï¼‰
        if epoch % 10 == 0 or epoch == 1:
            fig, _ = TrainingUtils.plot_roc_curve(val_labels, val_probs, epoch)
            writer.add_figure('ROC_Curve', fig, epoch)
            plt.close(fig)
    
    def _should_save_model(self, val_loss,val_f1):
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿å­˜æ¨¡å‹
        æ ‡å‡†ï¼šF1åˆ†æ•°æœ‰æ˜¾è‘—æå‡
        """
        if val_f1 > self.best_f1 + MIN_DELTA:
            print(f"ğŸ¯ F1åˆ†æ•°ä» {self.best_f1:.4f} æ”¹è¿›åˆ° {val_f1:.4f}ï¼Œä¿å­˜æ¨¡å‹")
            self.best_f1 = val_f1
            self.best_loss = val_loss  # åŒæ—¶è®°å½•æœ€ä½³æŸå¤±
            self.patience_counter = 0
            return True
        return False
    
    def _save_best_model(self, model, val_loss, val_f1, epoch, fold_idx, 
                     is_cv_fold=True, batch_size=None, lr=None):
        """
        é€šç”¨æ¨¡å‹ä¿å­˜æ–¹æ³•ï¼Œæ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        - is_cv_fold=True: ä¿å­˜äº¤å‰éªŒè¯çš„æŠ˜æ¨¡å‹
        - is_cv_fold=False: ä¿å­˜æœ€ç»ˆæ¨¡å‹
        """
        if is_cv_fold:
            # ä¿å­˜äº¤å‰éªŒè¯æŠ˜æ¨¡å‹
            model_filename = f"cv_fold{fold_idx}_epoch{epoch}_f1{val_f1:.4f}.pth"
            save_dir = os.path.join(self.experiment_base_dir, "CV_Fold_Models")
        else:
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_filename = f"final_model_{timestamp}_f1{val_f1:.4f}.pth"
            save_dir = os.path.join(self.experiment_base_dir, "Final_Models")
        
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, model_filename)
        
        # ä¿å­˜æ¨¡å‹
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'val_loss': val_loss,
            'val_f1': val_f1,
            'fold_idx': fold_idx if is_cv_fold else None,
            'config': {
                'kernel_config': self.kernel_config,
                'batch_size': batch_size or self.batch_size,
                'lr': lr or self.lr,
                'use_stream2': self.use_stream2,
                'augment': self.augment,
                'is_cv_fold': is_cv_fold
            }
        }, save_path)
        
        print(f"æ¨¡å‹å·²ä¿å­˜: {save_path}")
        return save_path

    def _generate_standard_model_name(self, fold, epoch, loss, f1_score=None, is_best=False):
        """ç”Ÿæˆæ ‡å‡†åŒ–çš„æ¨¡å‹æ–‡ä»¶å"""
        # åŸºç¡€ç»„ä»¶
        kernel_tag = f"K{self.kernel_config['stream1_kernel']}{self.kernel_config['stream2_first_kernel']}"
        stream_tag = "S2" if self.use_stream2 else "S1"
        augment_tag = "Aug" if self.augment else "NoAug"
        config_tag = f"{self.config_name}" if self.config_name else ""

        # ç¡®ä¿epochæ˜¯æ•´æ•°
        epoch_int = int(epoch)  # å¼ºåˆ¶è½¬æ¢ä¸ºæ•´æ•°
        
        # æ„å»ºæ–‡ä»¶åï¼ˆæŒ‰é€»è¾‘é¡ºåºï¼‰
        parts = [
            "Mscnn",
            kernel_tag,
            stream_tag,
            augment_tag,
            f"F{fold}",
            f"E{epoch_int:03d}",
            f"L{loss:.4f}".replace('.', 'p'),
        ]
        
        # å¦‚æœæœ‰F1åˆ†æ•°ï¼Œæ·»åŠ åˆ°æ–‡ä»¶åä¸­
        if f1_score is not None:
            parts.append(f"F1{f1_score:.4f}".replace('.', 'p'))

        if is_best:
            parts.append("BEST")
        
        parts.append(config_tag)
        
        # è¿‡æ»¤ç©ºéƒ¨åˆ†å¹¶è¿æ¥
        filename = "_".join(filter(None, parts)) + ".pth"

        # è¿‡æ»¤ç©ºéƒ¨åˆ†å¹¶è¿æ¥
        filename = "_".join(filter(None, parts)) + ".pth"
        
        return filename
    
    def _clean_old_models_fold(self, fold_models_dir, fold):
        """æ¸…ç†æ—§çš„åŒæŠ˜æ¨¡å‹æ–‡ä»¶ - åªä¿ç•™æœ€æ–°æœ€ä½³"""
        try:
            # è·å–ç›®å½•ä¸­æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
            pattern = os.path.join(fold_models_dir, f"*_F{fold}_*.pth")
            old_files = glob(pattern)
            
            if len(old_files) > 0:
                print(f"ğŸ”„ æ¸…ç†ç¬¬{fold}æŠ˜æ—§æ¨¡å‹ï¼Œä¿ç•™æœ€æ–°æœ€ä½³...")
                for f in old_files:
                    try:
                        os.remove(f)
                        print(f"   ğŸ—‘ï¸ åˆ é™¤: {os.path.basename(f)}")
                    except Exception as e:
                        print(f"   æ¸…ç†å¤±è´¥: {e}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ—§æ¨¡å‹æ—¶å‡ºé”™: {e}")
        
    
    def _should_early_stop(self, current_val_f1):
        """
        å•æŒ‡æ ‡æ—©åœç­–ç•¥ï¼šä»…ç›‘æ§F1åˆ†æ•°
        
        Args:
            current_val_f1: å½“å‰éªŒè¯F1åˆ†æ•°
        
        Returns:
            bool: æ˜¯å¦è§¦å‘æ—©åœ
        """
        
        # æ£€æŸ¥F1åˆ†æ•°æ˜¯å¦æœ‰æ”¹è¿›
        if current_val_f1 > self.best_f1 + MIN_DELTA:
            print(f"ğŸ¯ F1åˆ†æ•°æ”¹è¿›: {self.best_f1:.4f} â†’ {current_val_f1:.4f}ï¼Œé‡ç½®è€å¿ƒè®¡æ•°å™¨")
            self.best_f1 = current_val_f1
            self.best_epoch = self.current_epoch  # éœ€è¦ä¿å­˜å½“å‰epoch
            self.patience_counter = 0
            return False
        else:
            self.patience_counter += 1
            print(f"ğŸ”„ F1åˆ†æ•°æ— æ”¹è¿› ({self.best_f1:.4f})ï¼Œè€å¿ƒè®¡æ•°å™¨: {self.patience_counter}/{EARLY_STOP_PATIENCE}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ—©åœæ¡ä»¶
            if self.patience_counter >= EARLY_STOP_PATIENCE:
                print(f"\nğŸš¨ æ—©åœè§¦å‘!")
                print(f"   è¿ç»­ {EARLY_STOP_PATIENCE} ä¸ªepoch F1åˆ†æ•°æ²¡æœ‰æ˜¾è‘—æ”¹è¿›")
                print(f"   æœ€ä½³epoch: {self.best_epoch}, æœ€ä½³F1åˆ†æ•°: {self.best_f1:.4f}")
                print(f"   æœ€ç»ˆF1åˆ†æ•°: {current_val_f1:.4f}")
                return True
            
            return False

# ==================== å®Œæ•´è®­ç»ƒæ¨¡å— ====================
class CompleteTrainer:
    """ä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ"""
    
    @staticmethod
    def train_with_best_config(base_path, best_config_data, num_epochs):
        """
        ä½¿ç”¨æœç´¢å¾—åˆ°çš„æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒï¼ˆ5æŠ˜äº¤å‰éªŒè¯ï¼‰
        """
        print("=" * 80)
        print("ä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ")
        print("=" * 80)
        
        kernel_config = best_config_data['kernel_config']
        batch_size = best_config_data['batch_size']
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = f"/home/xusi/Logs/FinalTraining/Results_{timestamp}"
        os.makedirs(results_dir, exist_ok=True)
        
        print(f"æœ€ä½³é…ç½®: {kernel_config['name']}, æ‰¹å¤§å°: {batch_size}")
        print(f"å°†è¿›è¡Œå®Œæ•´çš„5æŠ˜äº¤å‰éªŒè¯ï¼Œæ¯ä¸ªæŠ˜è®­ç»ƒ{num_epochs}ä¸ªepoch")
        print(f"ğŸ“ ç»“æœç›®å½•: {results_dir}")
        
        # K-Foldè®­ç»ƒ
        K = 5
        fold_results = []
        
        for k in range(K):
            test_fold = k
            train_folds = [i for i in range(5) if i != k]
            
            print(f"\n{'='*60}")
            print(f"ğŸ“ˆ è®­ç»ƒæŠ˜ {k + 1}/{K} (æµ‹è¯•æŠ˜: cv{test_fold})")
            print(f"{'='*60}")
            
            trainer = ModelTrainer(
                base_path=base_path,
                kernel_config=kernel_config,
                batch_size=batch_size,
                lr=LEARNING_RATE,
                use_stream2=USE_STREAM2_SETTING,
                augment=AUGMENT_SETTING,
                experiment_base_dir=results_dir,
                config_name=kernel_config['name']
            )
            
            result = trainer.train_fold(
                train_folds=train_folds,
                test_fold=test_fold,
                num_epochs=num_epochs
            )
            
            fold_results.append(result)
            
            print(f"\nğŸ“Š ç¬¬{k+1}æŠ˜ç»“æœ:")
            print(f"  âœ… æœ€ä½³Loss: {result['best_loss']:.4f}")
            print(f"  âœ… éªŒè¯å‡†ç¡®ç‡: {result['final_val_acc']:.4f}")
            print(f"  âœ… AUC: {result['final_val_auc']:.4f}")
            print(f"  âœ… ç²¾ç¡®ç‡: {result['final_val_precision']:.4f}")
            print(f"  âœ… å¬å›ç‡: {result['final_val_recall']:.4f}")
            print(f"  âœ… F1åˆ†æ•°: {result['final_val_f1']:.4f}")
            print(f"  ğŸ’¾ æ¨¡å‹ä¿å­˜: {result.get('best_model_path', 'N/A')}")

        # ç»„ç»‡æ¨¡å‹æ–‡ä»¶
        models_dir = ModelManager.organize_model_files(results_dir, fold_results)

        # å¯é€‰ï¼šåˆ›å»ºé›†æˆæ¨¡å‹
        ensemble_path = ModelManager.create_final_ensemble_model(fold_results, results_dir, kernel_config)
        
        # è®¡ç®—å®Œæ•´è®­ç»ƒçš„å¹³å‡ç»“æœ
        avg_metrics = CompleteTrainer._compute_final_metrics(fold_results)
        
        # ä¿å­˜ç»“æœ
        final_results, simplified_results = CompleteTrainer._save_final_results(
            kernel_config, batch_size, fold_results, 
            avg_metrics, results_dir, ensemble_path
        )

        # æ‰“å°è¯¦ç»†çš„æ–‡ä»¶ä½ç½®ä¿¡æ¯
        CompleteTrainer._print_file_locations(results_dir, final_results, simplified_results)
        
        return avg_metrics,final_results,simplified_results
    
    @staticmethod
    def _print_file_locations(results_dir, final_results, simplified_results):
        """æ‰“å°æ–‡ä»¶ä¿å­˜ä½ç½®ä¿¡æ¯"""
        print("\n" + "="*80)
        print("ğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®")
        print("="*80)
        
        print(f"ğŸ  ä¸»ç»“æœç›®å½•: {results_dir}")
        
        # æ£€æŸ¥å¹¶åˆ—å‡ºæ‰€æœ‰é‡è¦æ–‡ä»¶
        important_files = {
            "å®Œæ•´è¯„ä¼°ç»“æœ": os.path.join(results_dir, "final_results.json"),
            "LLMå…¼å®¹ç»“æœ": os.path.join(results_dir, "cnn_evaluation_results.json"),
            "æ¨¡å‹ç›®å½•": os.path.join(results_dir, "Models"),
            "TensorBoardæ—¥å¿—": os.path.join(results_dir, "TensorBoard_Logs"),
            "æœ€ä½³æ¨¡å‹": os.path.join(results_dir, "Models", "Best_Models")
        }
        
        for desc, path in important_files.items():
            if os.path.exists(path):
                file_type = "ğŸ“ ç›®å½•" if os.path.isdir(path) else "ğŸ“„ æ–‡ä»¶"
                print(f"{file_type} {desc}: {path}")
            else:
                print(f"âŒ ç¼ºå¤±: {desc} ({path})")
        
        print(f"\nğŸ“Š CNNæ¨¡å‹è¯„ä¼°æŒ‡æ ‡:")
        print(f"  å‡†ç¡®ç‡: {final_results['accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡: {final_results['precision']:.4f}")
        print(f"  å¬å›ç‡: {final_results['recall']:.4f}")
        print(f"  F1åˆ†æ•°: {final_results['f1']:.4f}")
        print(f"  AUC: {final_results['auc']:.4f}")
        
        print(f"\nğŸ’¡ LLMå¯¹æ¯”æ–‡ä»¶: {os.path.join(results_dir, 'cnn_evaluation_results.json')}")
        print("   å¯ä»¥ç›´æ¥åœ¨LLMè¯„ä¼°ä»£ç ä¸­ä½¿ç”¨æ­¤æ–‡ä»¶è¿›è¡Œå¯¹æ¯”")
    
    @staticmethod
    def _compute_final_metrics(fold_results):
        """è®¡ç®—æœ€ç»ˆæŒ‡æ ‡"""
        avg_loss = np.mean([r['best_loss'] for r in fold_results])
        avg_acc = np.mean([r['final_val_acc'] for r in fold_results])
        avg_auc = np.mean([r['final_val_auc'] for r in fold_results])
        avg_precision = np.mean([r['final_val_precision'] for r in fold_results])
        avg_recall = np.mean([r['final_val_recall'] for r in fold_results])
        avg_f1 = np.mean([r['final_val_f1'] for r in fold_results])
        
        std_acc = np.std([r['final_val_acc'] for r in fold_results])
        std_auc = np.std([r['final_val_auc'] for r in fold_results])
        std_precision = np.std([r['final_val_precision'] for r in fold_results])
        std_recall = np.std([r['final_val_recall'] for r in fold_results])
        std_f1 = np.std([r['final_val_f1'] for r in fold_results])
        
        print("\n" + "=" * 80)
        print("å®Œæ•´è®­ç»ƒç»“æœæ±‡æ€»:")
        print("=" * 80)
        print(f"å¹³å‡Loss: {avg_loss:.4f}")
        print(f"å¹³å‡å‡†ç¡®ç‡: {avg_acc:.4f} Â± {std_acc:.4f}")
        print(f"å¹³å‡AUC: {avg_auc:.4f} Â± {std_auc:.4f}")
        print(f"å¹³å‡ç²¾ç¡®ç‡: {avg_precision:.4f} Â± {std_precision:.4f}")
        print(f"å¹³å‡å¬å›ç‡: {avg_recall:.4f} Â± {std_recall:.4f}")
        print(f"å¹³å‡F1åˆ†æ•°: {avg_f1:.4f} Â± {std_f1:.4f}")
        
        for k, result in enumerate(fold_results):
            print(f"æŠ˜ {k}: å‡†ç¡®ç‡={result['final_val_acc']:.4f}, "
                  f"AUC={result['final_val_auc']:.4f}, "
                  f"ç²¾ç¡®ç‡={result['final_val_precision']:.4f}, "
                  f"å¬å›ç‡={result['final_val_recall']:.4f}, "
                  f"F1={result['final_val_f1']:.4f}")
        
        return {
            'avg_loss': float(avg_loss),
            'avg_accuracy': float(avg_acc),
            'std_accuracy': float(std_acc),
            'avg_auc': float(avg_auc),
            'std_auc': float(std_auc),
            'avg_precision': float(avg_precision),
            'std_precision': float(std_precision),
            'avg_recall': float(avg_recall),
            'std_recall': float(std_recall),
            'avg_f1': float(avg_f1),
            'std_f1': float(std_f1)
        }
    
    @staticmethod
    def _save_final_results(kernel_config, batch_size, fold_results, 
                           avg_metrics, results_dir, ensemble_path=None):
        """ä¿å­˜æœ€ç»ˆè®­ç»ƒç»“æœ - ç”Ÿæˆå…¼å®¹LLMè¯„ä¼°çš„JSONæ ¼å¼"""
        
        # åˆ›å»ºä¸LLMè¯„ä¼°å…¼å®¹çš„JSONç»“æ„
        final_results = {
            'best_config': {
                'kernel_config': {
                    'name': kernel_config['name'],
                    'stream1_kernel': kernel_config['stream1_kernel'],
                    'stream2_first_kernel': kernel_config['stream2_first_kernel'],
                    'ch_in': 1,
                    'ch_out': 1,
                    'use_stream2': USE_STREAM2_SETTING
                },
                'batch_size': batch_size
            },
            
            # ä¸»è¦æ€§èƒ½æŒ‡æ ‡ï¼ˆä¸LLMè¯„ä¼°ç›¸åŒï¼‰
            'accuracy': avg_metrics['avg_accuracy'],
            'precision': avg_metrics['avg_precision'],
            'recall': avg_metrics['avg_recall'],
            'f1': avg_metrics['avg_f1'],
            'auc': avg_metrics['avg_auc'],
            
            # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            'final_metrics': avg_metrics,
            
            # äº¤å‰éªŒè¯ç»“æœï¼ˆæ¯ä¸ªfoldï¼‰
            'cross_validation_results': {
                f'cv{k}': {
                    'accuracy': float(r['final_val_acc']),
                    'precision': float(r['final_val_precision']),
                    'recall': float(r['final_val_recall']),
                    'f1': float(r['final_val_f1']),
                    'auc': float(r['final_val_auc'])
                } for k, r in enumerate(fold_results)
            },
            
            # åŸå§‹foldç»“æœï¼ˆä¿æŒåŸæœ‰æ ¼å¼ï¼‰
            'fold_results': [
                {
                    'fold': r['fold'],
                    'best_loss': float(r['best_loss']),
                    'best_epoch': r['best_epoch'],
                    'final_val_acc': float(r['final_val_acc']),
                    'final_val_auc': float(r['final_val_auc']),
                    'final_val_precision': float(r['final_val_precision']),
                    'final_val_recall': float(r['final_val_recall']),
                    'final_val_f1': float(r['final_val_f1'])
                } for r in fold_results
            ],
            
            'training_timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
            'model_name': 'ECG_CNN',
            'dataset': 'training2017'
        }
        
        # ä¿å­˜ä¸¤ä¸ªç‰ˆæœ¬ï¼šå®Œæ•´ç‰ˆæœ¬å’Œç®€åŒ–ç‰ˆæœ¬ï¼ˆç”¨äºLLMå¯¹æ¯”ï¼‰
        results_path = os.path.join(results_dir, 'final_results.json')
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜ç®€åŒ–ç‰ˆæœ¬
        simplified_results = {
            'best_config': final_results['best_config'],
            'accuracy': final_results['accuracy'],
            'precision': final_results['precision'],
            'recall': final_results['recall'],
            'f1': final_results['f1'],
            'auc': final_results['auc'],
            'cross_validation_results': final_results['cross_validation_results'],
            'model_files_location': os.path.join(results_dir, "Models")
        }
        
        simplified_path = os.path.join(results_dir, 'cnn_evaluation_results.json')
        with open(simplified_path, 'w', encoding='utf-8') as f:
            json.dump(simplified_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nå®Œæ•´è®­ç»ƒç»“æœå·²ä¿å­˜: {results_path}")
        print(f"LLMå…¼å®¹ç»“æœå·²ä¿å­˜: {simplified_path}")
        
        return final_results,simplified_path

    
# ==================== è¶…å‚æ•°æœç´¢æ¨¡å— ====================
class HyperparameterSearcher:
    """è¶…å‚æ•°æœç´¢å™¨"""
    
    # æœç´¢é…ç½®
    KERNEL_CONFIGS = [
        {'name': 'MS-CNN(3,9)', 'stream1_kernel': 3, 'stream2_first_kernel': 9},
        {'name': 'MS-CNN(3,7)', 'stream1_kernel': 3, 'stream2_first_kernel': 7},
        {'name': 'MS-CNN(3,5)', 'stream1_kernel': 3, 'stream2_first_kernel': 5},
        {'name': 'MS-CNN(3,3)', 'stream1_kernel': 3, 'stream2_first_kernel': 3},
    ]
    
    BATCH_SIZES = [32, 64, 128]
    
    def __init__(self, base_path):
        self.base_path = base_path
    
    def search(self, num_folds=3, num_epochs_search=15):
        """æ‰§è¡Œè¶…å‚æ•°æœç´¢"""
        print("=" * 80)
        print("å¼€å§‹æ™ºèƒ½è¶…å‚æ•°æœç´¢")
        print(f"å°†æµ‹è¯• {len(self.KERNEL_CONFIGS)} ç§å·ç§¯æ ¸é…ç½® Ã— "
              f"{len(self.BATCH_SIZES)} ç§æ‰¹å¤§å°")
        print(f"å…± {len(self.KERNEL_CONFIGS) * len(self.BATCH_SIZES)} ç§ç»„åˆ")
        print(f"å°†åœ¨ {num_folds} æŠ˜æ•°æ®ä¸Šå¿«é€Ÿè¯„ä¼°ï¼Œ"
              f"æ¯ç§ç»„åˆè®­ç»ƒ {num_epochs_search} ä¸ªepoch")
        print("=" * 80)
        
        # åˆ›å»ºç»“æœå­˜å‚¨ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        search_dir = f"/home/xusi/Logs/HyperparamSearch/Results_{timestamp}"
        os.makedirs(search_dir, exist_ok=True)
        
        all_results = {}
        
        # å¾ªç¯æ‰€æœ‰è¶…å‚æ•°ç»„åˆ
        for kernel_config in self.KERNEL_CONFIGS:
            for batch_size in self.BATCH_SIZES:
                config_results = self._evaluate_config(
                    kernel_config, batch_size, 
                    num_folds, num_epochs_search, search_dir
                )
                
                # è®¡ç®—å¹³å‡æ€§èƒ½
                avg_metrics = self._compute_average_metrics(config_results)
                
                all_results[avg_metrics['config_name']] = {
                    'kernel_config': kernel_config,
                    'batch_size': batch_size,
                    **avg_metrics,
                    'fold_results': config_results
                }
        
        # ä¿å­˜å’Œåˆ†æç»“æœ
        self._save_and_analyze_results(all_results, search_dir)
        return all_results, search_dir
    
    def _evaluate_config(self, kernel_config, batch_size, 
                        num_folds, num_epochs_search, search_dir):
        """è¯„ä¼°å•ä¸ªé…ç½®"""
        config_name = f"{kernel_config['name']}_BS{batch_size}"
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•é…ç½®: {config_name}")
        print(f"å·ç§¯æ ¸: Stream1={kernel_config['stream1_kernel']}, "
              f"Stream2å‰4å±‚={kernel_config['stream2_first_kernel']}")
        print(f"æ‰¹å¤§å°: {batch_size}")
        print(f"{'='*60}")
        
        config_results = []
        
        for fold_idx in range(num_folds):
            test_fold = fold_idx
            train_folds = [i for i in range(5) if i != test_fold]
            
            print(f"\næŠ˜ {fold_idx + 1}/{num_folds}")
            
            # è®­ç»ƒå’ŒéªŒè¯
            trainer = ModelTrainer(
                base_path=self.base_path,
                kernel_config=kernel_config,
                batch_size=batch_size,
                lr=LEARNING_RATE,
                use_stream2=USE_STREAM2_SETTING,
                augment=AUGMENT_SETTING,
                experiment_base_dir=search_dir
            )
            
            result = trainer.train_fold(
                train_folds=train_folds,
                test_fold=test_fold,
                num_epochs=num_epochs_search
            )
            
            config_results.append(result)
            
            print(f"  æœ€ä½³Loss: {result['best_loss']:.4f}, "
                  f"éªŒè¯å‡†ç¡®ç‡: {result['final_val_acc']:.4f}, "
                  f"AUC: {result['final_val_auc']:.4f}")
        
        return config_results
    
    def _compute_average_metrics(self, config_results):
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        avg_loss = np.mean([r['best_loss'] for r in config_results])
        avg_acc = np.mean([r['final_val_acc'] for r in config_results])
        avg_auc = np.mean([r['final_val_auc'] for r in config_results])
        
        return {
            'config_name': f"{config_results[0]['fold']}_avg",
            'avg_loss': float(avg_loss),
            'avg_accuracy': float(avg_acc),
            'avg_auc': float(avg_auc)
        }
    
    def _save_and_analyze_results(self, all_results, search_dir):
        """ä¿å­˜å’Œåˆ†ææœç´¢ç»“æœ"""
        self._save_results(all_results, search_dir)
        self._create_visual_report(all_results, search_dir)
        self._find_best_configuration(all_results, search_dir)
    
    def _save_results(self, all_results, search_dir):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        # JSONæ ¼å¼è¯¦ç»†ç»“æœ
        detailed_path = os.path.join(search_dir, 'detailed_results.json')
        with open(detailed_path, 'w') as f:
            json.dump(all_results, f, indent=2, default=str)
        
        # CSVæ ¼å¼æ‘˜è¦
        summary_data = []
        for config_name, data in all_results.items():
            summary_data.append({
                'Config': config_name,
                'Kernel': f"{data['kernel_config']['stream1_kernel']}_"
                         f"{data['kernel_config']['stream2_first_kernel']}",
                'Batch_Size': data['batch_size'],
                'Avg_Loss': data['avg_loss'],
                'Avg_Accuracy': data['avg_accuracy'],
                'Avg_AUC': data['avg_auc']
            })
        
        df = pd.DataFrame(summary_data)
        summary_path = os.path.join(search_dir, 'summary.csv')
        df.to_csv(summary_path, index=False)
        
        print(f"\næœç´¢ç»“æœå·²ä¿å­˜åˆ°: {search_dir}")
    
    def _create_visual_report(self, all_results, search_dir):
        """åˆ›å»ºå¯è§†åŒ–æŠ¥å‘Š"""
        try:
            config_names = list(all_results.keys())
            accuracies = [all_results[name]['avg_accuracy'] for name in config_names]
            auc_scores = [all_results[name]['avg_auc'] for name in config_names]
            
            fig, axes = plt.subplots(2, 1, figsize=(12, 10))
            
            # å‡†ç¡®ç‡æŸ±çŠ¶å›¾
            x = range(len(config_names))
            axes[0].bar(x, accuracies, color='skyblue', edgecolor='black')
            axes[0].set_ylabel('Accuracy')
            axes[0].set_title('è¶…å‚æ•°æœç´¢ç»“æœ - å‡†ç¡®ç‡')
            axes[0].set_xticks(x)
            axes[0].set_xticklabels(config_names, rotation=45, ha='right')
            axes[0].grid(True, alpha=0.3)
            
            # AUCæŸ±çŠ¶å›¾
            axes[1].bar(x, auc_scores, color='lightgreen', edgecolor='black')
            axes[1].set_xlabel('é…ç½®')
            axes[1].set_ylabel('AUC')
            axes[1].set_title('è¶…å‚æ•°æœç´¢ç»“æœ - AUC')
            axes[1].set_xticks(x)
            axes[1].set_xticklabels(config_names, rotation=45, ha='right')
            axes[1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_path = os.path.join(search_dir, 'search_results_chart.png')
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {chart_path}")
            
        except ImportError:
            print("æ³¨æ„: matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆã€‚")
    
    def _find_best_configuration(self, all_results, search_dir):
        """æ‰¾å‡ºæœ€ä½³è¶…å‚æ•°é…ç½®"""
        sorted_configs = sorted(
            all_results.items(),
            key=lambda x: x[1]['avg_accuracy'],
            reverse=True
        )
        
        print("\n" + "=" * 80)
        print("è¶…å‚æ•°æœç´¢æ’åç»“æœ:")
        print("=" * 80)
        
        best_config_name, best_config_data = sorted_configs[0]
        
        for rank, (config_name, data) in enumerate(sorted_configs, 1):
            marker = " â˜…" if rank == 1 else ""
            print(f"{rank:2d}. {config_name:30s} "
                  f"å‡†ç¡®ç‡: {data['avg_accuracy']:.4f} | "
                  f"AUC: {data['avg_auc']:.4f} | "
                  f"Loss: {data['avg_loss']:.4f}{marker}")
        
        print("\n" + "=" * 80)
        print("æœ€ä½³é…ç½®:")
        print("=" * 80)
        print(f"é…ç½®åç§°: {best_config_name}")
        print(f"å·ç§¯æ ¸: Stream1={best_config_data['kernel_config']['stream1_kernel']}, "
              f"Stream2å‰4å±‚={best_config_data['kernel_config']['stream2_first_kernel']}")
        print(f"æ‰¹å¤§å°: {best_config_data['batch_size']}")
        print(f"å¹³å‡å‡†ç¡®ç‡: {best_config_data['avg_accuracy']:.4f}")
        print(f"å¹³å‡AUC: {best_config_data['avg_auc']:.4f}")
        print(f"å¹³å‡Loss: {best_config_data['avg_loss']:.4f}")
        
        # ä¿å­˜æœ€ä½³é…ç½®
        best_config_path = os.path.join(search_dir, 'best_config.json')
        with open(best_config_path, 'w') as f:
            json.dump({
                'best_config_name': best_config_name,
                'best_config_data': best_config_data,
                'search_timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }, f, indent=2)
        
        return best_config_name, best_config_data



# ==================== å¯¹æ¯”å®éªŒæ¨¡å— ====================
class ComparisonExperiment:
    """å¯¹æ¯”å®éªŒæ§åˆ¶å™¨"""
    
    def __init__(self, base_path, comparison_mode='stream'):
        """
        åˆå§‹åŒ–å¯¹æ¯”å®éªŒ
        Args:
            base_path: æ•°æ®è·¯å¾„
            comparison_mode: 'stream'å¯¹æ¯”streamé…ç½®, 'augment'å¯¹æ¯”æ•°æ®å¢å¼º
        """
        self.base_path = base_path
        self.comparison_mode = comparison_mode
        
        # æ ¹æ®å¯¹æ¯”æ¨¡å¼é€‰æ‹©é…ç½®
        if comparison_mode == 'stream':
            self.comparison_configs = STREAM_COMPARISON_CONFIGS
            self.experiment_name = "Stream_Comparison"
            self.title_prefix = "Streamé…ç½®å¯¹æ¯”: "
        elif comparison_mode == 'augment':
            self.comparison_configs = AUGMENTATION_COMPARISON_CONFIGS
            self.experiment_name = "Augmentation_Comparison"
            self.title_prefix = "æ•°æ®å¢å¼ºå¯¹æ¯”: "
        else:
            raise ValueError(f"æœªçŸ¥çš„å¯¹æ¯”æ¨¡å¼: {comparison_mode}")
        
        # åˆ›å»ºç»“æœç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results_dir = f"/home/xusi/Logs/Comparison/{self.experiment_name}_{timestamp}"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.models_dir = os.path.join(self.results_dir, "Models")
        self.metrics_dir = os.path.join(self.results_dir, "Metrics")
        self.plots_dir = os.path.join(self.results_dir, "Plots")
        self.tensorboard_dir = os.path.join(self.results_dir, "TensorBoard")
        
        for dir_path in [self.models_dir, self.metrics_dir, self.plots_dir, self.tensorboard_dir]:
            os.makedirs(dir_path, exist_ok=True)
        
        # å­˜å‚¨ç»“æœ
        self.comparison_results = {}
        
        print(f"å¯¹æ¯”å®éªŒç›®å½•: {self.results_dir}")
    
    def run_comparison(self, num_folds=5, num_epochs=NUM_EPOCHS):
        """è¿è¡Œå¯¹æ¯”å®éªŒ"""
        print("\n" + "="*80)
        print(f"å¼€å§‹å¯¹æ¯”å®éªŒ: {self.title_prefix}")
        print("="*80)
        
        # è¿è¡Œæ¯ç§é…ç½®
        for config in self.comparison_configs:
            config_name = config['name']
            print(f"\n{'='*60}")
            print(f"è®­ç»ƒé…ç½®: {config_name}")
            print(f"æè¿°: {config['description']}")
            print(f"{'='*60}")
            
            # è®­ç»ƒè¯¥é…ç½®
            fold_results = self._train_configuration(
                config=config,
                num_folds=num_folds,
                num_epochs=num_epochs
            )
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            avg_metrics = self._calculate_average_metrics(fold_results)
            
            # å­˜å‚¨ç»“æœ
            self.comparison_results[config_name] = {
                'config': config,
                'fold_results': fold_results,
                'average_metrics': avg_metrics
            }
        
        # ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š
        self._generate_comparison_report()
        
        # ç”Ÿæˆç®€å•çš„å¯è§†åŒ–å¯¹æ¯”ç»“æœ
        self._create_comparison_visualizations()
        
        # ä¿å­˜å®Œæ•´çš„å®éªŒç»“æœ
        self._save_experiment_results()
        
        print(f"\nå¯¹æ¯”å®éªŒå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {self.results_dir}")
        
        return self.comparison_results
    
    def _train_configuration(self, config, num_folds, num_epochs):
        """è®­ç»ƒç‰¹å®šé…ç½®ï¼ˆ5æŠ˜äº¤å‰éªŒè¯ï¼‰"""
        fold_results = []
        
        for fold_idx in range(num_folds):
            print(f"\n--- è®­ç»ƒæŠ˜ {fold_idx+1}/{num_folds} ---")
            
            # å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æŠ˜
            test_fold = fold_idx
            train_folds = [i for i in range(num_folds) if i != fold_idx]
            
            # æ ¹æ®å¯¹æ¯”æ¨¡å¼è®¾ç½®å‚æ•°
            if self.comparison_mode == 'stream':
                use_stream2 = config['use_stream2']
                augment = AUGMENT_SETTING  # ä½¿ç”¨å…¨å±€è®¾ç½®
            else:  # augmentæ¨¡å¼
                use_stream2 = USE_STREAM2_SETTING  # ä½¿ç”¨å…¨å±€è®¾ç½®
                augment = config['augment']
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = ModelTrainer(
                base_path=self.base_path,
                kernel_config=DEFAULT_KERNEL_CONFIG,
                batch_size=BATCH_SIZE,
                lr=LEARNING_RATE,
                use_stream2=use_stream2,
                augment=augment,
                experiment_base_dir=self.results_dir,
                config_name=config['name']
            )
            
            # è®­ç»ƒå•ä¸ªæŠ˜
            fold_result = trainer.train_fold(
                train_folds=train_folds,
                test_fold=test_fold,
                num_epochs=num_epochs
            )
            
            fold_results.append(fold_result)
            
            print(f"æŠ˜ {fold_idx+1} ç»“æœ: "
                  f"å‡†ç¡®ç‡={fold_result['final_val_acc']:.4f}, "
                  f"AUC={fold_result['final_val_auc']:.4f}")
        
        return fold_results
    
    def _calculate_average_metrics(self, fold_results):
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        accuracies = [r['final_val_acc'] for r in fold_results]
        auc_scores = [r['final_val_auc'] for r in fold_results]
        losses = [r['best_loss'] for r in fold_results]
        
        return {
            'mean_accuracy': float(np.mean(accuracies)),
            'std_accuracy': float(np.std(accuracies)),
            'mean_auc': float(np.mean(auc_scores)),
            'std_auc': float(np.std(auc_scores)),
            'mean_loss': float(np.mean(losses)),
            'std_loss': float(np.std(losses)),
            'accuracy_95ci': [
                float(np.mean(accuracies) - 1.96 * np.std(accuracies) / np.sqrt(len(accuracies))),
                float(np.mean(accuracies) + 1.96 * np.std(accuracies) / np.sqrt(len(accuracies)))
            ],
            'auc_95ci': [
                float(np.mean(auc_scores) - 1.96 * np.std(auc_scores) / np.sqrt(len(auc_scores))),
                float(np.mean(auc_scores) + 1.96 * np.std(auc_scores) / np.sqrt(len(auc_scores)))
            ]
        }
    
    def _save_config_results(self, config_name, fold_results, avg_metrics):
        """ä¿å­˜å•ä¸ªé…ç½®çš„ç»“æœ"""
        # ä¿å­˜è¯¦ç»†ç»“æœ
        config_result = {
            'config_name': config_name,
            'average_metrics': avg_metrics,
            'fold_results': [
                {
                    'fold': r['fold'],
                    'best_loss': float(r['best_loss']),
                    'best_epoch': r['best_epoch'],
                    'final_val_acc': float(r['final_val_acc']),
                    'final_val_auc': float(r['final_val_auc'])
                } for r in fold_results
            ],
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        result_file = os.path.join(self.metrics_dir, f"{config_name}_results.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(config_result, f, indent=2, ensure_ascii=False)
    
    def _generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print(f"å¯¹æ¯”å®éªŒç»“æœæŠ¥å‘Š: {self.title_prefix}")
        print("=" * 80)
        
        report_data = []
        
        for config_name, results in self.comparison_results.items():
            metrics = results['average_metrics']
            config = results['config']
            
            print(f"\né…ç½®: {config_name}")
            print(f"æè¿°: {config['description']}")
            print(f"å‡†ç¡®ç‡: {metrics['mean_accuracy']:.4f} Â± {metrics['std_accuracy']:.4f}")
            print(f"AUC: {metrics['mean_auc']:.4f} Â± {metrics['std_auc']:.4f}")
            print(f"æŸå¤±: {metrics['mean_loss']:.4f} Â± {metrics['std_loss']:.4f}")
            print(f"å‡†ç¡®ç‡95%ç½®ä¿¡åŒºé—´: [{metrics['accuracy_95ci'][0]:.4f}, {metrics['accuracy_95ci'][1]:.4f}]")
            print(f"AUC 95%ç½®ä¿¡åŒºé—´: [{metrics['auc_95ci'][0]:.4f}, {metrics['auc_95ci'][1]:.4f}]")
            
            # æ”¶é›†æ•°æ®ç”¨äºè¡¨æ ¼
            report_data.append({
                'Configuration': config_name,
                'Description': config['description'],
                'Accuracy (meanÂ±std)': f"{metrics['mean_accuracy']:.4f}Â±{metrics['std_accuracy']:.4f}",
                'AUC (meanÂ±std)': f"{metrics['mean_auc']:.4f}Â±{metrics['std_auc']:.4f}",
                'Loss (meanÂ±std)': f"{metrics['mean_loss']:.4f}Â±{metrics['std_loss']:.4f}",
                'Accuracy_Mean': metrics['mean_accuracy'],
                'AUC_Mean': metrics['mean_auc']
            })
        
        # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”ï¼ˆå¦‚æœæœ‰ä¸¤ä¸ªé…ç½®ï¼‰
        if len(self.comparison_results) == 2:
            config_names = list(self.comparison_results.keys())
            config1_name = config_names[0]
            config2_name = config_names[1]
            
            metrics1 = self.comparison_results[config1_name]['average_metrics']
            metrics2 = self.comparison_results[config2_name]['average_metrics']
            
            acc_improvement = ((metrics2['mean_accuracy'] - metrics1['mean_accuracy']) 
                              / metrics1['mean_accuracy'] * 100)
            auc_improvement = ((metrics2['mean_auc'] - metrics1['mean_auc']) 
                              / metrics1['mean_auc'] * 100)
            
            print(f"\n{'='*60}")
            print(f"æ€§èƒ½æ”¹è¿›åˆ†æ")
            print(f"{'='*60}")
            print(f"{config2_name} ç›¸å¯¹äº {config1_name}:")
            print(f"å‡†ç¡®ç‡æ”¹è¿›: {acc_improvement:.2f}%")
            print(f"AUCæ”¹è¿›: {auc_improvement:.2f}%")
            
            if acc_improvement > 0:
                print(f"âœ“ {config2_name} åœ¨å‡†ç¡®ç‡ä¸Šè¡¨ç°æ›´å¥½")
            else:
                print(f"âœ— {config2_name} åœ¨å‡†ç¡®ç‡ä¸Šæ²¡æœ‰æ”¹è¿›")
        
        # ä¿å­˜æŠ¥å‘Šä¸ºCSV
        df_report = pd.DataFrame(report_data)
        report_path = os.path.join(self.metrics_dir, "comparison_report.csv")
        df_report.to_csv(report_path, index=False, encoding='utf-8')
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Šä¸ºJSON
        detailed_report = {
            'experiment_timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'comparison_mode': self.comparison_mode,
            'comparison_results': {
                name: {
                    'config': results['config'],
                    'average_metrics': results['average_metrics']
                } for name, results in self.comparison_results.items()
            },
            'experiment_config': {
                'num_folds': 5,
                'batch_size': BATCH_SIZE,
                'learning_rate': LEARNING_RATE,
                'num_epochs': NUM_EPOCHS,
                'kernel_config': DEFAULT_KERNEL_CONFIG
            }
        }
        
        json_path = os.path.join(self.metrics_dir, "detailed_comparison_results.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_report, f, indent=2, ensure_ascii=False)
    
    def _create_comparison_visualizations(self):
        """åˆ›å»ºç®€åŒ–çš„å¯¹æ¯”å¯è§†åŒ–å›¾è¡¨"""
        print("\nç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨...")
    
        # 1. å‡†ç¡®ç‡å¯¹æ¯”æŸ±çŠ¶å›¾
        self._create_simple_accuracy_comparison_chart()
        
        # 2. AUCå¯¹æ¯”æŸ±çŠ¶å›¾
        self._create_simple_auc_comparison_chart()
        
        # 3. æŸå¤±å¯¹æ¯”æŸ±çŠ¶å›¾
        self._create_simple_loss_comparison_chart()
        
        print(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {self.plots_dir}")
    
    def _create_simple_accuracy_comparison_chart(self):

        """Create accuracy comparison bar chart in English"""
        fig, ax = plt.subplots(figsize=(8, 6))
        
        config_names = []
        mean_accuracies = []
        std_accuracies = []
        colors = []
        descriptions = []
        
        for config_name, results in self.comparison_results.items():
            config = results['config']
            metrics = results['average_metrics']
            
            config_names.append(config_name)
            mean_accuracies.append(metrics['mean_accuracy'])
            std_accuracies.append(metrics['std_accuracy'])
            colors.append(config['color'])
            descriptions.append(config['description'])
        
        # Draw bar chart
        bars = ax.bar(config_names, mean_accuracies, yerr=std_accuracies,
                    capsize=10, color=colors, edgecolor='black', linewidth=1.5,
                    alpha=0.8)
        
        # Add value labels
        for bar, mean, std in zip(bars, mean_accuracies, std_accuracies):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{mean:.4f}\nÂ±{std:.4f}', ha='center', va='bottom', 
                fontsize=10, fontweight='bold')
        
        ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')
        
        # Set title based on comparison mode
        if self.comparison_mode == 'stream':
            ax.set_title('Accuracy Comparison: Stream1 Only vs Stream1+Stream2', 
                        fontsize=14, fontweight='bold', pad=20)
        else:
            ax.set_title('Accuracy Comparison: No Augmentation vs With Augmentation', 
                        fontsize=14, fontweight='bold', pad=20)
        
        ax.set_ylim([0, 1.1])
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_axisbelow(True)
        
        # Add legend with descriptions
        from matplotlib.patches import Patch
        legend_elements = []
        for i, config_name in enumerate(config_names):
            legend_elements.append(
                Patch(facecolor=colors[i], edgecolor='black', alpha=0.8,
                    label=f"{config_name}: {descriptions[i]}")
            )
        ax.legend(handles=legend_elements, loc='best', fontsize=10)
        
        plt.tight_layout()
        
        # Save chart
        save_path = os.path.join(self.plots_dir, "accuracy_comparison.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
  
    
    def _create_simple_auc_comparison_chart(self):
        """Create AUC comparison bar chart in English"""
        fig, ax = plt.subplots(figsize=(8, 6))
        
        config_names = []
        mean_aucs = []
        std_aucs = []
        colors = []
        
        for config_name, results in self.comparison_results.items():
            config = results['config']
            metrics = results['average_metrics']
            
            config_names.append(config_name)
            mean_aucs.append(metrics['mean_auc'])
            std_aucs.append(metrics['std_auc'])
            colors.append(config['color'])
        
        # Draw bar chart
        bars = ax.bar(config_names, mean_aucs, yerr=std_aucs,
                    capsize=10, color=colors, edgecolor='black', linewidth=1.5,
                    alpha=0.8)
        
        # Add value labels
        for bar, mean, std in zip(bars, mean_aucs, std_aucs):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{mean:.4f}\nÂ±{std:.4f}', ha='center', va='bottom', 
                fontsize=10, fontweight='bold')
        
        ax.set_ylabel('AUC Score', fontsize=12, fontweight='bold')
        
        # Set title based on comparison mode
        if self.comparison_mode == 'stream':
            ax.set_title('AUC Comparison: Stream1 Only vs Stream1+Stream2', 
                        fontsize=14, fontweight='bold', pad=20)
        else:
            ax.set_title('AUC Comparison: No Augmentation vs With Augmentation', 
                        fontsize=14, fontweight='bold', pad=20)
        
        ax.set_ylim([0, 1.1])
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_axisbelow(True)
        
        plt.tight_layout()
        
        # Save chart
        save_path = os.path.join(self.plots_dir, "auc_comparison.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()

    def _create_simple_loss_comparison_chart(self):
        """åˆ›å»ºæŸå¤±å¯¹æ¯”æŸ±çŠ¶å›¾"""
        fig, ax = plt.subplots(figsize=(8, 6))
    
        config_names = []
        mean_losses = []
        std_losses = []
        colors = []
        
        for config_name, results in self.comparison_results.items():
            config = results['config']
            metrics = results['average_metrics']
            
            config_names.append(config_name)
            mean_losses.append(metrics['mean_loss'])
            std_losses.append(metrics['std_loss'])
            colors.append(config['color'])
        
        # Draw bar chart
        bars = ax.bar(config_names, mean_losses, yerr=std_losses,
                    capsize=10, color=colors, edgecolor='black', linewidth=1.5,
                    alpha=0.8)
        
        # Add value labels
        for bar, mean, std in zip(bars, mean_losses, std_losses):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{mean:.4f}\nÂ±{std:.4f}', ha='center', va='bottom', 
                fontsize=10, fontweight='bold')
        
        ax.set_ylabel('Loss', fontsize=12, fontweight='bold')
        
        # Set title based on comparison mode
        if self.comparison_mode == 'stream':
            ax.set_title('Loss Comparison: Stream1 Only vs Stream1+Stream2', 
                        fontsize=14, fontweight='bold', pad=20)
        else:
            ax.set_title('Loss Comparison: No Augmentation vs With Augmentation', 
                        fontsize=14, fontweight='bold', pad=20)
        
        # Calculate appropriate y-axis limit
        max_loss = max(mean_losses) + max(std_losses) if std_losses else max(mean_losses)
        ax.set_ylim([0, max_loss * 1.2])
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_axisbelow(True)
        
        plt.tight_layout()
        
        # Save chart
        save_path = os.path.join(self.plots_dir, "loss_comparison.png")
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _save_experiment_results(self):
        """ä¿å­˜å®Œæ•´çš„å®éªŒç»“æœ"""
        # ä¿å­˜å®Œæ•´çš„å¯¹æ¯”ç»“æœ
        complete_results = {
            'experiment_info': {
                'name': self.experiment_name,
                'mode': self.comparison_mode,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'base_path': self.base_path
            },
            'configurations': {
                name: {
                    'config': results['config'],
                    'average_metrics': results['average_metrics']
                } for name, results in self.comparison_results.items()
            },
            'detailed_results': self.comparison_results
        }
        
        complete_results_path = os.path.join(self.results_dir, "complete_comparison_results.json")
        with open(complete_results_path, 'w', encoding='utf-8') as f:
            json.dump(complete_results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"å®Œæ•´å®éªŒç»“æœå·²ä¿å­˜: {complete_results_path}")


# ==================== ä¸»ç¨‹åºæ¨¡å— ====================
class TrainingPipeline:
    """è®­ç»ƒç®¡é“ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.base_path = os.path.abspath(
            os.path.join(os.path.dirname(__file__), '../Dataset')
        )
    
    def run(self):
        """è¿è¡Œè®­ç»ƒç®¡é“"""
        parser = argparse.ArgumentParser(description='ECGæˆ¿é¢¤æ£€æµ‹è®­ç»ƒè„šæœ¬')
        parser.add_argument('--mode', type=str, default=EXPERIMENT_MODE,
                            choices=['search', 'train', 'full', 'compare'],
                            help='è¿è¡Œæ¨¡å¼: search=è¶…å‚æ•°æœç´¢, train=é»˜è®¤è®­ç»ƒ, full=æœ€ä½³é…ç½®å®Œæ•´è®­ç»ƒ, compare=å¯¹æ¯”å®éªŒ')
        parser.add_argument('--use_best_config', action='store_true',
                            help='ä½¿ç”¨ä¹‹å‰æœç´¢å¾—åˆ°çš„æœ€ä½³é…ç½®')
        parser.add_argument('--best_config_path', type=str,
                            default='/home/xusi/Logs/HyperparamSearch/latest_best_config.json',
                            help='æœ€ä½³é…ç½®æ–‡ä»¶è·¯å¾„')
        parser.add_argument('--compare_mode', type=str, default=COMPARISON_MODE,
                            choices=['stream', 'augment'],
                            help='å¯¹æ¯”å®éªŒæ¨¡å¼: stream=å¯¹æ¯”Streamé…ç½®, augment=å¯¹æ¯”æ•°æ®å¢å¼º')
        
        args = parser.parse_args()
        
        # æ‰§è¡Œå¯¹åº”æ¨¡å¼
        if args.mode == 'search':
            self._run_search_mode()
        elif args.mode == 'train':
            self._run_train_mode()
        elif args.mode == 'full' or args.use_best_config:
            self._run_full_mode(args)
        elif args.mode == 'compare':
            self._run_compare_mode(args)
        else:
            print(f"æœªçŸ¥æ¨¡å¼: {args.mode}")
    
    def _run_search_mode(self):
        """è¿è¡Œè¶…å‚æ•°æœç´¢æ¨¡å¼"""
        print("\næ¨¡å¼: è¶…å‚æ•°æœç´¢")
        print(f"FIXED_LENGTH = {FIXED_LENGTH} ({FIXED_LENGTH / 300:.1f}ç§’ @300Hz)")
        
        # è¿™é‡Œéœ€è¦å¯¼å…¥HyperparameterSearcherç±»ï¼ˆå‡è®¾å®ƒåœ¨åŒä¸€ä¸ªæ–‡ä»¶ä¸­ï¼‰
        searcher = HyperparameterSearcher(self.base_path)
        search_results, search_dir = searcher.search(
            num_folds=3,
            num_epochs_search=15
        )
        
        # æ›´æ–°æœ€æ–°æœ€ä½³é…ç½®
        latest_best_config = os.path.join(search_dir, 'best_config.json')
        if os.path.exists(latest_best_config):
            import shutil
            shutil.copy2(latest_best_config, 
                        '/home/xusi/Logs/HyperparamSearch/latest_best_config.json')
            print(f"\nå·²æ›´æ–°æœ€æ–°æœ€ä½³é…ç½®: /home/xusi/Logs/HyperparamSearch/latest_best_config.json")
    
    def _run_train_mode(self):
        """è¿è¡Œé»˜è®¤è®­ç»ƒæ¨¡å¼"""
        print("\næ¨¡å¼: ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ")
        print(f"å·ç§¯æ ¸: MS-CNN(3,7), æ‰¹å¤§å°: {BATCH_SIZE}")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        experiment_base_dir = os.path.join(
            "/home/xusi/Logs/DefaultTraining", f"Run_{timestamp}"
        )
        os.makedirs(experiment_base_dir, exist_ok=True)
        
        # ä½¿ç”¨é»˜è®¤é…ç½®
        default_kernel_config = DEFAULT_KERNEL_CONFIG
        
        # 5æŠ˜äº¤å‰éªŒè¯
        K = 5
        fold_results = []
        
        for k in range(K):
            test_fold = k
            train_folds = [i for i in range(5) if i != k]
            
            trainer = ModelTrainer(
                base_path=self.base_path,
                kernel_config=default_kernel_config,
                batch_size=BATCH_SIZE,
                lr=LEARNING_RATE,
                use_stream2=USE_STREAM2_SETTING,
                augment=AUGMENT_SETTING,
                experiment_base_dir=experiment_base_dir
            )
            
            result = trainer.train_fold(
                train_folds=train_folds,
                test_fold=test_fold,
                num_epochs=NUM_EPOCHS
            )
            
            fold_results.append(result)
            print(f"æŠ˜ {k + 1} å®Œæˆ: "
                  f"å‡†ç¡®ç‡={result['final_val_acc']:.4f}, "
                  f"AUC={result['final_val_auc']:.4f}")
        
        # è®¡ç®—å¹³å‡ç»“æœ
        avg_acc = np.mean([r['final_val_acc'] for r in fold_results])
        avg_auc = np.mean([r['final_val_auc'] for r in fold_results])
        
        print(f"\né»˜è®¤é…ç½®è®­ç»ƒå®Œæˆ!")
        print(f"å¹³å‡å‡†ç¡®ç‡: {avg_acc:.4f}")
        print(f"å¹³å‡AUC: {avg_auc:.4f}")
    
    def _run_full_mode(self, args):
        """è¿è¡Œå®Œæ•´è®­ç»ƒæ¨¡å¼"""
        print("\næ¨¡å¼: ä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ")

        best_config_path = "/home/xusi/Logs/HyperparamSearch/latest_best_config.json"

        # åŠ è½½æœ€ä½³é…ç½®
        if os.path.exists(best_config_path):
            with open(best_config_path, 'r') as f:
                best_config_data = json.load(f)
            
            best_config_name = best_config_data['best_config_name']
            best_config_details = best_config_data['best_config_data']
            
            print(f"åŠ è½½æœ€ä½³é…ç½®: {best_config_name}")
            print(f"æœç´¢æ—¶é—´: {best_config_data.get('search_timestamp', 'æœªçŸ¥')}")
            
            # ä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ
            avg_metrics, final_results,simplified_path = CompleteTrainer.train_with_best_config(
                self.base_path,
                best_config_details,
                num_epochs=NUM_EPOCHS
            )
        
            # æ‰“å°ä¸LLMè¯„ä¼°å…¼å®¹çš„ç»“æœ
            print("\n" + "=" * 80)
            print("CNNæ¨¡å‹è¯„ä¼°ç»“æœï¼ˆLLMå…¼å®¹æ ¼å¼ï¼‰:")
            print("=" * 80)
            print(f"å¹³å‡å‡†ç¡®ç‡: {final_results['accuracy']:.4f}")
            print(f"å¹³å‡ç²¾ç¡®ç‡: {final_results['precision']:.4f}")
            print(f"å¹³å‡å¬å›ç‡: {final_results['recall']:.4f}")
            print(f"å¹³å‡F1åˆ†æ•°: {final_results['f1']:.4f}")
            print(f"å¹³å‡AUC: {final_results['auc']:.4f}")

            print(f"\nğŸ’¡ å°†æ­¤æ–‡ä»¶è·¯å¾„ç”¨äºLLMå¯¹æ¯”:")
            print(f"   CNN_BASELINE_RESULTS = '{simplified_path}'")
        
            print("\näº¤å‰éªŒè¯ç»“æœ:")
            for fold_name, metrics in final_results['cross_validation_results'].items():
                print(f"  {fold_name}: å‡†ç¡®ç‡={metrics['accuracy']:.4f}, "
                    f"ç²¾ç¡®ç‡={metrics['precision']:.4f}, "
                    f"å¬å›ç‡={metrics['recall']:.4f}, "
                    f"F1={metrics['f1']:.4f}, "
                    f"AUC={metrics['auc']:.4f}")
        else:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æœ€ä½³é…ç½®æ–‡ä»¶ {best_config_path}")
            print("è¯·å…ˆè¿è¡Œè¶…å‚æ•°æœç´¢æ¨¡å¼: python Train_Process.py --mode search")

    def _run_compare_mode(self, args):
        """è¿è¡Œå¯¹æ¯”å®éªŒæ¨¡å¼"""
        print("\næ¨¡å¼: å¯¹æ¯”å®éªŒ")
        
        # åˆ›å»ºå¯¹æ¯”å®éªŒ
        comparison_experiment = ComparisonExperiment(
            base_path=self.base_path,
            comparison_mode=args.compare_mode
        )
        
        # è¿è¡Œå¯¹æ¯”å®éªŒ
        results = comparison_experiment.run_comparison(
            num_folds=5,
            num_epochs=NUM_EPOCHS
        )
        
        print(f"\nå¯¹æ¯”å®éªŒå®Œæˆï¼")
        print(f"å¯¹æ¯”æ¨¡å¼: {args.compare_mode}")
        print(f"ç»“æœç›®å½•: {comparison_experiment.results_dir}")
        
        # è¾“å‡ºä¸»è¦å‘ç°
        print("\n" + "="*60)
        print("ä¸»è¦å‘ç°:")
        print("="*60)
        
        config_names = list(results.keys())
        if len(config_names) >= 2:
            config1 = config_names[0]
            config2 = config_names[1]
            
            metrics1 = results[config1]['average_metrics']
            metrics2 = results[config2]['average_metrics']
            
            acc_diff = metrics2['mean_accuracy'] - metrics1['mean_accuracy']
            auc_diff = metrics2['mean_auc'] - metrics1['mean_auc']
            
            print(f"1. {config2} ç›¸å¯¹äº {config1}:")
            print(f"   å‡†ç¡®ç‡å·®å¼‚: {acc_diff:+.4f} ({acc_diff/metrics1['mean_accuracy']*100:+.1f}%)")
            print(f"   AUCå·®å¼‚: {auc_diff:+.4f} ({auc_diff/metrics1['mean_auc']*100:+.1f}%)")
            
            if acc_diff > 0 and auc_diff > 0:
                print(f"   âœ“ {config2} åœ¨ä¸¤é¡¹æŒ‡æ ‡ä¸Šå‡è¡¨ç°æ›´å¥½")
            elif acc_diff > 0:
                print(f"   âš  {config2} åœ¨å‡†ç¡®ç‡ä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†AUCç•¥å·®")
            elif auc_diff > 0:
                print(f"   âš  {config2} åœ¨AUCä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†å‡†ç¡®ç‡ç•¥å·®")
            else:
                print(f"   âœ— {config2} åœ¨ä¸¤é¡¹æŒ‡æ ‡ä¸Šå‡æœªè¡¨ç°å‡ºä¼˜åŠ¿")
        
        print("\nè¯¦ç»†å¯¹æ¯”ç»“æœè¯·æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨å’ŒæŠ¥å‘Šæ–‡ä»¶ã€‚")


# ==================== ç¨‹åºå…¥å£ ====================
if __name__ == '__main__':
    pipeline = TrainingPipeline()
    pipeline.run()
    print("\nè®­ç»ƒå®Œæˆ!")