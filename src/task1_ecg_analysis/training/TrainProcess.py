"""
ECGæˆ¿é¢¤æ£€æµ‹è®­ç»ƒä¸»ç¨‹åº - ä¿®å¤ç‰ˆï¼ˆæ·»åŠ æ¨¡å‹ä¿å­˜åŠŸèƒ½ï¼‰
"""

# ==================== è§£å†³å¯¼å…¥é—®é¢˜ ====================
import sys
import os

# è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
current_file = os.path.abspath(__file__)
print(f"å½“å‰æ–‡ä»¶: {current_file}")

# æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è®¡ç®—é¡¹ç›®æ ¹ç›®å½•
# å½“å‰æ–‡ä»¶: EE5046_Projects/src/task1_ecg_analysis/training/TrainProcess.py
# é¡¹ç›®æ ¹ç›®å½•åº”è¯¥æ˜¯: EE5046_Projects (ä¸Šä¸‰çº§ç›®å½•)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_file))))

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.insert(0, project_root)
print(f"é¡¹ç›®æ ¹ç›®å½•å·²æ·»åŠ åˆ°Pythonè·¯å¾„: {project_root}")


# ==================== å¯¼å…¥éƒ¨åˆ† ====================
import argparse
import json
import os
import sys
from datetime import datetime
import numpy as np
import shutil

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import pandas as pd
import torch
from sklearn.metrics import (accuracy_score, roc_auc_score, roc_curve, 
                           precision_score, recall_score, f1_score)
from torch.utils.data import DataLoader,random_split
from tqdm import tqdm

# è‡ªå®šä¹‰æ¨¡å—
from src.common.Config import (AUGMENT_SETTING, BATCH_SIZE, EARLY_STOP_PATIENCE,
                    EXPERIMENT_MODE, FIXED_LENGTH, INPUT_CHANNELS,
                    LEARNING_RATE, MIN_DELTA, NUM_EPOCHS, OUTPUT_CLASSES,
                    USE_STREAM2_SETTING, COMPARISON_MODE,
                    STREAM_COMPARISON_CONFIGS, AUGMENTATION_COMPARISON_CONFIGS,
                    DEFAULT_KERNEL_CONFIG)
from src.task1_ecg_analysis.data.DataManager import DataManager
from src.task1_ecg_analysis.data.FoldDataset import FoldDataset
from src.task1_ecg_analysis.visualization.TrainingVisualizer import TrainingVisualizer
from TrainModel import Mscnn


# è®¾å¤‡è®¾ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ==================== æ–°å¢ï¼šç»¼åˆè¯„åˆ†è®¡ç®—å™¨ ====================
class CompositeScoreCalculator:
    """è®¡ç®—ç»¼åˆè¯„åˆ†ï¼Œè€ƒè™‘å¤šä¸ªæŒ‡æ ‡"""
    
    # é»˜è®¤æƒé‡é…ç½®
    DEFAULT_WEIGHTS = {
        'accuracy': 0.35,
        'auc': 0.30,
        'f1': 0.25,
        'stability': 0.10  # ç¨³å®šæ€§åˆ†æ•°ï¼ˆåŸºäºæ–¹å·®ï¼‰
    }
    
    @staticmethod
    def calculate_composite_score(metrics, weights=None, fold_results=None):
        """
        è®¡ç®—ç»¼åˆè¯„åˆ†
        Args:
            metrics: åŒ…å«å•ä¸ªè¯„ä¼°æŒ‡æ ‡çš„å­—å…¸ï¼Œå¦‚ {'accuracy': 0.85, 'auc': 0.90, 'f1': 0.82}
            weights: å„æŒ‡æ ‡çš„æƒé‡ï¼Œé»˜è®¤ä¸ºDEFAULT_WEIGHTS
            fold_results: äº¤å‰éªŒè¯çš„è¯¦ç»†ç»“æœï¼ˆç”¨äºè®¡ç®—ç¨³å®šæ€§ï¼‰
        Returns:
            composite_score: ç»¼åˆè¯„åˆ†
            breakdown: å„æŒ‡æ ‡è´¡çŒ®æ˜ç»†
        """
        if weights is None:
            weights = CompositeScoreCalculator.DEFAULT_WEIGHTS
        
        # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„æŒ‡æ ‡éƒ½å­˜åœ¨
        required_metrics = ['accuracy', 'auc', 'f1']
        for metric in required_metrics:
            if metric not in metrics:
                raise ValueError(f"ç¼ºå°‘å¿…è¦æŒ‡æ ‡: {metric}")
        
        # è®¡ç®—ç¨³å®šæ€§åˆ†æ•°ï¼ˆå¦‚æœæœ‰fold_resultsï¼‰
        stability_score = 1.0  # é»˜è®¤å€¼
        if fold_results is not None and len(fold_results) > 1:
            # æå–å„æŠ˜çš„å‡†ç¡®ç‡
            accuracies = [fold['best_val_acc'] for fold in fold_results]
            # ç¨³å®šæ€§åˆ†æ•° = 1 - å˜å¼‚ç³»æ•°ï¼ˆå½’ä¸€åŒ–æ–¹å·®ï¼‰
            cv = np.std(accuracies) / (np.mean(accuracies) + 1e-8)  # å˜å¼‚ç³»æ•°
            stability_score = max(0, 1 - cv)  # ç¡®ä¿åœ¨0-1ä¹‹é—´
        
        # è®¡ç®—åŠ æƒç»¼åˆè¯„åˆ†
        composite_score = 0
        breakdown = {}
        
        for metric, weight in weights.items():
            if metric == 'stability':
                score = stability_score
            elif metric in metrics:
                score = metrics[metric]
            else:
                score = 0.5  # é»˜è®¤å€¼
            
            contribution = score * weight
            composite_score += contribution
            breakdown[metric] = {
                'score': score,
                'weight': weight,
                'contribution': contribution
            }
        
        return composite_score, breakdown
    
    @staticmethod
    def normalize_metrics(metrics, ideal_values=None):
        """å½’ä¸€åŒ–æŒ‡æ ‡åˆ°0-1èŒƒå›´"""
        if ideal_values is None:
            ideal_values = {
                'accuracy': 1.0,
                'auc': 1.0,
                'f1': 1.0,
                'precision': 1.0,
                'recall': 1.0
            }
        
        normalized = {}
        for metric, value in metrics.items():
            if metric in ideal_values:
                # ç®€å•çº¿æ€§å½’ä¸€åŒ–
                normalized[metric] = min(value / ideal_values[metric], 1.0)
            else:
                normalized[metric] = value
        
        return normalized


# ==================== æ¨¡å‹æ–‡ä»¶ç®¡ç†å™¨ï¼ˆä¿æŒä¸å˜ï¼‰ ====================
class ModelFileManager:
    """ç®¡ç†æ¨¡å‹æ–‡ä»¶çš„ä¿å­˜å’ŒåŠ è½½"""
    
    @staticmethod
    def create_experiment_dir(base_dir, experiment_type, config_name=""):
        """åˆ›å»ºå®éªŒç›®å½•ç»“æ„"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if config_name:
            dir_name = f"{experiment_type}_{config_name}_{timestamp}"
        else:
            dir_name = f"{experiment_type}_{timestamp}"
        
        exp_dir = os.path.join(base_dir, dir_name)
        subdirs = [
            "models",           # ä¿å­˜æ¨¡å‹æ–‡ä»¶
            "logs",            # è®­ç»ƒæ—¥å¿—
            "configs",         # é…ç½®æ–‡ä»¶
            "metrics",         # æ€§èƒ½æŒ‡æ ‡
            "visualizations"   # å¯è§†åŒ–å›¾è¡¨
        ]
        
        for subdir in subdirs:
            os.makedirs(os.path.join(exp_dir, subdir), exist_ok=True)
        
        print(f"ğŸ“ åˆ›å»ºå®éªŒç›®å½•: {exp_dir}")
        return exp_dir
    
    @staticmethod
    def save_model(model, save_path, metadata=None):
        """ä¿å­˜æ¨¡å‹å’Œå…ƒæ•°æ®"""
        model_state = {
            'model_state_dict': model.state_dict(),
            'model_config': getattr(model, 'config', {}),
            'save_time': datetime.now().isoformat()
        }
        
        if metadata:
            model_state.update(metadata)
        
        torch.save(model_state, save_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")
    
    @staticmethod
    def generate_model_name(config, fold=None, epoch=None, metric=None, composite_score=None):
        """ç”Ÿæˆæ¨¡å‹æ–‡ä»¶å"""
        parts = []
        
        # åŸºç¡€ä¿¡æ¯
        kernel_config = config.get('kernel_config', {})
        if kernel_config.get('name'):
            parts.append(kernel_config['name'])
        else:
            parts.append(f"K{kernel_config.get('stream1_kernel', '?')}")
            parts.append(f"S2{kernel_config.get('stream2_first_kernel', '?')}")
        
        # è®­ç»ƒé…ç½®
        parts.append(f"BS{config.get('batch_size', BATCH_SIZE)}")
        parts.append(f"LR{config.get('lr', LEARNING_RATE)}")
        
        # è®­ç»ƒçŠ¶æ€
        if fold is not None:
            parts.append(f"F{fold}")
        if epoch is not None:
            parts.append(f"E{epoch}")
        
        # æ€§èƒ½æŒ‡æ ‡
        if composite_score is not None:
            parts.append(f"CS{composite_score:.4f}".replace('.', 'p'))
        elif metric is not None:
            parts.append(f"A{metric:.4f}".replace('.', 'p'))
        
        # æ—¶é—´æˆ³
        parts.append(datetime.now().strftime("%m%d%H%M"))
        
        return "_".join(parts) + ".pth"
    
    @staticmethod
    def save_metrics(metrics, save_path):
        """ä¿å­˜æ€§èƒ½æŒ‡æ ‡"""
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š æŒ‡æ ‡å·²ä¿å­˜: {save_path}")
    
    @staticmethod
    def save_config(config, save_path):
        """ä¿å­˜é…ç½®"""
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"âš™ï¸ é…ç½®å·²ä¿å­˜: {save_path}")


# ==================== æ”¹è¿›çš„æ¨¡å‹è®­ç»ƒå™¨æ¨¡å— ====================
class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨ï¼Œè´Ÿè´£å•æ¬¡è®­ç»ƒéªŒè¯è¿‡ç¨‹"""
    
    def __init__(self, base_path, kernel_config, batch_size, lr, 
                 use_stream2, augment, experiment_dir, config_name=None,
                 composite_weights=None):
        self.base_path = base_path
        self.kernel_config = kernel_config
        self.batch_size = batch_size
        self.lr = lr
        self.use_stream2 = use_stream2
        self.augment = augment
        self.experiment_dir = experiment_dir
        self.config_name = config_name
        self.file_manager = ModelFileManager()
        self.visualizer = TrainingVisualizer()
        
        # ç»¼åˆè¯„åˆ†æƒé‡
        self.composite_weights = composite_weights or CompositeScoreCalculator.DEFAULT_WEIGHTS
        

        print(f"åˆå§‹åŒ–DataManagerï¼Œæ•°æ®é›†è·¯å¾„: {self.base_path}")
        if not os.path.exists(self.base_path):
            print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {self.base_path}")
            print("è¯·æ£€æŸ¥Datasetç›®å½•æ˜¯å¦æ­£ç¡®æ”¾ç½®")
            sys.exit(1)
        
        self.data_manager = DataManager(base_path)
        # æ•°æ®ç®¡ç†å™¨
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_acc = 0
        self.best_val_auc = 0
        self.best_val_f1 = 0
        self.best_composite_score = 0
        self.best_model_state = None
        self.best_epoch = 0
        self.early_stop_counter = 0

    def cross_validate_on_train_set(self, train_cv_indices, num_epochs, k_folds=5, save_models=True):
        """
        åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯
        """
        print(f"\n{'='*60}")
        print(f"åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ {k_folds} æŠ˜äº¤å‰éªŒè¯")
        print(f"{'='*60}")
        
        # åˆ›å»ºKæŠ˜åˆ’åˆ†
        kfold_splits = self.data_manager.create_kfold_splits(train_cv_indices, k_folds)
        if not kfold_splits:
            print("é”™è¯¯: æ— æ³•åˆ›å»ºKæŠ˜åˆ’åˆ†")
            return {}, []
        
        fold_results = []
        fold_models = []

        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºæŠ˜çš„è®­ç»ƒè¿›åº¦
        fold_pbar = self.visualizer.create_progress_bar(k_folds, "äº¤å‰éªŒè¯è¿›åº¦")
        
        # è®­ç»ƒæ¯ä¸€æŠ˜
        for fold_idx, (train_data, val_data) in enumerate(kfold_splits):
            print(f"\n--- ç¬¬ {fold_idx + 1}/{k_folds} æŠ˜ ---")
            
            # è®­ç»ƒå½“å‰æŠ˜
            fold_result, fold_model = self._train_single_fold(
                train_data, val_data, fold_idx, num_epochs, save_models
            )
            
            fold_results.append(fold_result)
            fold_models.append(fold_model)

            # æ›´æ–°è¿›åº¦æ¡
            fold_pbar.update(1)
            
            # ä½¿ç”¨ç»¼åˆè¯„åˆ†è¿›è¡Œè¯„ä»·
            metrics = {
                'accuracy': fold_result['best_val_acc'],
                'auc': fold_result['best_val_auc'],
                'f1': fold_result['best_val_f1']
            }
            composite_score, breakdown = CompositeScoreCalculator.calculate_composite_score(metrics)
            
            print(f"æŠ˜ {fold_idx + 1} ç»“æœ:")
            print(f"  éªŒè¯å‡†ç¡®ç‡: {fold_result['best_val_acc']:.4f}")
            print(f"  AUC: {fold_result['best_val_auc']:.4f}")
            print(f"  F1åˆ†æ•°: {fold_result['best_val_f1']:.4f}")
            print(f"  ç»¼åˆè¯„åˆ†: {composite_score:.4f}")

        fold_pbar.close()
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = self._compute_average_metrics(fold_results)
        
        # è®¡ç®—å¹³å‡ç»¼åˆè¯„åˆ†
        all_metrics = [{
            'accuracy': r['best_val_acc'],
            'auc': r['best_val_auc'],
            'f1': r['best_val_f1']
        } for r in fold_results]
        
        avg_composite_score = np.mean([
            CompositeScoreCalculator.calculate_composite_score(m)[0] for m in all_metrics
        ])
        avg_metrics['avg_composite_score'] = float(avg_composite_score)
        
        # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
        if self.experiment_dir:
            cv_results = {
                'avg_metrics': avg_metrics,
                'fold_results': fold_results,
                'config': self._get_config_dict(),
                'timestamp': datetime.now().isoformat(),
                'composite_score_weights': self.composite_weights
            }
            
            results_path = os.path.join(self.experiment_dir, "metrics", "cross_validation_results.json")
            self.file_manager.save_metrics(cv_results, results_path)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºç»¼åˆè¯„åˆ†æœ€é«˜çš„æŠ˜ï¼‰
            if fold_models and save_models:
                # è®¡ç®—æ¯æŠ˜çš„ç»¼åˆè¯„åˆ†
                composite_scores = []
                for r in fold_results:
                    metrics = {
                        'accuracy': r['best_val_acc'],
                        'auc': r['best_val_auc'],
                        'f1': r['best_val_f1']
                    }
                    score, _ = CompositeScoreCalculator.calculate_composite_score(metrics)
                    composite_scores.append(score)
                
                # é€‰æ‹©ç»¼åˆè¯„åˆ†æœ€é«˜çš„æŠ˜
                best_fold_idx = np.argmax(composite_scores)
                best_model = fold_models[best_fold_idx]
                best_fold_result = fold_results[best_fold_idx]
                best_composite_score = composite_scores[best_fold_idx]
                
                model_name = self.file_manager.generate_model_name(
                    self._get_config_dict(),
                    fold=best_fold_idx,
                    epoch=best_fold_result['best_epoch'],
                    composite_score=best_composite_score
                )
                
                model_path = os.path.join(self.experiment_dir, "models", model_name)
                metadata = {
                    'fold': best_fold_idx,
                    'val_acc': best_fold_result['best_val_acc'],
                    'val_auc': best_fold_result['best_val_auc'],
                    'val_f1': best_fold_result['best_val_f1'],
                    'composite_score': best_composite_score,
                    'epoch': best_fold_result['best_epoch']
                }
                self.file_manager.save_model(best_model, model_path, metadata)
        
        print(f"\n{'='*60}")
        print(f"{k_folds}æŠ˜äº¤å‰éªŒè¯ç»“æœæ±‡æ€»:")
        print(f"{'='*60}")
        print(f"å¹³å‡éªŒè¯å‡†ç¡®ç‡: {avg_metrics['avg_val_acc']:.4f} Â± {avg_metrics['std_val_acc']:.4f}")
        print(f"å¹³å‡AUC: {avg_metrics['avg_val_auc']:.4f} Â± {avg_metrics['std_val_auc']:.4f}")
        print(f"å¹³å‡F1åˆ†æ•°: {avg_metrics['avg_val_f1']:.4f} Â± {avg_metrics['std_val_f1']:.4f}")
        print(f"å¹³å‡ç»¼åˆè¯„åˆ†: {avg_metrics['avg_composite_score']:.4f}")

        # å¯è§†åŒ–äº¤å‰éªŒè¯ç»“æœ
        if self.experiment_dir:
            self._visualize_cv_results(fold_results, avg_metrics)
        
        return avg_metrics, fold_results
    
    def train_final_model(self, train_cv_indices, num_epochs, save_model=True, val_ratio=0.2):
        """
        ä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ŒåŒ…å«éªŒè¯é›†
        Args:
            train_cv_indices: è®­ç»ƒé›†CVç´¢å¼•
            num_epochs: è®­ç»ƒè½®æ•°
            save_model: æ˜¯å¦ä¿å­˜æ¨¡å‹
            val_ratio: ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›†çš„æ¯”ä¾‹
        """
        print(f"\nä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
        print(f"è®­ç»ƒé›†: CV{', '.join(map(str, train_cv_indices))}")
        print(f"éªŒè¯é›†æ¯”ä¾‹: {val_ratio:.1%}")
        
        # åŠ è½½è®­ç»ƒé›†æ•°æ®
        train_data = self.data_manager.load_cv_files(train_cv_indices)
        if len(train_data) == 0:
            print("é”™è¯¯: è®­ç»ƒé›†æ•°æ®ä¸ºç©º")
            return None, {}
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        total_size = len(train_data)
        val_size = int(total_size * val_ratio)
        train_size = total_size - val_size
        
        # éšæœºåˆ’åˆ†
        torch.manual_seed(42)  # ç¡®ä¿å¯é‡å¤æ€§
        train_subset, val_subset = random_split(train_data, [train_size, val_size])
        
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {train_size}, éªŒè¯æ ·æœ¬æ•°: {val_size}")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = FoldDataset(
            list(train_subset), self.base_path, is_train=True, augment=self.augment
        )
        val_dataset = FoldDataset(
            list(val_subset), self.base_path, is_train=False, augment=False
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset, batch_size=self.batch_size, 
            shuffle=True, num_workers=0
        )
        val_loader = DataLoader(
            val_dataset, batch_size=self.batch_size, 
            shuffle=False, num_workers=0
        )
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = Mscnn(
            INPUT_CHANNELS,
            OUTPUT_CLASSES,
            use_stream2=self.use_stream2,
            stream1_kernel=self.kernel_config['stream1_kernel'],
            stream2_first_kernel=self.kernel_config['stream2_first_kernel']
        ).to(device)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = torch.nn.BCELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)
        
        # è®­ç»ƒå¾ªç¯
        train_losses = []
        train_accs = []
        val_losses = []
        val_accs = []
        val_aucs = []
        val_f1s = []
        
        best_train_acc = 0
        best_val_acc = 0
        best_val_auc = 0
        best_val_f1 = 0
        best_composite_score = 0
        best_model_state = None
        best_epoch = 0
        early_stop_counter = 0
        
        for epoch in range(1, num_epochs + 1):
            # è®­ç»ƒ
            model.train()
            train_loss = 0.0
            train_preds = []
            train_labels = []
            
            for x, y in train_loader:
                x = x.to(device).float()
                x = x.view(-1, 1, FIXED_LENGTH)
                y = y.to(device).float()
                
                optimizer.zero_grad()
                outputs = model(x)
                loss = criterion(outputs, y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                preds = (outputs.detach().cpu().numpy() >= 0.5).astype(int)
                train_preds.extend(preds.flatten())
                train_labels.extend(y.detach().cpu().numpy().flatten())
            
            # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
            avg_train_loss = train_loss / len(train_loader)
            train_acc = accuracy_score(train_labels, train_preds)
            
            train_losses.append(avg_train_loss)
            train_accs.append(train_acc)
            
            # éªŒè¯
            val_loss, val_acc, val_auc, val_labels, val_probs, val_precision, val_recall, val_f1 = self._validate_model(
                model, criterion, val_loader
            )
            
            val_losses.append(val_loss)
            val_accs.append(val_acc)
            val_aucs.append(val_auc)
            val_f1s.append(val_f1)
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†
            val_metrics = {
                'accuracy': val_acc,
                'auc': val_auc,
                'f1': val_f1
            }
            composite_score, breakdown = CompositeScoreCalculator.calculate_composite_score(val_metrics)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = False
            if composite_score > best_composite_score + MIN_DELTA:
                is_best = True
                best_composite_score = composite_score
                best_val_acc = val_acc
                best_val_auc = val_auc
                best_val_f1 = val_f1
                best_epoch = epoch
                best_model_state = model.state_dict().copy()
                early_stop_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
            else:
                early_stop_counter += 1
            
            # æ›´æ–°è®­ç»ƒæœ€ä½³å‡†ç¡®ç‡
            if train_acc > best_train_acc:
                best_train_acc = train_acc
            
            # æ‰“å°è¿›åº¦
            if epoch % 5 == 0 or epoch == 1 or epoch == num_epochs:
                print(f"  Epoch {epoch}/{num_epochs}:")
                print(f"    è®­ç»ƒ - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f}")
                print(f"    éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}, F1: {val_f1:.4f}")
                print(f"    ç»¼åˆè¯„åˆ†: {composite_score:.4f}")
            
            # æ—©åœæ£€æŸ¥
            if early_stop_counter >= EARLY_STOP_PATIENCE:
                print(f"  âš ï¸ æ—©åœè§¦å‘äºepoch {epoch}ï¼Œè¿ç»­{EARLY_STOP_PATIENCE}ä¸ªepochéªŒè¯é›†æ— æ˜¾è‘—æå‡")
                break
        
        print(f"  æœ€ä½³éªŒè¯ç»¼åˆè¯„åˆ†: {best_composite_score:.4f} (Epoch {best_epoch})")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€
        if best_model_state:
            model.load_state_dict(best_model_state)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        if save_model and self.experiment_dir:
            model_name = self.file_manager.generate_model_name(
                self._get_config_dict(),
                epoch=best_epoch,
                composite_score=best_composite_score
            )
            
            model_path = os.path.join(self.experiment_dir, "models", "final_" + model_name)
            metadata = {
                'best_train_acc': best_train_acc,
                'best_val_acc': best_val_acc,
                'best_val_auc': best_val_auc,
                'best_val_f1': best_val_f1,
                'best_composite_score': best_composite_score,
                'avg_train_loss': np.mean(train_losses),
                'num_epochs': epoch,  # å®é™…è®­ç»ƒçš„epochæ•°ï¼ˆå¯èƒ½å› æ—©åœè€Œå°äºnum_epochsï¼‰
                'best_epoch': best_epoch,
                'early_stopped': early_stop_counter >= EARLY_STOP_PATIENCE
            }
            self.file_manager.save_model(model, model_path, metadata)
        
        train_metrics = {
            'final_train_loss': train_losses[-1],
            'final_train_acc': train_accs[-1],
            'best_train_acc': best_train_acc,
            'best_val_acc': best_val_acc,
            'best_val_auc': best_val_auc,
            'best_val_f1': best_val_f1,
            'best_composite_score': best_composite_score,
            'avg_train_loss': np.mean(train_losses),
            'avg_val_loss': np.mean(val_losses),
            'best_epoch': best_epoch,
            'total_epochs': epoch,
            'early_stopped': early_stop_counter >= EARLY_STOP_PATIENCE
        }
        
        # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
        if self.experiment_dir:
            metrics_path = os.path.join(self.experiment_dir, "metrics", "final_training_metrics.json")
            self.file_manager.save_metrics(train_metrics, metrics_path)
        
        return model, train_metrics
    
    def _train_single_fold(self, train_data, val_data, fold_idx, num_epochs, save_model=True, min_epochs=10):
        """è®­ç»ƒå•ä¸ªæŠ˜ï¼ŒåŒ…å«æ—©åœæœºåˆ¶"""
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = FoldDataset(
            train_data, self.base_path, is_train=True, augment=self.augment
        )
        val_dataset = FoldDataset(
            val_data, self.base_path, is_train=False, augment=False
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset, batch_size=self.batch_size, 
            shuffle=True, num_workers=0
        )
        val_loader = DataLoader(
            val_dataset, batch_size=1, shuffle=False, num_workers=0
        )
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = Mscnn(
            INPUT_CHANNELS,
            OUTPUT_CLASSES,
            use_stream2=self.use_stream2,
            stream1_kernel=self.kernel_config['stream1_kernel'],
            stream2_first_kernel=self.kernel_config['stream2_first_kernel']
        ).to(device)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = torch.nn.BCELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)
        
        # è®­ç»ƒçŠ¶æ€
        best_val_acc = 0
        best_val_auc = 0
        best_val_f1 = 0
        best_composite_score = 0
        best_epoch = 0
        best_model_state = None
        early_stop_counter = 0
        
        # ç”¨äºå¯è§†åŒ–çš„å†å²è®°å½•
        train_losses = []
        train_accs = []
        val_losses = []
        val_accs = []
        val_aucs = []
        val_f1s = []

        # åˆ›å»ºepochè¿›åº¦æ¡
        epoch_pbar = tqdm(
            range(1, num_epochs + 1),
            desc=f"æŠ˜ {fold_idx+1} è®­ç»ƒè¿›åº¦",
            position=0,
            leave=True,
            dynamic_ncols=True,
            mininterval=1.0
        )

        # è®­ç»ƒå¾ªç¯
        for epoch in epoch_pbar:
            # è®­ç»ƒ
            model.train()
            train_loss = 0.0
            train_preds = []
            train_labels = []
            
            # åˆ›å»ºæ‰¹æ¬¡è¿›åº¦æ¡ - ä½¿ç”¨ä¸åŒçš„position
            batch_pbar = tqdm(
                enumerate(train_loader, 1),
                total=len(train_loader),
                desc="æ‰¹æ¬¡è®­ç»ƒ",
                position=1,
                leave=False,
                dynamic_ncols=True,
                mininterval=0.5,
                maxinterval=1.0,
                bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}'
            )
            for batch_idx, (x, y) in batch_pbar:
                x = x.to(device).float()
                x = x.view(-1, 1, FIXED_LENGTH)
                y = y.to(device).float()
                
                optimizer.zero_grad()
                outputs = model(x)
                loss = criterion(outputs, y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                preds = (outputs.detach().cpu().numpy() >= 0.5).astype(int)
                train_preds.extend(preds.flatten())
                train_labels.extend(y.detach().cpu().numpy().flatten())

                # æ›´æ–°æ‰¹æ¬¡è¿›åº¦æ¡ - ä½¿ç”¨æ›´è¯¦ç»†çš„æ ¼å¼
                avg_loss_so_far = train_loss / batch_idx
                batch_pbar.set_postfix({
                    'batch': f'{batch_idx}/{len(train_loader)}',
                    'loss': f'{loss.item():.4f}',
                    'avg_loss': f'{avg_loss_so_far:.4f}'
                })

            batch_pbar.close()
            
            # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
            train_acc = accuracy_score(train_labels, train_preds)
            avg_train_loss = train_loss / len(train_loader)

            train_losses.append(avg_train_loss)
            train_accs.append(train_acc)
            
            # éªŒè¯ - ä¿®å¤è¿™é‡Œçš„è§£åŒ…é—®é¢˜
            val_metrics = self._validate_model(model, criterion, val_loader)
            val_loss, val_acc, val_auc, _, _, val_precision, val_recall, val_f1 = val_metrics

            val_losses.append(val_loss)
            val_accs.append(val_acc)
            val_aucs.append(val_auc)
            val_f1s.append(val_f1)
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†
            val_metrics_dict = {
                'accuracy': val_acc,
                'auc': val_auc,
                'f1': val_f1
            }
            composite_score, breakdown = CompositeScoreCalculator.calculate_composite_score(val_metrics_dict)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = False
            if composite_score > best_composite_score + MIN_DELTA:
                is_best = True
                best_composite_score = composite_score
                best_val_acc = val_acc
                best_val_auc = val_auc
                best_val_f1 = val_f1
                best_epoch = epoch
                best_model_state = model.state_dict().copy()
                early_stop_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
            else:
                early_stop_counter += 1

            # æ›´æ–°epochè¿›åº¦æ¡
            epoch_pbar.set_postfix({
                'train_loss': avg_train_loss,
                'train_acc': train_acc,
                'val_acc': val_acc,
                'val_f1': val_f1,
                'best': 'â˜…' if is_best else ''
            })
            epoch_pbar.update(1)
            
            # æ—©åœæ£€æŸ¥ï¼ˆè‡³å°‘è®­ç»ƒmin_epochsä¸ªepochï¼‰
            if epoch >= min_epochs and early_stop_counter >= EARLY_STOP_PATIENCE:
                print(f"    â¹ï¸ æ—©åœè§¦å‘äºepoch {epoch}")
                break

        epoch_pbar.close()
        
        print(f"  æœ€ä½³éªŒè¯ç»¼åˆè¯„åˆ†: {best_composite_score:.4f} (Epoch {best_epoch})")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€
        if best_model_state:
            model.load_state_dict(best_model_state)
        
        # ä¿å­˜å½“å‰æŠ˜çš„æœ€ä½³æ¨¡å‹
        if save_model and self.experiment_dir:
            model_name = self.file_manager.generate_model_name(
                self._get_config_dict(),
                fold=fold_idx,
                epoch=best_epoch,
                composite_score=best_composite_score
            )
            
            model_path = os.path.join(self.experiment_dir, "models", f"fold{fold_idx}_" + model_name)
            metadata = {
                'fold': fold_idx,
                'val_acc': best_val_acc,
                'val_auc': best_val_auc,
                'val_f1': best_val_f1,
                'composite_score': best_composite_score,
                'epoch': best_epoch,
                'early_stopped': early_stop_counter >= EARLY_STOP_PATIENCE
            }
            self.file_manager.save_model(model, model_path, metadata)
        
        fold_result = {
            'fold': fold_idx,
            'best_val_acc': float(best_val_acc),
            'best_val_auc': float(best_val_auc),
            'best_val_f1': float(best_val_f1),
            'best_composite_score': float(best_composite_score),
            'best_epoch': best_epoch,
            'early_stopped': early_stop_counter >= EARLY_STOP_PATIENCE,
            'total_epochs': epoch
        }
        
        return fold_result, model
    
    def _validate_model(self, model, criterion, val_loader):
        """éªŒè¯æ¨¡å‹"""
        model.eval()
        running_loss = 0.0
        all_probs = []
        all_labels = []
        
        with torch.no_grad():
            for x, y in val_loader:
                x = x.to(device).float()
                x = x.view(-1, 1, FIXED_LENGTH)
                y = y.to(device).float()
                
                probs = model(x)
                loss = criterion(probs, y)
                running_loss += loss.item()
                
                all_probs.extend(probs.cpu().numpy().flatten())
                all_labels.extend(y.cpu().numpy().flatten())
        
        # è®¡ç®—æŒ‡æ ‡
        avg_loss = running_loss / len(val_loader)
        all_probs = np.array(all_probs)
        all_labels = np.array(all_labels)
        
        preds = (all_probs >= 0.5).astype(int)
        acc = accuracy_score(all_labels, preds)
        
        try:
            auc = roc_auc_score(all_labels, all_probs)
        except:
            auc = 0.5
        
        try:
            precision = precision_score(all_labels, preds, average='binary', zero_division=0)
            recall = recall_score(all_labels, preds, average='binary', zero_division=0)
            f1 = f1_score(all_labels, preds, average='binary', zero_division=0)
        except:
            precision = 0.0
            recall = 0.0
            f1 = 0.0
        
        model.train()
        return avg_loss, acc, auc, all_labels, all_probs, precision, recall, f1
    
    def _compute_average_metrics(self, fold_results):
        if not fold_results:
            return {
                'avg_val_acc': 0.0,
                'std_val_acc': 0.0,
                'avg_val_auc': 0.0,
                'std_val_auc': 0.0,
                'avg_val_f1': 0.0,
                'std_val_f1': 0.0,
                'num_folds': 0
            }
    
        val_accs = [r['best_val_acc'] for r in fold_results]
        val_aucs = [r['best_val_auc'] for r in fold_results]
        val_f1s = [r['best_val_f1'] for r in fold_results]
        
        return {
            'avg_val_acc': float(np.mean(val_accs)),
            'std_val_acc': float(np.std(val_accs)),
            'avg_val_auc': float(np.mean(val_aucs)),
            'std_val_auc': float(np.std(val_aucs)),
            'avg_val_f1': float(np.mean(val_f1s)),
            'std_val_f1': float(np.std(val_f1s)),
            'num_folds': len(fold_results)
        }
    
    def evaluate_on_test_set(self, test_cv_indices, model, save_results=True):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        print(f"\nåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹")
        print(f"æµ‹è¯•é›†: CV{', '.join(map(str, test_cv_indices))}")
        
        # åŠ è½½æµ‹è¯•é›†æ•°æ®
        test_data = self.data_manager.load_cv_files(test_cv_indices)
        if len(test_data) == 0:
            print("é”™è¯¯: æµ‹è¯•é›†æ•°æ®ä¸ºç©º")
            return {}
        
        # åˆ›å»ºæ•°æ®é›†
        test_dataset = FoldDataset(
            test_data, self.base_path, is_train=False, augment=False
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        test_loader = DataLoader(
            test_dataset, batch_size=1, shuffle=False, num_workers=0
        )
        
        # è¯„ä¼°
        criterion = torch.nn.BCELoss()
        test_loss, test_acc, test_auc, _, _, test_precision, test_recall, test_f1 = self._validate_model(
            model, criterion, test_loader
        )
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        test_metrics = {
            'accuracy': test_acc,
            'auc': test_auc,
            'f1': test_f1
        }
        test_composite_score, breakdown = CompositeScoreCalculator.calculate_composite_score(test_metrics)
        
        test_results = {
            'test_acc': test_acc,
            'test_auc': test_auc,
            'test_precision': test_precision,
            'test_recall': test_recall,
            'test_f1': test_f1,
            'test_composite_score': test_composite_score,
            'test_loss': test_loss,
            'evaluation_time': datetime.now().isoformat(),
            'score_breakdown': breakdown
        }
        
        print(f"æµ‹è¯•é›†ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {test_acc:.4f}")
        print(f"  AUC: {test_auc:.4f}")
        print(f"  ç²¾ç¡®ç‡: {test_precision:.4f}")
        print(f"  å¬å›ç‡: {test_recall:.4f}")
        print(f"  F1åˆ†æ•°: {test_f1:.4f}")
        print(f"  ç»¼åˆè¯„åˆ†: {test_composite_score:.4f}")
        print(f"  æŸå¤±: {test_loss:.4f}")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        if save_results and self.experiment_dir:
            results_path = os.path.join(self.experiment_dir, "metrics", "test_evaluation.json")
            self.file_manager.save_metrics(test_results, results_path)
        
        return test_results
    
    def _visualize_cv_results(self, fold_results, avg_metrics):
        """å¯è§†åŒ–äº¤å‰éªŒè¯ç»“æœ"""
        if not self.experiment_dir:
            return
        
        # ç»˜åˆ¶å„æŠ˜æ€§èƒ½å¯¹æ¯”
        fold_accs = [r['best_val_acc'] for r in fold_results]
        fold_aucs = [r['best_val_auc'] for r in fold_results]
        fold_f1s = [r['best_val_f1'] for r in fold_results]
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # å„æŠ˜å‡†ç¡®ç‡
        axes[0].bar(range(1, len(fold_accs) + 1), fold_accs, color='skyblue', edgecolor='black')
        axes[0].axhline(y=avg_metrics['avg_val_acc'], color='red', linestyle='--', 
                       label=f'å¹³å‡å€¼: {avg_metrics["avg_val_acc"]:.4f}')
        axes[0].set_title('å„æŠ˜éªŒè¯å‡†ç¡®ç‡', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('æŠ˜æ•°', fontsize=12)
        axes[0].set_ylabel('å‡†ç¡®ç‡', fontsize=12)
        axes[0].set_xticks(range(1, len(fold_accs) + 1))
        axes[0].legend()
        axes[0].grid(True, alpha=0.3, axis='y')
        
        # å„æŠ˜AUC
        axes[1].bar(range(1, len(fold_aucs) + 1), fold_aucs, color='lightgreen', edgecolor='black')
        axes[1].axhline(y=avg_metrics['avg_val_auc'], color='red', linestyle='--', 
                       label=f'å¹³å‡å€¼: {avg_metrics["avg_val_auc"]:.4f}')
        axes[1].set_title('å„æŠ˜AUCåˆ†æ•°', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('æŠ˜æ•°', fontsize=12)
        axes[1].set_ylabel('AUC', fontsize=12)
        axes[1].set_xticks(range(1, len(fold_aucs) + 1))
        axes[1].legend()
        axes[1].grid(True, alpha=0.3, axis='y')
        
        # å„æŠ˜F1åˆ†æ•°
        axes[2].bar(range(1, len(fold_f1s) + 1), fold_f1s, color='lightcoral', edgecolor='black')
        axes[2].axhline(y=avg_metrics['avg_val_f1'], color='red', linestyle='--', 
                       label=f'å¹³å‡å€¼: {avg_metrics["avg_val_f1"]:.4f}')
        axes[2].set_title('å„æŠ˜F1åˆ†æ•°', fontsize=14, fontweight='bold')
        axes[2].set_xlabel('æŠ˜æ•°', fontsize=12)
        axes[2].set_ylabel('F1åˆ†æ•°', fontsize=12)
        axes[2].set_xticks(range(1, len(fold_f1s) + 1))
        axes[2].legend()
        axes[2].grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        vis_path = os.path.join(self.experiment_dir, "visualizations", "cv_results_comparison.png")
        plt.savefig(vis_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š äº¤å‰éªŒè¯ç»“æœå¯è§†åŒ–å·²ä¿å­˜: {vis_path}")

    def _get_config_dict(self):
        """è·å–å½“å‰è®­ç»ƒé…ç½®"""
        return {
            'kernel_config': self.kernel_config,
            'batch_size': self.batch_size,
            'lr': self.lr,
            'use_stream2': self.use_stream2,
            'augment': self.augment,
            'config_name': self.config_name,
            'composite_weights': self.composite_weights
        }


# ==================== æ”¹è¿›çš„è¶…å‚æ•°æœç´¢æ¨¡å— ====================
class HyperparameterSearcher:
    """è¶…å‚æ•°æœç´¢å™¨ï¼Œä½¿ç”¨ç»¼åˆè¯„åˆ†é€‰æ‹©æœ€ä½³é…ç½®"""
    
    # æœç´¢é…ç½®
    KERNEL_CONFIGS = [
        {'name': 'MS-CNN(3,9)', 'stream1_kernel': 3, 'stream2_first_kernel': 9},
        {'name': 'MS-CNN(3,7)', 'stream1_kernel': 3, 'stream2_first_kernel': 7},
        {'name': 'MS-CNN(3,5)', 'stream1_kernel': 3, 'stream2_first_kernel': 5},
        {'name': 'MS-CNN(3,3)', 'stream1_kernel': 3, 'stream2_first_kernel': 3},
    ]
    
    BATCH_SIZES = [32, 64, 128]
    
    def __init__(self, base_path, composite_weights=None):
        self.base_path = base_path
        self.file_manager = ModelFileManager()
        self.composite_weights = composite_weights
        self.visualizer = TrainingVisualizer()
    
    def search(self, num_epochs_search=20):
        """æ‰§è¡Œè¶…å‚æ•°æœç´¢ï¼Œä½¿ç”¨ç»¼åˆè¯„åˆ†è¯„ä¼°é…ç½®"""
        print("=" * 80)
        print("è¶…å‚æ•°æœç´¢æ¨¡å¼ï¼ˆä½¿ç”¨ç»¼åˆè¯„åˆ†å’Œæ—©åœæœºåˆ¶ï¼‰")
        print("=" * 80)
        
        # åˆ›å»ºæœç´¢ç›®å½•
        search_dir = self.file_manager.create_experiment_dir(
            "/home/xusi/EE5046_Projects/Task1_Results/HyperparamSearch",
            "HyperparamSearch"
        )
        
        # ä¿å­˜æœç´¢é…ç½®
        search_config = {
            'kernel_configs': self.KERNEL_CONFIGS,
            'batch_sizes': self.BATCH_SIZES,
            'num_epochs': num_epochs_search,
            'learning_rate': LEARNING_RATE,
            'use_stream2': USE_STREAM2_SETTING,
            'augment': AUGMENT_SETTING,
            'composite_weights': self.composite_weights,
            'search_time': datetime.now().isoformat()
        }
        
        config_path = os.path.join(search_dir, "configs", "search_config.json")
        self.file_manager.save_config(search_config, config_path)
        
        best_composite_score = 0
        best_config = None
        all_results = {}
        
        # è®¡ç®—æ€»é…ç½®æ•°
        total_configs = len(self.KERNEL_CONFIGS) * len(self.BATCH_SIZES)
        
        # åˆ›å»ºæ€»ä½“æœç´¢è¿›åº¦æ¡
        config_pbar = self.visualizer.create_progress_bar(total_configs, "è¶…å‚æ•°æœç´¢è¿›åº¦")
        config_idx = 0

        # éå†æ‰€æœ‰è¶…å‚æ•°ç»„åˆ
        for kernel_config in self.KERNEL_CONFIGS:
            for batch_size in self.BATCH_SIZES:
                config_name = f"{kernel_config['name']}_BS{batch_size}"
                print(f"\n{'='*50}")
                print(f"æµ‹è¯•é…ç½®: {config_name}")
                print(f"{'='*50}")
                
                # ä¸ºæ¯ä¸ªé…ç½®åˆ›å»ºå­ç›®å½•
                config_dir = os.path.join(search_dir, "configs", config_name)
                subdirs = [
                    "models",           # ä¿å­˜æ¨¡å‹æ–‡ä»¶
                    "logs",            # è®­ç»ƒæ—¥å¿—
                    "configs",         # é…ç½®æ–‡ä»¶
                    "metrics",         # æ€§èƒ½æŒ‡æ ‡
                    "visualizations"   # å¯è§†åŒ–å›¾è¡¨
                ]
                for subdir in subdirs:
                    os.makedirs(os.path.join(config_dir, subdir), exist_ok=True)
                
                # åˆ›å»ºè®­ç»ƒå™¨
                trainer = ModelTrainer(
                    base_path=self.base_path,
                    kernel_config=kernel_config,
                    batch_size=batch_size,
                    lr=LEARNING_RATE,
                    use_stream2=USE_STREAM2_SETTING,
                    augment=AUGMENT_SETTING,
                    experiment_dir=config_dir,
                    config_name=config_name,
                    composite_weights=self.composite_weights
                )
                
                # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯
                train_indices = [0, 1, 2, 3]
                avg_metrics, fold_results = trainer.cross_validate_on_train_set(
                    train_indices, num_epochs_search, k_folds=5
                )

                avg_composite_score = avg_metrics.get('avg_composite_score', 0)
                
                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                config_pbar.set_postfix({
                    'config': config_name,
                    'score': f"{avg_composite_score:.4f}",
                    'best': 'â˜…' if avg_composite_score > best_composite_score else ''
                })

                
                # ä¿å­˜é…ç½®ç»“æœ
                config_result = {
                    'config_name': config_name,
                    'kernel_config': kernel_config,
                    'batch_size': batch_size,
                    'avg_accuracy': avg_metrics['avg_val_acc'],
                    'avg_auc': avg_metrics['avg_val_auc'],
                    'avg_f1': avg_metrics['avg_val_f1'],
                    'avg_composite_score': avg_composite_score,
                    'std_accuracy': avg_metrics['std_val_acc'],
                    'std_auc': avg_metrics['std_val_auc'],
                    'std_f1': avg_metrics['std_val_f1'],
                    'fold_results': fold_results,
                    'directory': config_dir
                }
                
                result_path = os.path.join(config_dir, f"{config_name}_results.json")
                self.file_manager.save_metrics(config_result, result_path)
                
                all_results[config_name] = config_result
                
                # æ›´æ–°æœ€ä½³é…ç½®ï¼ˆåŸºäºç»¼åˆè¯„åˆ†ï¼‰
                if avg_composite_score > best_composite_score + MIN_DELTA:
                    best_composite_score = avg_composite_score
                    best_config = config_result
                    print(f"  ğŸ¯ æ–°çš„æœ€ä½³é…ç½®!")

                # æ›´æ–°è¿›åº¦æ¡
                config_pbar.update(1)

        config_pbar.close()
        
        # ä¿å­˜æ‰€æœ‰ç»“æœå’Œæœ€ä½³é…ç½®
        self._save_search_results(all_results, best_config, search_dir)

        # å¯è§†åŒ–æœç´¢ç»“æœ
        self._visualize_search_results(all_results, search_dir)
        
        return best_config, search_dir
    
    def _save_search_results(self, all_results, best_config, search_dir):
        """ä¿å­˜æœç´¢ç»“æœ"""
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        all_results_path = os.path.join(search_dir, "metrics", "all_search_results.json")
        self.file_manager.save_metrics(all_results, all_results_path)
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        sorted_configs = sorted(
            all_results.items(),
            key=lambda x: x[1]['avg_composite_score'],
            reverse=True
        )
        
        # åˆ›å»ºæ’åæŠ¥å‘Š
        ranking_report = {
            'search_timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'total_configs_tested': len(all_results),
            'ranking': []
        }
        
        print(f"\n{'='*80}")
        print("è¶…å‚æ•°æœç´¢æ’åç»“æœï¼ˆæŒ‰ç»¼åˆè¯„åˆ†ï¼‰:")
        print(f"{'='*80}")
        
        for rank, (config_name, data) in enumerate(sorted_configs, 1):
            marker = " ğŸ†" if rank == 1 else ""
            print(f"{rank:2d}. {config_name:25s} "
                  f"ç»¼åˆè¯„åˆ†: {data['avg_composite_score']:.4f} | "
                  f"å‡†ç¡®ç‡: {data['avg_accuracy']:.4f} | "
                  f"AUC: {data['avg_auc']:.4f} | "
                  f"F1: {data['avg_f1']:.4f}{marker}")
            
            ranking_report['ranking'].append({
                'rank': rank,
                'config_name': config_name,
                'avg_composite_score': data['avg_composite_score'],
                'avg_accuracy': data['avg_accuracy'],
                'avg_auc': data['avg_auc'],
                'avg_f1': data['avg_f1']
            })
        
        # ä¿å­˜æœ€ä½³é…ç½®
        best_config_data = {
            'best_config_name': best_config['config_name'],
            'best_config_data': best_config,
            'search_timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'selection_criteria': 'composite_score',
            'composite_weights': self.composite_weights
        }
        
        best_config_path = os.path.join(search_dir, "configs", "best_config.json")
        self.file_manager.save_config(best_config_data, best_config_path)
        
        # åŒæ—¶ä¿å­˜åˆ°æ ‡å‡†ä½ç½®ä»¥ä¾¿å®Œæ•´è®­ç»ƒä½¿ç”¨
        latest_best_path = "/home/xusi/EE5046_Projects/Task1_Results/HyperparamSearch/latest_best_config.json"
        self.file_manager.save_config(best_config_data, latest_best_path)
        
        # ä¿å­˜æ’åæŠ¥å‘Š
        ranking_path = os.path.join(search_dir, "metrics", "ranking_report.json")
        self.file_manager.save_metrics(ranking_report, ranking_path)
        
        print(f"\n{'='*80}")
        print("è¶…å‚æ•°æœç´¢å®Œæˆ!")
        print(f"{'='*80}")
        print(f"æœ€ä½³é…ç½®: {best_config['config_name']}")
        print(f"ç»¼åˆè¯„åˆ†: {best_config['avg_composite_score']:.4f}")
        print(f"å¹³å‡å‡†ç¡®ç‡: {best_config['avg_accuracy']:.4f}")
        print(f"å¹³å‡AUC: {best_config['avg_auc']:.4f}")
        print(f"å¹³å‡F1åˆ†æ•°: {best_config['avg_f1']:.4f}")
        print(f"ç»“æœç›®å½•: {search_dir}")
        print(f"æœ€ä½³é…ç½®å·²ä¿å­˜: {latest_best_path}")


    def _visualize_search_results(self, all_results, search_dir):
        """å¯è§†åŒ–è¶…å‚æ•°æœç´¢ç»“æœ"""
        if not all_results:
            return
        
        # æå–é…ç½®åç§°å’ŒæŒ‡æ ‡
        config_names = []
        composite_scores = []
        accuracies = []
        aucs = []
        f1s = []
        
        for config_name, data in all_results.items():
            config_names.append(config_name)
            composite_scores.append(data['avg_composite_score'])
            accuracies.append(data['avg_accuracy'])
            aucs.append(data['avg_auc'])
            f1s.append(data['avg_f1'])
        
        # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # ç»¼åˆè¯„åˆ†æ’å
        sorted_indices = np.argsort(composite_scores)[::-1]
        sorted_names = [config_names[i] for i in sorted_indices]
        sorted_scores = [composite_scores[i] for i in sorted_indices]
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(sorted_scores)))
        bars1 = axes[0, 0].bar(range(len(sorted_scores)), sorted_scores, color=colors)
        axes[0, 0].set_title('è¶…å‚æ•°é…ç½®ç»¼åˆè¯„åˆ†æ’å', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('é…ç½®æ’å', fontsize=12)
        axes[0, 0].set_ylabel('ç»¼åˆè¯„åˆ†', fontsize=12)
        axes[0, 0].set_xticks(range(len(sorted_scores)))
        axes[0, 0].set_xticklabels([f'#{i+1}' for i in range(len(sorted_scores))], rotation=45)
        axes[0, 0].grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars1, sorted_scores):
            height = bar.get_height()
            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                          f'{score:.3f}', ha='center', va='bottom', fontsize=9)
        
        # å‡†ç¡®ç‡å¯¹æ¯”
        axes[0, 1].bar(range(len(config_names)), accuracies, color='skyblue', edgecolor='black')
        axes[0, 1].set_title('å„é…ç½®å¹³å‡å‡†ç¡®ç‡', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('é…ç½®', fontsize=12)
        axes[0, 1].set_ylabel('å‡†ç¡®ç‡', fontsize=12)
        axes[0, 1].set_xticks(range(len(config_names)))
        axes[0, 1].set_xticklabels(config_names, rotation=45, ha='right')
        axes[0, 1].grid(True, alpha=0.3, axis='y')
        
        # AUCå¯¹æ¯”
        axes[1, 0].bar(range(len(config_names)), aucs, color='lightgreen', edgecolor='black')
        axes[1, 0].set_title('å„é…ç½®å¹³å‡AUC', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('é…ç½®', fontsize=12)
        axes[1, 0].set_ylabel('AUC', fontsize=12)
        axes[1, 0].set_xticks(range(len(config_names)))
        axes[1, 0].set_xticklabels(config_names, rotation=45, ha='right')
        axes[1, 0].grid(True, alpha=0.3, axis='y')
        
        # F1åˆ†æ•°å¯¹æ¯”
        axes[1, 1].bar(range(len(config_names)), f1s, color='lightcoral', edgecolor='black')
        axes[1, 1].set_title('å„é…ç½®å¹³å‡F1åˆ†æ•°', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('é…ç½®', fontsize=12)
        axes[1, 1].set_ylabel('F1åˆ†æ•°', fontsize=12)
        axes[1, 1].set_xticks(range(len(config_names)))
        axes[1, 1].set_xticklabels(config_names, rotation=45, ha='right')
        axes[1, 1].grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        vis_path = os.path.join(search_dir, "visualizations", "search_results_comparison.png")
        plt.savefig(vis_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š è¶…å‚æ•°æœç´¢ç»“æœå¯è§†åŒ–å·²ä¿å­˜: {vis_path}")


# ==================== å®Œæ•´è®­ç»ƒæ¨¡å—ï¼ˆä¿®æ”¹ç‰ˆï¼‰ ====================
class CompleteTrainer:
    """ä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ"""
    
    @staticmethod
    def train_with_best_config(base_path, best_config_data):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹ï¼š
        1. åœ¨è®­ç»ƒé›†ï¼ˆCV0~CV3ï¼‰ä¸Šä½¿ç”¨æœ€ä½³è¶…å‚æ•°è¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯
        2. ä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆåŒ…å«éªŒè¯é›†å’Œæ—©åœï¼‰
        3. åœ¨æµ‹è¯•é›†ï¼ˆCV4ï¼‰ä¸Šæœ€ç»ˆè¯„ä¼°
        """
        print("=" * 80)
        print("å®Œæ•´è®­ç»ƒæ¨¡å¼ï¼ˆä½¿ç”¨ç»¼åˆè¯„åˆ†å’Œæ—©åœæœºåˆ¶ï¼‰")
        print("=" * 80)
        
        # åˆ›å»ºå®éªŒç›®å½•
        file_manager = ModelFileManager()
        experiment_dir = file_manager.create_experiment_dir(
            "/home/xusi/EE5046_Projects/Task1_Results/CompleteTraining",
            "CompleteTraining",
            f"{best_config_data['kernel_config']['name']}_BS{best_config_data['batch_size']}"
        )
        
        # 1. åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯
        print("\næ­¥éª¤1: åœ¨è®­ç»ƒé›†ï¼ˆCV0~CV3ï¼‰ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯")
        
        # ä½¿ç”¨æœ€ä½³é…ç½®ä¸­çš„ç»¼åˆè¯„åˆ†æƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
        composite_weights = best_config_data.get('composite_weights')
        
        trainer = ModelTrainer(
            base_path=base_path,
            kernel_config=best_config_data['kernel_config'],
            batch_size=best_config_data['batch_size'],
            lr=LEARNING_RATE,
            use_stream2=USE_STREAM2_SETTING,
            augment=AUGMENT_SETTING,
            experiment_dir=experiment_dir,
            config_name="BestConfig",
            composite_weights=composite_weights
        )
        
        train_indices = [0, 1, 2, 3]
        cv_metrics, cv_results = trainer.cross_validate_on_train_set(
            train_indices, NUM_EPOCHS, k_folds=5
        )
        
        print(f"\n5æŠ˜äº¤å‰éªŒè¯ç»“æœ:")
        print(f"å¹³å‡å‡†ç¡®ç‡: {cv_metrics['avg_val_acc']:.4f} Â± {cv_metrics['std_val_acc']:.4f}")
        print(f"å¹³å‡AUC: {cv_metrics['avg_val_auc']:.4f} Â± {cv_metrics['std_val_auc']:.4f}")
        print(f"å¹³å‡F1åˆ†æ•°: {cv_metrics['avg_val_f1']:.4f} Â± {cv_metrics['std_val_f1']:.4f}")
        print(f"å¹³å‡ç»¼åˆè¯„åˆ†: {cv_metrics.get('avg_composite_score', 0):.4f}")
        
        # 2. ä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆåŒ…å«éªŒè¯é›†å’Œæ—©åœï¼‰
        print("\næ­¥éª¤2: ä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†ï¼ˆCV0~CV3ï¼‰è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆåŒ…å«éªŒè¯é›†ï¼‰")
        final_model, train_metrics = trainer.train_final_model(train_indices, NUM_EPOCHS)
        
        # 3. åœ¨æµ‹è¯•é›†ï¼ˆCV4ï¼‰ä¸Šè¯„ä¼°
        print("\næ­¥éª¤3: åœ¨æµ‹è¯•é›†ï¼ˆCV4ï¼‰ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹")
        test_results = trainer.evaluate_on_test_set([4], final_model)
        
        print(f"\næœ€ç»ˆæµ‹è¯•ç»“æœ:")
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_results['test_acc']:.4f}")
        print(f"æµ‹è¯•é›†AUC: {test_results['test_auc']:.4f}")
        print(f"æµ‹è¯•é›†F1åˆ†æ•°: {test_results['test_f1']:.4f}")
        print(f"æµ‹è¯•é›†ç»¼åˆè¯„åˆ†: {test_results['test_composite_score']:.4f}")
        
        # ä¿å­˜å®Œæ•´è®­ç»ƒæ‘˜è¦
        summary = {
            'cross_validation': cv_metrics,
            'final_training': train_metrics,
            'test_evaluation': test_results,
            'best_config': best_config_data,
            'complete_training_time': datetime.now().isoformat(),
            'early_stop_patience': EARLY_STOP_PATIENCE,
            'min_delta': MIN_DELTA,
            'composite_weights': composite_weights
        }
        
        summary_path = os.path.join(experiment_dir, "metrics", "complete_training_summary.json")
        file_manager.save_metrics(summary, summary_path)
        
        return cv_metrics, test_results, experiment_dir


# ==================== ä¸»ç¨‹åºæ¨¡å—ï¼ˆä¿®æ”¹ç‰ˆï¼‰ ====================
class TrainingPipeline:
    """è®­ç»ƒç®¡é“ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        # æ•°æ®é›†è·¯å¾„
        self.base_path = "/home/xusi/EE5046_Projects/Dataset"

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.base_path):
            print(f"âŒ Datasetç›®å½•ä¸å­˜åœ¨: {self.base_path}")
            print("è¯·ç¡®ä¿Datasetç›®å½•åœ¨æŒ‡å®šä½ç½®")
            sys.exit(1)
        
        # æ£€æŸ¥cvç›®å½•
        cv_dir = os.path.join(self.base_path, "cv")
        if not os.path.exists(cv_dir):
            print(f"âŒ cvç›®å½•ä¸å­˜åœ¨: {cv_dir}")
            print("è¯·ç¡®ä¿Datasetç›®å½•åŒ…å«cvå­ç›®å½•")
            sys.exit(1)
        
        # æ£€æŸ¥training2017ç›®å½•
        training_dir = os.path.join(self.base_path, "training2017")
        if not os.path.exists(training_dir):
            print(f"âŒ training2017ç›®å½•ä¸å­˜åœ¨: {training_dir}")
            print("è¯·ç¡®ä¿Datasetç›®å½•åŒ…å«training2017å­ç›®å½•")
            sys.exit(1)
        
        print(f"âœ… ä½¿ç”¨æ•°æ®é›†ç›®å½•: {self.base_path}")
        print(f"âœ… cvç›®å½•: {cv_dir}")
        print(f"âœ… training2017ç›®å½•: {training_dir}")
        
        # æ£€æŸ¥CSVæ–‡ä»¶
        for i in range(5):
            csv_file = os.path.join(cv_dir, f"cv{i}.csv")
            if os.path.exists(csv_file):
                print(f"âœ… æ‰¾åˆ°æ–‡ä»¶: cv{i}.csv")
            else:
                print(f"âš ï¸ è­¦å‘Š: æ–‡ä»¶ cv{i}.csv ä¸å­˜åœ¨")

        self.train_indices = [0, 1, 2, 3]
        self.test_indices = [4]
        
        # è‡ªå®šä¹‰ç»¼åˆè¯„åˆ†æƒé‡ï¼ˆå¯æ ¹æ®ä»»åŠ¡è°ƒæ•´ï¼‰
        self.custom_weights = {
            'accuracy': 0.40,  # æé«˜å‡†ç¡®ç‡æƒé‡
            'auc': 0.35,       # AUCæƒé‡
            'f1': 0.20,        # F1åˆ†æ•°æƒé‡
            'stability': 0.05  # ç¨³å®šæ€§æƒé‡
        }
    
    def run(self):
        """è¿è¡Œè®­ç»ƒç®¡é“"""
        parser = argparse.ArgumentParser(description='ECGæˆ¿é¢¤æ£€æµ‹è®­ç»ƒè„šæœ¬ï¼ˆæ”¹è¿›ç‰ˆï¼‰')
        parser.add_argument('--mode', type=str, default=EXPERIMENT_MODE,
                            choices=['search', 'train', 'full', 'compare'],
                            help='è¿è¡Œæ¨¡å¼')
        parser.add_argument('--compare_mode', type=str, default=COMPARISON_MODE,
                            choices=['stream', 'augment'],
                            help='å¯¹æ¯”å®éªŒæ¨¡å¼')
        parser.add_argument('--weights', type=str, default='default',
                            choices=['default', 'accuracy_focus', 'balanced', 'auc_focus'],
                            help='ç»¼åˆè¯„åˆ†æƒé‡ç­–ç•¥')
        
        parser.add_argument('--no_progress', action='store_true',
                            help='ç¦ç”¨è¿›åº¦æ¡æ˜¾ç¤º')
        
        args = parser.parse_args()

        if args.no_progress:
            from tqdm import tqdm
            tqdm.__init__ = lambda self, *args, **kwargs: None
        
        print(f"\næ•°æ®é›†åˆ’åˆ†ç­–ç•¥:")
        print(f"è®­ç»ƒé›†: CV{', '.join(map(str, self.train_indices))}")
        print(f"æµ‹è¯•é›†: CV{', '.join(map(str, self.test_indices))}")
        print(f"æ—©åœè€å¿ƒå€¼: {EARLY_STOP_PATIENCE}, æœ€å°æå‡: {MIN_DELTA}")
        
        # æ ¹æ®æƒé‡ç­–ç•¥é€‰æ‹©æƒé‡
        if args.weights == 'accuracy_focus':
            weights = {'accuracy': 0.50, 'auc': 0.30, 'f1': 0.15, 'stability': 0.05}
        elif args.weights == 'auc_focus':
            weights = {'accuracy': 0.30, 'auc': 0.50, 'f1': 0.15, 'stability': 0.05}
        elif args.weights == 'balanced':
            weights = {'accuracy': 0.35, 'auc': 0.35, 'f1': 0.25, 'stability': 0.05}
        else:
            weights = None  # ä½¿ç”¨é»˜è®¤æƒé‡
        
        if weights:
            print(f"ç»¼åˆè¯„åˆ†æƒé‡: {weights}")
        
        if args.mode == 'search':
            self._run_search_mode(weights)
        elif args.mode == 'train':
            self._run_train_mode(weights)
        elif args.mode == 'full':
            self._run_full_mode(args, weights)
        elif args.mode == 'compare':
            self._run_compare_mode(args, weights)
    
    def _run_search_mode(self, weights):
        """è¿è¡Œè¶…å‚æ•°æœç´¢æ¨¡å¼"""
        print("\næ¨¡å¼: è¶…å‚æ•°æœç´¢ï¼ˆä½¿ç”¨ç»¼åˆè¯„åˆ†ï¼‰")
        
        searcher = HyperparameterSearcher(self.base_path, composite_weights=weights)
        best_config, search_dir = searcher.search(num_epochs_search=30)
        
        print(f"\nè¶…å‚æ•°æœç´¢å®Œæˆ!")
        print(f"æœ€ä½³é…ç½®: {best_config['config_name']}")
        print(f"ç»¼åˆè¯„åˆ†: {best_config['avg_composite_score']:.4f}")
        print(f"ç»“æœç›®å½•: {search_dir}")
    
    def _run_train_mode(self, weights):
        """è¿è¡Œé»˜è®¤è®­ç»ƒæ¨¡å¼"""
        print("\næ¨¡å¼: é»˜è®¤é…ç½®è®­ç»ƒï¼ˆä½¿ç”¨ç»¼åˆè¯„åˆ†å’Œæ—©åœï¼‰")
        
        # åˆ›å»ºå®éªŒç›®å½•
        file_manager = ModelFileManager()
        experiment_dir = file_manager.create_experiment_dir(
            "/home/xusi/EE5046_Projects/Task1_Results/DefaultTraining",
            "DefaultTraining",
            f"{DEFAULT_KERNEL_CONFIG['name']}_BS{BATCH_SIZE}"
        )
        
        trainer = ModelTrainer(
            base_path=self.base_path,
            kernel_config=DEFAULT_KERNEL_CONFIG,
            batch_size=BATCH_SIZE,
            lr=LEARNING_RATE,
            use_stream2=USE_STREAM2_SETTING,
            augment=AUGMENT_SETTING,
            experiment_dir=experiment_dir,
            config_name="DefaultConfig",
            composite_weights=weights
        )
        
        # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯
        cv_metrics, cv_results = trainer.cross_validate_on_train_set(
            self.train_indices, NUM_EPOCHS, k_folds=5
        )
        
        print(f"\né»˜è®¤è®­ç»ƒå®Œæˆ!")
        print(f"å¹³å‡ç»¼åˆè¯„åˆ†: {cv_metrics.get('avg_composite_score', 0):.4f}")
        print(f"ç»“æœç›®å½•: {experiment_dir}")
    
    def _run_full_mode(self, args, weights):
        """è¿è¡Œå®Œæ•´è®­ç»ƒæ¨¡å¼"""
        print("\næ¨¡å¼: ä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ")
        
        best_config_path = "/home/xusi/EE5046_Projects/Task1_Results/HyperparamSearch/latest_best_config.json"
        
        if os.path.exists(best_config_path):
            with open(best_config_path, 'r') as f:
                best_config_data = json.load(f)
            
            print(f"åŠ è½½æœ€ä½³é…ç½®: {best_config_data['best_config_name']}")
            print(f"é€‰æ‹©æ ‡å‡†: {best_config_data.get('selection_criteria', 'accuracy')}")
            
            # ä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ
            cv_metrics, test_results, experiment_dir = CompleteTrainer.train_with_best_config(
                self.base_path, best_config_data['best_config_data']
            )
            
            print(f"\nå®Œæ•´è®­ç»ƒå®Œæˆ!")
            print(f"äº¤å‰éªŒè¯å¹³å‡ç»¼åˆè¯„åˆ†: {cv_metrics.get('avg_composite_score', 0):.4f}")
            print(f"æµ‹è¯•é›†ç»¼åˆè¯„åˆ†: {test_results['test_composite_score']:.4f}")
            print(f"ç»“æœç›®å½•: {experiment_dir}")
        else:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æœ€ä½³é…ç½®æ–‡ä»¶ {best_config_path}")
            print("è¯·å…ˆè¿è¡Œè¶…å‚æ•°æœç´¢æ¨¡å¼: python TrainProcess.py --mode search")
    
    def _run_compare_mode(self, args, weights):
        """è¿è¡Œå¯¹æ¯”å®éªŒæ¨¡å¼"""
        print("\næ¨¡å¼: å¯¹æ¯”å®éªŒ")
        print("æ³¨æ„: å¯¹æ¯”å®éªŒæ¨¡å¼æš‚æœªå®ç°ç»¼åˆè¯„åˆ†ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘")
        
        # å¯¹æ¯”å®éªŒæ¨¡å—éœ€è¦ç›¸åº”ä¿®æ”¹ï¼Œè¿™é‡Œæš‚æ—¶è·³è¿‡
        print("å¯¹æ¯”å®éªŒæ¨¡å¼æš‚æœªæ›´æ–°ï¼Œè¯·ä½¿ç”¨åŸæœ‰ç‰ˆæœ¬")
        return


# ==================== ç¨‹åºå…¥å£ ====================
if __name__ == '__main__':
    # è®¾å¤‡è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“å’Œæ ·å¼
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # è¿è¡Œè®­ç»ƒç®¡é“
    try:
        pipeline = TrainingPipeline()
        pipeline.run()
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()