"""
ECGæˆ¿é¢¤æ£€æµ‹è®­ç»ƒä¸»ç¨‹åº - ä¿®å¤ç‰ˆï¼ˆæ·»åŠ æ¨¡å‹ä¿å­˜åŠŸèƒ½ï¼‰
"""

# ==================== è§£å†³å¯¼å…¥é—®é¢˜ ====================
import sys
import os

# è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
current_file = os.path.abspath(__file__)
print(f"å½“å‰æ–‡ä»¶: {current_file}")

# æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è®¡ç®—é¡¹ç›®æ ¹ç›®å½•
# å½“å‰æ–‡ä»¶: EE5046_Projects/src/task1_ecg_analysis/training/TrainProcess.py
# é¡¹ç›®æ ¹ç›®å½•åº”è¯¥æ˜¯: EE5046_Projects (ä¸Šä¸‰çº§ç›®å½•)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_file))))

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.insert(0, project_root)
print(f"é¡¹ç›®æ ¹ç›®å½•å·²æ·»åŠ åˆ°Pythonè·¯å¾„: {project_root}")


# ==================== å¯¼å…¥éƒ¨åˆ† ====================
import argparse
import json
import os
import sys
from datetime import datetime
import numpy as np
import shutil

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics import (accuracy_score, balanced_accuracy_score, roc_auc_score, roc_curve, 
                           precision_score, recall_score, f1_score)
from torch.utils.data import DataLoader,random_split
from tqdm import tqdm

# è‡ªå®šä¹‰æ¨¡å—
from src.common.Config import (AUGMENT_SETTING, BATCH_SIZE, EARLY_STOP_PATIENCE,
                    EXPERIMENT_MODE, FIXED_LENGTH, FOCAL_PRESET_CONFIGS, INPUT_CHANNELS,
                    LEARNING_RATE, LOSS_FUNCTION_CONFIG, MIN_DELTA, NUM_EPOCHS, OUTPUT_CLASSES, USE_FOCAL_LOSS,
                    USE_STREAM2_SETTING, COMPARISON_MODE,
                    STREAM_COMPARISON_CONFIGS, AUGMENTATION_COMPARISON_CONFIGS,
                    DEFAULT_KERNEL_CONFIG,LR_SCHEDULER_CONFIG, get_loss_config)
from src.task1_ecg_analysis.data.DataManager import DataManager
from src.task1_ecg_analysis.data.FoldDataset import FoldDataset
from src.task1_ecg_analysis.visualization.TrainingVisualizer import TrainingVisualizer
from src.task1_ecg_analysis.data.BalancedFoldDataset import BalancedFoldDataset
from TrainModel import Mscnn


# è®¾å¤‡è®¾ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ==================== æ–°å¢ï¼šç»¼åˆè¯„åˆ†è®¡ç®—å™¨ ====================
class CompositeScoreCalculator:
    """è®¡ç®—ç»¼åˆè¯„åˆ†ï¼Œè€ƒè™‘å¤šä¸ªæŒ‡æ ‡"""
    
    # é»˜è®¤æƒé‡é…ç½®
    DEFAULT_WEIGHTS = {
        'accuracy': 0.35,
        'auc': 0.30,
        'f1': 0.25,
        'stability': 0.10  # ç¨³å®šæ€§åˆ†æ•°ï¼ˆåŸºäºæ–¹å·®ï¼‰
    }
    
    @staticmethod
    def calculate_composite_score(metrics, weights=None, fold_results=None):
        """
        è®¡ç®—ç»¼åˆè¯„åˆ†
        Args:
            metrics: åŒ…å«å•ä¸ªè¯„ä¼°æŒ‡æ ‡çš„å­—å…¸ï¼Œå¦‚ {'accuracy': 0.85, 'auc': 0.90, 'f1': 0.82}
            weights: å„æŒ‡æ ‡çš„æƒé‡ï¼Œé»˜è®¤ä¸ºDEFAULT_WEIGHTS
            fold_results: äº¤å‰éªŒè¯çš„è¯¦ç»†ç»“æœï¼ˆç”¨äºè®¡ç®—ç¨³å®šæ€§ï¼‰
        Returns:
            composite_score: ç»¼åˆè¯„åˆ†
            breakdown: å„æŒ‡æ ‡è´¡çŒ®æ˜ç»†
        """
        if weights is None:
            weights = CompositeScoreCalculator.DEFAULT_WEIGHTS
        
        # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„æŒ‡æ ‡éƒ½å­˜åœ¨
        required_metrics = ['accuracy', 'auc', 'f1']
        for metric in required_metrics:
            if metric not in metrics:
                raise ValueError(f"ç¼ºå°‘å¿…è¦æŒ‡æ ‡: {metric}")
        
        # è®¡ç®—ç¨³å®šæ€§åˆ†æ•°ï¼ˆå¦‚æœæœ‰fold_resultsï¼‰
        stability_score = 1.0  # é»˜è®¤å€¼
        if fold_results is not None and len(fold_results) > 1:
            # æå–å„æŠ˜çš„å‡†ç¡®ç‡
            accuracies = [fold['best_val_acc'] for fold in fold_results]
            # ç¨³å®šæ€§åˆ†æ•° = 1 - å˜å¼‚ç³»æ•°ï¼ˆå½’ä¸€åŒ–æ–¹å·®ï¼‰
            cv = np.std(accuracies) / (np.mean(accuracies) + 1e-8)  # å˜å¼‚ç³»æ•°
            stability_score = max(0, 1 - cv)  # ç¡®ä¿åœ¨0-1ä¹‹é—´
        
        # è®¡ç®—åŠ æƒç»¼åˆè¯„åˆ†
        composite_score = 0
        breakdown = {}
        
        for metric, weight in weights.items():
            if metric == 'stability':
                score = stability_score
            elif metric in metrics:
                score = metrics[metric]
            else:
                score = 0.5  # é»˜è®¤å€¼
            
            contribution = score * weight
            composite_score += contribution
            breakdown[metric] = {
                'score': score,
                'weight': weight,
                'contribution': contribution
            }
        
        return composite_score, breakdown
    
    @staticmethod
    def normalize_metrics(metrics, ideal_values=None):
        """å½’ä¸€åŒ–æŒ‡æ ‡åˆ°0-1èŒƒå›´"""
        if ideal_values is None:
            ideal_values = {
                'accuracy': 1.0,
                'auc': 1.0,
                'f1': 1.0,
                'precision': 1.0,
                'recall': 1.0
            }
        
        normalized = {}
        for metric, value in metrics.items():
            if metric in ideal_values:
                # ç®€å•çº¿æ€§å½’ä¸€åŒ–
                normalized[metric] = min(value / ideal_values[metric], 1.0)
            else:
                normalized[metric] = value
        
        return normalized

# ==================== FocalLossç±» ====================
class FocalLoss(nn.Module):
    """
    Focal Loss for dense object detection.
    Paper: Focal Loss for Dense Object Detection
    https://arxiv.org/abs/1708.02002
    
    Args:
        alpha (float, optional): Weighting factor for the rare class (0 < alpha < 1).
        gamma (float, optional): Focusing parameter (gamma >= 0). Higher gamma reduces 
                                the loss contribution from easy examples.
        reduction (str, optional): Specifies the reduction to apply to the output:
                                   'none' | 'mean' | 'sum'
        logits (bool, optional): If True, expects raw logits as input,
                                 otherwise expects probabilities (0-1).
    """
    
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', logits=True):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.logits = logits
        
    def forward(self, inputs, targets):
        if self.logits:
            # If using logits, apply sigmoid first
            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        else:
            # If using probabilities, use regular BCE
            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')
        
        # Get probabilities from logits if needed
        if self.logits:
            pt = torch.sigmoid(inputs)
        else:
            pt = inputs
        
        # Ensure pt is within [0, 1]
        pt = torch.clamp(pt, 1e-8, 1 - 1e-8)
        
        # Calculate p_t
        p_t = pt * targets + (1 - pt) * (1 - targets)
        
        # Calculate alpha_t
        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        
        # Calculate modulating factor
        modulating_factor = (1 - p_t) ** self.gamma
        
        # Focal loss
        focal_loss = alpha_t * modulating_factor * BCE_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class WeightedFocalLoss(nn.Module):
    """
    Weighted Focal Loss with dynamic alpha calculation based on class distribution.
    """
    
    def __init__(self, pos_weight=None, gamma=2.0, reduction='mean', logits=True):
        super(WeightedFocalLoss, self).__init__()
        self.pos_weight = pos_weight
        self.gamma = gamma
        self.reduction = reduction
        self.logits = logits
        
    def forward(self, inputs, targets):
        if self.logits:
            BCE_loss = F.binary_cross_entropy_with_logits(
                inputs, targets, reduction='none'
            )
        else:
            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')
        
        # Get probabilities from logits if needed
        if self.logits:
            pt = torch.sigmoid(inputs)
        else:
            pt = inputs
        
        # Ensure pt is within [0, 1]
        pt = torch.clamp(pt, 1e-8, 1 - 1e-8)
        
        # Calculate p_t
        p_t = pt * targets + (1 - pt) * (1 - targets)
        
        # Calculate alpha_t based on class distribution
        if self.pos_weight is not None:
            # Use provided pos_weight to calculate alpha
            alpha_t = self.pos_weight * targets + (1 - targets)
            # Normalize so that alpha_t sums to 2 (like in original focal loss)
            alpha_t = alpha_t / (alpha_t.mean() + 1e-8) * 1.0
        else:
            # Default: equal weighting
            alpha_t = torch.ones_like(targets)
        
        # Calculate modulating factor
        modulating_factor = (1 - p_t) ** self.gamma
        
        # Focal loss
        focal_loss = alpha_t * modulating_factor * BCE_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss
# ==================== æ¨¡å‹æ–‡ä»¶ç®¡ç†å™¨ ====================
class ModelFileManager:
    """ç®¡ç†æ¨¡å‹æ–‡ä»¶çš„ä¿å­˜å’ŒåŠ è½½"""
    
    @staticmethod
    def create_experiment_dir(base_dir, experiment_type, config_name=""):
        """åˆ›å»ºå®éªŒç›®å½•ç»“æ„"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if config_name:
            dir_name = f"{experiment_type}_{config_name}_{timestamp}"
        else:
            dir_name = f"{experiment_type}_{timestamp}"
        
        exp_dir = os.path.join(base_dir, dir_name)
        subdirs = [
            "models",           # ä¿å­˜æ¨¡å‹æ–‡ä»¶
            "logs",            # è®­ç»ƒæ—¥å¿—
            "configs",         # é…ç½®æ–‡ä»¶
            "metrics",         # æ€§èƒ½æŒ‡æ ‡
            "visualizations"   # å¯è§†åŒ–å›¾è¡¨
        ]
        
        for subdir in subdirs:
            os.makedirs(os.path.join(exp_dir, subdir), exist_ok=True)
        
        print(f"ğŸ“ åˆ›å»ºå®éªŒç›®å½•: {exp_dir}")
        return exp_dir
    
    @staticmethod
    def save_model(model, save_path, metadata=None):
        """ä¿å­˜æ¨¡å‹å’Œå…ƒæ•°æ®"""
        model_state = {
            'model_state_dict': model.state_dict(),
            'model_config': getattr(model, 'config', {}),
            'save_time': datetime.now().isoformat()
        }
        
        if metadata:
            model_state.update(metadata)
        
        torch.save(model_state, save_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")
    
    @staticmethod
    def generate_model_name(config, fold=None, epoch=None, metric=None, composite_score=None):
        """ç”Ÿæˆæ¨¡å‹æ–‡ä»¶å"""
        parts = []
        
        # åŸºç¡€ä¿¡æ¯
        kernel_config = config.get('kernel_config', {})
        if kernel_config.get('name'):
            parts.append(kernel_config['name'])
        else:
            parts.append(f"K{kernel_config.get('stream1_kernel', '?')}")
            parts.append(f"S2{kernel_config.get('stream2_first_kernel', '?')}")
        
        # è®­ç»ƒé…ç½®
        parts.append(f"BS{config.get('batch_size', BATCH_SIZE)}")
        parts.append(f"LR{config.get('lr', LEARNING_RATE)}")
        
        # è®­ç»ƒçŠ¶æ€
        if fold is not None:
            parts.append(f"F{fold}")
        if epoch is not None:
            parts.append(f"E{epoch}")
        
        # æ€§èƒ½æŒ‡æ ‡
        if composite_score is not None:
            parts.append(f"CS{composite_score:.4f}".replace('.', 'p'))
        elif metric is not None:
            parts.append(f"A{metric:.4f}".replace('.', 'p'))
        
        # æ—¶é—´æˆ³
        parts.append(datetime.now().strftime("%m%d%H%M"))
        
        return "_".join(parts) + ".pth"
    
    @staticmethod
    def save_metrics(metrics, save_path):
        """ä¿å­˜æ€§èƒ½æŒ‡æ ‡"""
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š æŒ‡æ ‡å·²ä¿å­˜: {save_path}")
    
    @staticmethod
    def save_config(config, save_path):
        """ä¿å­˜é…ç½®"""
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"âš™ï¸ é…ç½®å·²ä¿å­˜: {save_path}")


# ==================== æ”¹è¿›çš„æ¨¡å‹è®­ç»ƒå™¨æ¨¡å— ====================
class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨ï¼Œè´Ÿè´£å•æ¬¡è®­ç»ƒéªŒè¯è¿‡ç¨‹"""
    
    def __init__(self, base_path, kernel_config, batch_size, lr, 
                 use_stream2, augment, experiment_dir, config_name=None,
                 composite_weights=None,lr_scheduler_config=None,
                 use_focal_loss=USE_FOCAL_LOSS,focal_alpha=0.25,focal_gamma=2.0):
        self.base_path = base_path
        self.kernel_config = kernel_config
        self.batch_size = batch_size
        self.lr = lr
        self.use_stream2 = use_stream2
        self.augment = augment
        self.experiment_dir = experiment_dir
        self.config_name = config_name
        self.file_manager = ModelFileManager()
        self.visualizer = TrainingVisualizer()

        # Focal Lossç›¸å…³å‚æ•°
        self.use_focal_loss = use_focal_loss
        self.focal_alpha = focal_alpha
        self.focal_gamma = focal_gamma

        # æ‰“å°é…ç½®ä¿¡æ¯
        if self.use_focal_loss:
            print(f"ğŸ“ˆ ä½¿ç”¨Focal Loss: alpha={focal_alpha}, gamma={focal_gamma}")
        else:
            print(f"ğŸ“ˆ ä½¿ç”¨BCEWithLogitsLoss")
        
        # ç»¼åˆè¯„åˆ†æƒé‡
        self.composite_weights = composite_weights or CompositeScoreCalculator.DEFAULT_WEIGHTS

        # å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
        self.lr_scheduler_config = lr_scheduler_config or LR_SCHEDULER_CONFIG  # ä½¿ç”¨é…ç½®æˆ–é»˜è®¤é…ç½®
        print(f"ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®: {self.lr_scheduler_config['scheduler_type']}")
        
        print(f"åˆå§‹åŒ–DataManagerï¼Œæ•°æ®é›†è·¯å¾„: {self.base_path}")
        if not os.path.exists(self.base_path):
            print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {self.base_path}")
            print("è¯·æ£€æŸ¥Datasetç›®å½•æ˜¯å¦æ­£ç¡®æ”¾ç½®")
            sys.exit(1)
        
        self.data_manager = DataManager(base_path)
        # æ•°æ®ç®¡ç†å™¨
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_acc = 0
        self.best_val_auc = 0
        self.best_val_f1 = 0
        self.best_composite_score = 0
        self.best_model_state = None
        self.best_epoch = 0
        self.early_stop_counter = 0

    def _find_precision_optimized_threshold(self, labels, probs, return_all=False):
        """å¯»æ‰¾æ›´æ³¨é‡ç²¾ç¡®ç‡çš„é˜ˆå€¼"""
        # é¦–å…ˆç¡®ä¿æœ‰è¶³å¤Ÿçš„æ¦‚ç‡å˜åŒ–
        if np.max(probs) - np.min(probs) < 0.1:
            print("  è­¦å‘Š: æ¦‚ç‡å˜åŒ–å¾ˆå°ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼")
            if return_all:
                return 0.5, 0, 0, 0
            else:
                return 0.5
        
        # æœç´¢èŒƒå›´æ ¹æ®æ¦‚ç‡åˆ†å¸ƒè°ƒæ•´
        prob_min = max(0.1, np.min(probs))
        prob_max = min(0.9, np.max(probs))
        
        # å¦‚æœæ¦‚ç‡èŒƒå›´å¤ªå°ï¼Œæ‰©å¤§æœç´¢èŒƒå›´
        if prob_max - prob_min < 0.3:
            prob_min = max(0.1, prob_min - 0.1)
            prob_max = min(0.9, prob_max + 0.1)
        
        thresholds = np.linspace(prob_min, prob_max, 51)
        
        best_score = 0
        best_threshold = 0.5
        best_precision = 0
        best_recall = 0
        
        for th in thresholds:
            preds = (probs >= th).astype(int)
            pos_predictions = np.sum(preds)
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•é¢„æµ‹ï¼Œè·³è¿‡è¿™ä¸ªé˜ˆå€¼
            if pos_predictions == 0:
                continue
                
            precision = precision_score(labels, preds, zero_division=0)
            recall = recall_score(labels, preds, zero_division=0)
            f1 = f1_score(labels, preds, zero_division=0)
            
            # æ›´æ³¨é‡ç²¾ç¡®ç‡çš„è¯„åˆ†
            # ä½†ä¹Ÿè¦ç¡®ä¿æœ‰ä¸€å®šæ•°é‡çš„é¢„æµ‹
            weighted_score = 0.5 * precision + 0.3 * f1 + 0.2 * min(recall, 0.5)
            
            if weighted_score > best_score:
                best_score = weighted_score
                best_threshold = th
                best_precision = precision
                best_recall = recall
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•é˜ˆå€¼èƒ½é¢„æµ‹å‡ºæ­£æ ·æœ¬ï¼Œä½¿ç”¨æ›´ä½çš„é˜ˆå€¼
        if best_score == 0:
            print("  è­¦å‘Š: æ²¡æœ‰é˜ˆå€¼èƒ½é¢„æµ‹å‡ºæ­£æ ·æœ¬ï¼Œä½¿ç”¨æ›´ä½çš„é˜ˆå€¼")
            best_threshold = prob_min  # ä½¿ç”¨æœ€ä½çš„é˜ˆå€¼
            preds = (probs >= best_threshold).astype(int)
            best_precision = precision_score(labels, preds, zero_division=0)
            best_recall = recall_score(labels, preds, zero_division=0)
            best_f1 = f1_score(labels, preds, zero_division=0)
        
        if return_all:
            return best_threshold, best_f1, best_precision, best_recall
        else:
            return best_threshold

    def _find_best_two_stage_combo(self, labels, probs, max_combinations=20):
        """å¯»æ‰¾æœ€ä½³çš„ä¸¤é˜¶æ®µé˜ˆå€¼ç»„åˆ"""
        print("  æœç´¢æœ€ä½³ä¸¤é˜¶æ®µé˜ˆå€¼ç»„åˆ...")
        
        # é¦–å…ˆæ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒ
        print(f"  æ¦‚ç‡èŒƒå›´: [{np.min(probs):.4f}, {np.max(probs):.4f}]")
        print(f"  æ¦‚ç‡å¹³å‡å€¼: {np.mean(probs):.4f}")
        
        # ç¡®ä¿æœ‰æ­£æ ·æœ¬çš„æ¦‚ç‡
        if np.max(probs) < 0.3:  # å¦‚æœæœ€å¤§æ¦‚ç‡éƒ½å¾ˆä½
            print("  è­¦å‘Š: æ‰€æœ‰é¢„æµ‹æ¦‚ç‡éƒ½å¾ˆä½ï¼Œå¯èƒ½æ¨¡å‹æœ‰é—®é¢˜")
            return {
                'stage1_threshold': 0.1,  # ä½¿ç”¨å¾ˆä½çš„é˜ˆå€¼
                'stage2_threshold': 0.3,
                'precision': 0,
                'recall': 0,
                'f1': 0
            }
        
        # å®šä¹‰æœç´¢èŒƒå›´ï¼ˆæ ¹æ®å®é™…æ¦‚ç‡åˆ†å¸ƒè°ƒæ•´ï¼‰
        prob_min = max(0.1, np.percentile(probs, 5))  # ç¬¬5ç™¾åˆ†ä½æ•°ä½œä¸ºä¸‹é™
        prob_max = min(0.9, np.percentile(probs, 95))  # ç¬¬95ç™¾åˆ†ä½æ•°ä½œä¸ºä¸Šé™
        
        # ç¡®ä¿æœç´¢èŒƒå›´åˆç†
        if prob_max - prob_min < 0.2:
            prob_min = max(0.1, prob_min - 0.1)
            prob_max = min(0.9, prob_max + 0.1)
        
        stage1_options = np.linspace(prob_min, min(prob_max, 0.6), 6)
        stage2_options = np.linspace(max(prob_min, 0.4), prob_max, 8)
        
        best_f1 = 0
        best_combo = {
            'stage1_threshold': 0.3,
            'stage2_threshold': 0.5,
            'precision': 0,
            'recall': 0,
            'f1': 0
        }
        
        tested_combos = 0
        
        for s1 in stage1_options:
            for s2 in stage2_options:
                if s2 <= s1:  # ç¡®ä¿ç¬¬äºŒé˜¶æ®µé˜ˆå€¼é«˜äºç¬¬ä¸€é˜¶æ®µ
                    continue
                
                # ä¸¤é˜¶æ®µé¢„æµ‹
                stage1_preds = (probs >= s1).astype(int)
                pos_indices = np.where(stage1_preds == 1)[0]
                final_preds = stage1_preds.copy()
                
                if len(pos_indices) > 0:
                    pos_probs = probs[pos_indices]
                    stage2_preds = (pos_probs >= s2).astype(int)
                    final_preds[pos_indices] = stage2_preds
                
                # è®¡ç®—æŒ‡æ ‡
                try:
                    precision = precision_score(labels, final_preds, zero_division=0)
                    recall = recall_score(labels, final_preds, zero_division=0)
                    f1 = f1_score(labels, final_preds, zero_division=0)
                except:
                    precision = recall = f1 = 0
                
                # æ£€æŸ¥æ˜¯å¦é¢„æµ‹äº†æ­£æ ·æœ¬
                pos_predictions = np.sum(final_preds)
                if pos_predictions == 0:
                    # å¦‚æœæ²¡æœ‰é¢„æµ‹æ­£æ ·æœ¬ï¼Œè·³è¿‡è¿™ä¸ªç»„åˆ
                    continue
                
                # ç»¼åˆè¯„åˆ†ï¼šå¹³è¡¡ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1
                composite_score = 0.4 * precision + 0.4 * f1 + 0.2 * recall
                
                if composite_score > best_f1:
                    best_f1 = composite_score
                    best_combo = {
                        'stage1_threshold': s1,
                        'stage2_threshold': s2,
                        'precision': precision,
                        'recall': recall,
                        'f1': f1,
                        'pos_predictions': int(pos_predictions)
                    }
                
                tested_combos += 1
                if tested_combos >= max_combinations:
                    break
            
            if tested_combos >= max_combinations:
                break
        
        print(f"  æµ‹è¯•äº† {tested_combos} ç§é˜ˆå€¼ç»„åˆ")
        print(f"  æœ€ä½³ç»„åˆ: stage1={best_combo['stage1_threshold']:.2f}, stage2={best_combo['stage2_threshold']:.2f}")
        print(f"  é¢„æµ‹æ­£æ ·æœ¬æ•°: {best_combo.get('pos_predictions', 0)}")
        print(f"  å¯¹åº”æŒ‡æ ‡: ç²¾ç¡®ç‡={best_combo['precision']:.4f}, å¬å›ç‡={best_combo['recall']:.4f}, F1={best_combo['f1']:.4f}")
        
        return best_combo

    def _two_stage_evaluate(self, probs, labels, stage1_th=0.4, stage2_th=None):
        """ä¸¤é˜¶æ®µè¯„ä¼° - ä¼˜åŒ–ç‰ˆæœ¬"""
        if stage2_th is None:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šç¬¬äºŒé˜¶æ®µé˜ˆå€¼ï¼Œä½¿ç”¨æ›´æ³¨é‡ç²¾ç¡®ç‡çš„é˜ˆå€¼
            stage2_th = self._find_precision_optimized_threshold(labels, probs)
        
        print(f"ä¸¤é˜¶æ®µé˜ˆå€¼ç­–ç•¥: ç¬¬ä¸€é˜¶æ®µ={stage1_th:.2f}, ç¬¬äºŒé˜¶æ®µ={stage2_th:.2f}")
        
        # ç¬¬ä¸€é˜¶æ®µï¼šä½é˜ˆå€¼è·å–é«˜å¬å›
        stage1_preds = (probs >= stage1_th).astype(int)
        stage1_recall = recall_score(labels, stage1_preds, zero_division=0)
        stage1_precision = precision_score(labels, stage1_preds, zero_division=0)
        print(f"ç¬¬ä¸€é˜¶æ®µ: å¬å›ç‡={stage1_recall:.4f}, ç²¾ç¡®ç‡={stage1_precision:.4f}")
        
        # ç¬¬äºŒé˜¶æ®µï¼šåªå¯¹ç¬¬ä¸€é˜¶æ®µé¢„æµ‹ä¸ºæ­£çš„æ ·æœ¬ä½¿ç”¨é«˜é˜ˆå€¼
        stage1_pos_indices = np.where(stage1_preds == 1)[0]
        if len(stage1_pos_indices) == 0:
            print("âš ï¸ ç¬¬ä¸€é˜¶æ®µæ²¡æœ‰é¢„æµ‹ä¸ºæ­£çš„æ ·æœ¬")
            final_preds = stage1_preds
        else:
            stage1_pos_probs = probs[stage1_pos_indices]
            
            # å¯¹è¿™äº›æ ·æœ¬ä½¿ç”¨ç¬¬äºŒé˜¶æ®µé˜ˆå€¼
            stage2_pos_preds = (stage1_pos_probs >= stage2_th).astype(int)
            
            # åˆå¹¶ç»“æœ
            final_preds = stage1_preds.copy()
            final_preds[stage1_pos_indices] = stage2_pos_preds
            
            print(f"ç¬¬ä¸€é˜¶æ®µæ­£æ ·æœ¬æ•°: {len(stage1_pos_indices)}")
            print(f"ç¬¬äºŒé˜¶æ®µä¿ç•™æ•°: {np.sum(stage2_pos_preds)} (è¿‡æ»¤ç‡: {(1 - np.sum(stage2_pos_preds)/len(stage1_pos_indices))*100:.1f}%)")
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        final_recall = recall_score(labels, final_preds, zero_division=0)
        final_precision = precision_score(labels, final_preds, zero_division=0)
        final_f1 = f1_score(labels, final_preds, zero_division=0)
        final_acc = accuracy_score(labels, final_preds)
        
        print(f"ç¬¬äºŒé˜¶æ®µç»“æœ:")
        print(f"  å¬å›ç‡: {final_recall:.4f} (ç›¸æ¯”ç¬¬ä¸€é˜¶æ®µ: {final_recall-stage1_recall:+.4f})")
        print(f"  ç²¾ç¡®ç‡: {final_precision:.4f} (ç›¸æ¯”ç¬¬ä¸€é˜¶æ®µ: {final_precision-stage1_precision:+.4f})")
        print(f"  F1åˆ†æ•°: {final_f1:.4f}")
        print(f"  å‡†ç¡®ç‡: {final_acc:.4f}")
        
        return {
            'predictions': final_preds,
            'acc': final_acc,
            'recall': final_recall,
            'precision': final_precision,
            'f1': final_f1,
            'stage1_threshold': stage1_th,
            'stage2_threshold': stage2_th,
            'stage1_recall': stage1_recall,
            'stage1_precision': stage1_precision
        }
    
    def _create_criterion(self, pos_weight, device):
        """åˆ›å»ºæŸå¤±å‡½æ•°ï¼ˆæ”¯æŒBCEå’ŒFocal Lossï¼‰"""
        if self.use_focal_loss:
            # ä½¿ç”¨Focal Loss
            if self.focal_alpha is not None:
                # ä½¿ç”¨å›ºå®šçš„alpha
                criterion = FocalLoss(
                    alpha=self.focal_alpha,
                    gamma=self.focal_gamma,
                    reduction='mean',
                    logits=True
                ).to(device)
            else:
                # ä½¿ç”¨åŠ æƒFocal Loss
                criterion = WeightedFocalLoss(
                    pos_weight=pos_weight,
                    gamma=self.focal_gamma,
                    reduction='mean',
                    logits=True
                ).to(device)
            print(f"âœ… åˆ›å»ºFocal Loss: alpha={self.focal_alpha if self.focal_alpha else 'dynamic'}, "
                f"gamma={self.focal_gamma}")
        else:
            # ä½¿ç”¨BCEWithLogitsLoss
            criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)
            print(f"âœ… åˆ›å»ºBCEWithLogitsLoss: pos_weight={pos_weight.item():.2f}")
        
        return criterion

    def _create_lr_scheduler(self, optimizer, num_epochs, train_loader=None):
        """æ ¹æ®é…ç½®åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if not self.lr_scheduler_config.get('use_scheduler', True):
            print("âš ï¸ æœªå¯ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨")
            return None
        
        scheduler_type = self.lr_scheduler_config.get('scheduler_type', 'plateau')
        
        if scheduler_type == 'plateau':
            config = self.lr_scheduler_config.get('plateau_config', {})
            # ç§»é™¤ verbose å‚æ•°
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode=config.get('mode', 'max'),
                factor=config.get('factor', 0.5),
                patience=config.get('patience', 5),
                min_lr=config.get('min_lr', 1e-6),
                # verbose=config.get('verbose', True)  # æ³¨é‡Šæ‰æˆ–ç§»é™¤è¿™è¡Œ
            )
            print(f"âœ… åˆ›å»º ReduceLROnPlateau è°ƒåº¦å™¨ï¼Œè€å¿ƒå€¼: {config.get('patience', 5)}")
        
        elif scheduler_type == 'cosine':
            config = self.lr_scheduler_config.get('cosine_config', {})
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=config.get('T_max', num_epochs),
                eta_min=config.get('eta_min', 1e-6)
            )
            print(f"âœ… åˆ›å»º CosineAnnealingLR è°ƒåº¦å™¨ï¼ŒT_max: {config.get('T_max', num_epochs)}")
        
        elif scheduler_type == 'onecycle':
            config = self.lr_scheduler_config.get('onecycle_config', {})
            if train_loader is None:
                print("âš ï¸ OneCycleLR éœ€è¦ train_loaderï¼Œä½¿ç”¨é»˜è®¤è°ƒåº¦å™¨")
                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)
            else:
                scheduler = torch.optim.lr_scheduler.OneCycleLR(
                    optimizer,
                    max_lr=config.get('max_lr', self.lr),
                    steps_per_epoch=len(train_loader),
                    epochs=num_epochs,
                    pct_start=config.get('pct_start', 0.3),
                    div_factor=config.get('div_factor', 25.0),
                    final_div_factor=config.get('final_div_factor', 1e4)
                )
                print(f"âœ… åˆ›å»º OneCycleLR è°ƒåº¦å™¨ï¼Œmax_lr: {config.get('max_lr', self.lr)}")
        
        elif scheduler_type == 'step':
            config = self.lr_scheduler_config.get('step_config', {})
            scheduler = torch.optim.lr_scheduler.StepLR(
                optimizer,
                step_size=config.get('step_size', 10),
                gamma=config.get('gamma', 0.5)
            )
            print(f"âœ… åˆ›å»º StepLR è°ƒåº¦å™¨ï¼Œstep_size: {config.get('step_size', 10)}, gamma: {config.get('gamma', 0.5)}")
        
        else:
            print(f"âš ï¸ æœªçŸ¥çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹: {scheduler_type}ï¼Œä½¿ç”¨é»˜è®¤ ReduceLROnPlateau")
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='max',
                factor=0.5,
                patience=5,
                min_lr=1e-6
                # verbose=True  # åŒæ ·ç§»é™¤è¿™é‡Œçš„ verbose
            )
        
        return scheduler
    
    def cross_validate_on_train_set(self, train_cv_indices, num_epochs, k_folds=5, save_models=True):
        """
        åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯ï¼ˆç®€åŒ–è¾“å‡ºç‰ˆæœ¬ï¼‰
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¯ å¼€å§‹ {k_folds} æŠ˜äº¤å‰éªŒè¯")
        print(f"è®­ç»ƒé›†: CV{', '.join(map(str, train_cv_indices))}")
        print(f"æ¨¡å‹é…ç½®: {self.kernel_config.get('name', 'Unknown')}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}, å­¦ä¹ ç‡: {self.lr}")
        print(f"æ•°æ®å¢å¼º: {'æ˜¯' if self.augment else 'å¦'}")
        print(f"{'='*60}")
        
        # åˆ›å»ºKæŠ˜åˆ’åˆ†
        kfold_splits = self.data_manager.create_kfold_splits(train_cv_indices, k_folds)
        if not kfold_splits:
            print("âŒ é”™è¯¯: æ— æ³•åˆ›å»ºKæŠ˜åˆ’åˆ†")
            return {}, []
        
        fold_results = []
        fold_models = []

        # åˆ›å»ºç®€æ´çš„è¿›åº¦æ¡
        fold_pbar = tqdm(
            range(k_folds), 
            desc="äº¤å‰éªŒè¯è¿›åº¦",
            bar_format='{desc}: {percentage:3.0f}%|{bar:20}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'
        )
        
        # è®­ç»ƒæ¯ä¸€æŠ˜
        for fold_idx, (train_data, val_data) in enumerate(kfold_splits):
            fold_pbar.set_description(f"è®­ç»ƒæŠ˜ {fold_idx+1}/{k_folds}")
            
            # è®­ç»ƒå½“å‰æŠ˜
            fold_result, fold_model = self._train_single_fold(
                train_data, val_data, fold_idx, num_epochs, save_models
            )
            
            fold_results.append(fold_result)
            fold_models.append(fold_model)
            
            # æ›´æ–°è¿›åº¦æ¡å¹¶æ˜¾ç¤ºå½“å‰æŠ˜çš„ç»“æœ
            fold_pbar.set_postfix({
                'acc': f"{fold_result['best_val_acc']:.3f}",
                'f1': f"{fold_result['best_val_f1']:.3f}"
            })
            
            # æ˜¾ç¤ºå½“å‰æŠ˜çš„ç®€å•ç»“æœ
            print(f"  âœ… æŠ˜ {fold_idx+1} å®Œæˆ: éªŒè¯å‡†ç¡®ç‡={fold_result['best_val_acc']:.4f}, "
                f"F1={fold_result['best_val_f1']:.4f} (æœ€ä½³ epoch {fold_result['best_epoch']})")

            fold_pbar.update(1)
        
        fold_pbar.close()
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = self._compute_average_metrics(fold_results)
        
        # è®¡ç®—å¹³å‡ç»¼åˆè¯„åˆ†
        all_metrics = [{
            'accuracy': r['best_val_acc'],
            'auc': r['best_val_auc'],
            'f1': r['best_val_f1']
        } for r in fold_results]
        
        avg_composite_score = np.mean([
            CompositeScoreCalculator.calculate_composite_score(m)[0] for m in all_metrics
        ])
        avg_metrics['avg_composite_score'] = float(avg_composite_score)
        
        # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
        if self.experiment_dir:
            cv_results = {
                'avg_metrics': avg_metrics,
                'fold_results': fold_results,
                'config': self._get_config_dict(),
                'timestamp': datetime.now().isoformat(),
                'composite_score_weights': self.composite_weights
            }
            
            results_path = os.path.join(self.experiment_dir, "metrics", "cross_validation_results.json")
            self.file_manager.save_metrics(cv_results, results_path)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºç»¼åˆè¯„åˆ†æœ€é«˜çš„æŠ˜ï¼‰
            if fold_models and save_models:
                # è®¡ç®—æ¯æŠ˜çš„ç»¼åˆè¯„åˆ†
                composite_scores = []
                for r in fold_results:
                    metrics = {
                        'accuracy': r['best_val_acc'],
                        'auc': r['best_val_auc'],
                        'f1': r['best_val_f1']
                    }
                    score, _ = CompositeScoreCalculator.calculate_composite_score(metrics)
                    composite_scores.append(score)
                
                # é€‰æ‹©ç»¼åˆè¯„åˆ†æœ€é«˜çš„æŠ˜
                best_fold_idx = np.argmax(composite_scores)
                best_model = fold_models[best_fold_idx]
                best_fold_result = fold_results[best_fold_idx]
                best_composite_score = composite_scores[best_fold_idx]
                
                model_name = self.file_manager.generate_model_name(
                    self._get_config_dict(),
                    fold=best_fold_idx,
                    epoch=best_fold_result['best_epoch'],
                    composite_score=best_composite_score
                )
                
                model_path = os.path.join(self.experiment_dir, "models", model_name)
                metadata = {
                    'fold': best_fold_idx,
                    'val_acc': best_fold_result['best_val_acc'],
                    'val_auc': best_fold_result['best_val_auc'],
                    'val_f1': best_fold_result['best_val_f1'],
                    'composite_score': best_composite_score,
                    'epoch': best_fold_result['best_epoch'],
                    'early_stopped': best_fold_result.get('early_stopped', False)
                }
                self.file_manager.save_model(best_model, model_path, metadata)
                
                print(f"  ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_name}")
        
        # æ˜¾ç¤ºäº¤å‰éªŒè¯ç»“æœæ€»ç»“
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {k_folds}æŠ˜äº¤å‰éªŒè¯ç»“æœæ€»ç»“")
        print(f"{'='*60}")
        
        # æ˜¾ç¤ºæ¯æŠ˜è¯¦ç»†ç»“æœ
        print(f"{'æŠ˜':<4} {'éªŒè¯å‡†ç¡®ç‡':<12} {'AUC':<12} {'F1åˆ†æ•°':<12} {'ç»¼åˆè¯„åˆ†':<12} {'æœ€ä½³epoch':<10}")
        print(f"{'-'*70}")
        
        for i, result in enumerate(fold_results):
            metrics = {
                'accuracy': result['best_val_acc'],
                'auc': result['best_val_auc'],
                'f1': result['best_val_f1']
            }
            composite_score, _ = CompositeScoreCalculator.calculate_composite_score(metrics)
            
            print(f" {i+1:<3} {result['best_val_acc']:<12.4f} {result['best_val_auc']:<12.4f} "
                f"{result['best_val_f1']:<12.4f} {composite_score:<12.4f} {result['best_epoch']:<10}")
        
        print(f"{'-'*70}")
        print(f" å¹³å‡: {avg_metrics['avg_val_acc']:<12.4f} {avg_metrics['avg_val_auc']:<12.4f} "
            f"{avg_metrics['avg_val_f1']:<12.4f} {avg_metrics['avg_composite_score']:<12.4f}")
        print(f" æ ‡å‡†å·®: {avg_metrics['std_val_acc']:<12.4f} {avg_metrics['std_val_auc']:<12.4f} "
            f"{avg_metrics['std_val_f1']:<12.4f}")
        print(f"{'='*60}")
        
        # å¯è§†åŒ–äº¤å‰éªŒè¯ç»“æœ
        if self.experiment_dir:
            self._visualize_cv_results(fold_results, avg_metrics)
        
        return avg_metrics, fold_results
    
    def train_final_model(self, train_cv_indices, num_epochs, save_model=True, val_ratio=0.2):
        """
        ä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ŒåŒ…å«éªŒè¯é›†
        Args:
            train_cv_indices: è®­ç»ƒé›†CVç´¢å¼•
            num_epochs: è®­ç»ƒè½®æ•°
            save_model: æ˜¯å¦ä¿å­˜æ¨¡å‹
            val_ratio: ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›†çš„æ¯”ä¾‹
        """
        print(f"\nä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
        print(f"è®­ç»ƒé›†: CV{', '.join(map(str, train_cv_indices))}")
        print(f"éªŒè¯é›†æ¯”ä¾‹: {val_ratio:.1%}")
        
        # åŠ è½½è®­ç»ƒé›†æ•°æ®
        train_data = self.data_manager.load_cv_files(train_cv_indices)
        if len(train_data) == 0:
            print("é”™è¯¯: è®­ç»ƒé›†æ•°æ®ä¸ºç©º")
            return None, {}
        
        # æ–°å¢ï¼šæ£€æŸ¥æ•°æ®åˆ†å¸ƒ
        print("\n=== æ•°æ®åˆ†å¸ƒè¯Šæ–­ ===")
        
        # æ£€æŸ¥æ•´ä¸ªè®­ç»ƒé›†çš„ç±»åˆ«åˆ†å¸ƒ
        all_labels = []
        for _, label in train_data:
            all_labels.append(label)
        
        all_labels_np = np.array(all_labels)
        print(f"æ•´ä¸ªè®­ç»ƒé›† (CV0~CV3) ç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(all_labels_np)}")
        print(f"  æ­£æ ·æœ¬æ•°: {np.sum(all_labels_np)}")
        print(f"  è´Ÿæ ·æœ¬æ•°: {len(all_labels_np) - np.sum(all_labels_np)}")
        print(f"  æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(all_labels_np):.2%}")
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        total_size = len(train_data)
        val_size = int(total_size * val_ratio)
        train_size = total_size - val_size
        
        # éšæœºåˆ’åˆ†
        torch.manual_seed(42)  # ç¡®ä¿å¯é‡å¤æ€§
        train_subset, val_subset = random_split(train_data, [train_size, val_size])

        # æ–°å¢ï¼šæ£€æŸ¥åˆ’åˆ†åçš„åˆ†å¸ƒ
        train_labels = []
        for idx in train_subset.indices:
            _, label = train_data[idx]
            train_labels.append(label)
        
        val_labels = []
        for idx in val_subset.indices:
            _, label = train_data[idx]
            val_labels.append(label)
        
        print(f"\nåˆ’åˆ†åç»Ÿè®¡:")
        print(f"  è®­ç»ƒé›†å¤§å°: {train_size}, éªŒè¯é›†å¤§å°: {val_size}")
        print(f"  è®­ç»ƒé›†æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(train_labels):.2%}")
        print(f"  éªŒè¯é›†æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(val_labels):.2%}")
        print(f"  è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ: æ­£={np.sum(train_labels)}, è´Ÿ={len(train_labels)-np.sum(train_labels)}")
        print(f"  éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ: æ­£={np.sum(val_labels)}, è´Ÿ={len(val_labels)-np.sum(val_labels)}")
            
        # åˆ›å»ºå¹³è¡¡æ•°æ®é›†
        train_dataset = BalancedFoldDataset(
            list(train_subset),
            base_path=self.base_path,
            is_train=True,
            augment=True,
            target_ratio=0.5,  # 1:3.3 çš„æ¯”ä¾‹ï¼Œæ¯”åŸå§‹1:10æ›´å¹³è¡¡
            augmentation_config={
                'positive_augment_factor': 15,  # å¤§å¹…å¢åŠ æ­£æ ·æœ¬å¢å¼ºå€æ•°ï¼ˆåŸæ¥æ˜¯3ï¼‰
                'noise_std': 0.02,              # å¢åŠ å™ªå£°å¼ºåº¦
                'scale_range': (0.8, 1.2),      # æ‰©å¤§ç¼©æ”¾èŒƒå›´
                'shift_range': (-25, 25),       # æ‰©å¤§å¹³ç§»èŒƒå›´
                'use_mixup': True,
                'mixup_alpha': 0.3,             # å¢åŠ mixupå¼ºåº¦
                'use_time_warp': True,          # æ–°å¢æ—¶é—´æ‰­æ›²
                'time_warp_factor': 0.4,
                'use_random_cutout': True,      # æ–°å¢éšæœºé®æŒ¡
                'cutout_size': 60,
                'cutout_probability': 0.4,
                'use_frequency_mask': True,     # æ–°å¢é¢‘åŸŸæ©ç 
                'freq_mask_ratio': 0.2,
                # ECGç‰¹æœ‰å¢å¼º
                'use_baseline_wander': True,
                'bw_amplitude': 0.03,
                'use_powerline_noise': True,
                'pl_amplitude': 0.015
            }
        )
        val_dataset = FoldDataset(
            list(val_subset),
            base_path=self.base_path,
            is_train=False,
            augment=False
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset, batch_size=self.batch_size, 
            shuffle=True, num_workers=0
        )
        val_loader = DataLoader(
            val_dataset, batch_size=self.batch_size, 
            shuffle=False, num_workers=0
        )

        # è®¡ç®—æ­£ç±»æƒé‡ï¼ˆå¤„ç†ä¸å¹³è¡¡ï¼‰
        pos_weight = self._calculate_pos_weight(train_loader)
        print(f"æ­£ç±»æƒé‡: {pos_weight.item():.2f}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = Mscnn(
            INPUT_CHANNELS,
            OUTPUT_CLASSES,
            use_stream2=self.use_stream2,
            stream1_kernel=self.kernel_config['stream1_kernel'],
            stream2_first_kernel=self.kernel_config['stream2_first_kernel']
        ).to(device)
        
        criterion = self._create_criterion(pos_weight, device)
        optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = self._create_lr_scheduler(optimizer, num_epochs, train_loader)
        
        # è®­ç»ƒå¾ªç¯
        train_losses = []
        train_accs = []
        train_aucs = []
        train_f1s = []
        val_losses = []
        val_accs = []
        val_aucs = []
        val_f1s = []
        
        best_train_acc = 0
        best_val_acc = 0
        best_val_auc = 0
        best_val_f1 = 0
        best_composite_score = 0
        best_model_state = None
        best_epoch = 0
        early_stop_counter = 0
        
        for epoch in range(1, num_epochs + 1):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            epoch_train_loss = 0.0
            train_probs = []
            train_labels = []
            
            for x, y in train_loader:
                x = x.to(device).float()
                x = x.view(-1, 1, FIXED_LENGTH)
                y = y.to(device).float()
                
                optimizer.zero_grad()
                logits = model(x)
                loss = criterion(logits, y)
                loss.backward()
                optimizer.step()
                
                epoch_train_loss += loss.item()
                
                # æ”¶é›†æ¦‚ç‡å’Œæ ‡ç­¾ï¼ˆç”¨äºåç»­è®¡ç®—æŒ‡æ ‡ï¼‰
                probs = torch.sigmoid(logits)
                train_probs.extend(probs.detach().cpu().numpy().flatten())
                train_labels.extend(y.detach().cpu().numpy().flatten())
            
            # ä½¿ç”¨æ–°å‡½æ•°è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡
            train_metrics = self._calculate_training_metrics(model, train_loader, criterion, device)
            
            avg_train_loss = epoch_train_loss / len(train_loader)
            train_acc = train_metrics['acc']
            train_auc = train_metrics['auc']
            train_f1 = train_metrics['f1']
            train_threshold = train_metrics['threshold']
            
            train_losses.append(avg_train_loss)
            train_accs.append(train_acc)
            train_aucs.append(train_auc)
            train_f1s.append(train_f1)
            
            # éªŒè¯é˜¶æ®µ
            val_res = self._validate_model(model, val_loader, criterion, device)
            
            val_loss = val_res['loss']
            val_acc = val_res['acc']
            val_auc = val_res['auc']
            val_f1 = val_res['f1']
            val_threshold = val_res['threshold']
            
            val_losses.append(val_loss)
            val_accs.append(val_acc)
            val_aucs.append(val_auc)
            val_f1s.append(val_f1)
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†
            val_metrics = {
                'accuracy': val_acc,
                'auc': val_auc,
                'f1': val_f1
            }
            composite_score, breakdown = CompositeScoreCalculator.calculate_composite_score(val_metrics)
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            if scheduler is not None:
                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    scheduler.step(composite_score)
                else:
                    scheduler.step()
            
            # è®°å½•å½“å‰å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = False
            if composite_score > best_composite_score + MIN_DELTA:
                is_best = True
                best_composite_score = composite_score
                best_val_acc = val_acc
                best_val_auc = val_auc
                best_val_f1 = val_f1
                best_epoch = epoch
                best_model_state = model.state_dict().copy()
                early_stop_counter = 0
            else:
                early_stop_counter += 1
            
            # æ›´æ–°è®­ç»ƒæœ€ä½³å‡†ç¡®ç‡
            if train_acc > best_train_acc:
                best_train_acc = train_acc
            
            # æ‰“å°è¿›åº¦ - ç°åœ¨æ˜¾ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„é˜ˆå€¼
            if epoch % 5 == 0 or epoch == 1 or epoch == num_epochs:
                print(f"  Epoch {epoch}/{num_epochs}:")
                print(f"    è®­ç»ƒ - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f}, F1: {train_f1:.4f}, é˜ˆå€¼: {train_threshold:.3f}")
                print(f"    éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}, F1: {val_f1:.4f}, é˜ˆå€¼: {val_threshold:.3f}")
                print(f"    ç»¼åˆè¯„åˆ†: {composite_score:.4f}, å­¦ä¹ ç‡: {current_lr:.2e}")
            
            # æ—©åœæ£€æŸ¥
            if early_stop_counter >= EARLY_STOP_PATIENCE:
                print(f"  âš ï¸ æ—©åœè§¦å‘äºepoch {epoch}ï¼Œè¿ç»­{EARLY_STOP_PATIENCE}ä¸ªepochéªŒè¯é›†æ— æ˜¾è‘—æå‡")
                break
        
        print(f"  æœ€ä½³éªŒè¯ç»¼åˆè¯„åˆ†: {best_composite_score:.4f} (Epoch {best_epoch})")
        print(f"  æœ€ç»ˆå­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€
        if best_model_state:
            model.load_state_dict(best_model_state)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        if save_model and self.experiment_dir:
            model_name = self.file_manager.generate_model_name(
                self._get_config_dict(),
                epoch=best_epoch,
                composite_score=best_composite_score
            )
            
            model_path = os.path.join(self.experiment_dir, "models", "final_" + model_name)
            metadata = {
                'best_train_acc': best_train_acc,
                'best_val_acc': best_val_acc,
                'best_val_auc': best_val_auc,
                'best_val_f1': best_val_f1,
                'best_composite_score': best_composite_score,
                'avg_train_loss': np.mean(train_losses),
                'num_epochs': epoch,  # å®é™…è®­ç»ƒçš„epochæ•°ï¼ˆå¯èƒ½å› æ—©åœè€Œå°äºnum_epochsï¼‰
                'best_epoch': best_epoch,
                'early_stopped': early_stop_counter >= EARLY_STOP_PATIENCE,
                'final_lr': optimizer.param_groups[0]['lr'],  # ä¿å­˜æœ€ç»ˆå­¦ä¹ ç‡
                'pos_weight': pos_weight.item(),  # ä¿å­˜æ­£ç±»æƒé‡
                'lr_scheduler_type': self.lr_scheduler_config.get('scheduler_type', 'plateau')
            }
            self.file_manager.save_model(model, model_path, metadata)
        
        train_metrics = {
            'final_train_loss': train_losses[-1],
            'final_train_acc': train_accs[-1],
            'best_train_acc': best_train_acc,
            'best_val_acc': best_val_acc,
            'best_val_auc': best_val_auc,
            'best_val_f1': best_val_f1,
            'best_composite_score': best_composite_score,
            'avg_train_loss': np.mean(train_losses),
            'avg_val_loss': np.mean(val_losses),
            'best_epoch': best_epoch,
            'total_epochs': epoch,
            'early_stopped': early_stop_counter >= EARLY_STOP_PATIENCE,
            'final_lr': optimizer.param_groups[0]['lr'],
            'pos_weight': pos_weight.item()
        }
        
        # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
        if self.experiment_dir:
            metrics_path = os.path.join(self.experiment_dir, "metrics", "final_training_metrics.json")
            self.file_manager.save_metrics(train_metrics, metrics_path)
        
        return model, train_metrics
    
    def test_basic_functionality(self):
        """æµ‹è¯•åŸºæœ¬åŠŸèƒ½ï¼Œç¡®è®¤æ²¡æœ‰å®ç°é”™è¯¯"""
        print("\n=== åŸºæœ¬åŠŸèƒ½æµ‹è¯• ===")
        
        # 1. åŠ è½½å°‘é‡æ•°æ®
        test_indices = [0]  # åªä½¿ç”¨CV0
        test_data = self.data_manager.load_cv_files(test_indices)
        
        if len(test_data) == 0:
            print("é”™è¯¯: æµ‹è¯•æ•°æ®ä¸ºç©º")
            return
        
        # åªå–å‰100ä¸ªæ ·æœ¬
        test_data = test_data[:100]
        
        # 2. åˆ›å»ºç®€å•çš„æ¨¡å‹
        model = Mscnn(
            INPUT_CHANNELS,
            OUTPUT_CLASSES,
            use_stream2=self.use_stream2,
            stream1_kernel=self.kernel_config['stream1_kernel'],
            stream2_first_kernel=self.kernel_config['stream2_first_kernel']
        ).to(device)
        
        # 3. åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
        test_dataset = FoldDataset(
            test_data, self.base_path, is_train=False, augment=False
        )
        test_loader = DataLoader(
            test_dataset, batch_size=1, shuffle=False, num_workers=0
        )
        
        # 4. æµ‹è¯•å‰å‘ä¼ æ’­
        model.eval()
        sample_count = 0
        with torch.no_grad():
            for x, y in test_loader:
                x = x.to(device).float()
                x = x.view(-1, 1, FIXED_LENGTH)
                y = y.to(device).float()
                
                outputs = model(x)
                probs = torch.sigmoid(outputs)
                
                # æ‰“å°å‰å‡ ä¸ªæ ·æœ¬çš„ä¿¡æ¯
                if sample_count < 5:
                    print(f"æ ·æœ¬ {sample_count}:")
                    print(f"  è¾“å…¥å½¢çŠ¶: {x.shape}")
                    print(f"  æ ‡ç­¾: {y.cpu().numpy()[0][0]:.1f}")
                    print(f"  è¾“å‡ºlogits: {outputs.cpu().numpy()[0][0]:.4f}")
                    print(f"  æ¦‚ç‡: {probs.cpu().numpy()[0][0]:.4f}")
                    print()
                
                sample_count += 1
                if sample_count >= 10:
                    break
        
        print(f"æµ‹è¯•å®Œæˆï¼Œå¤„ç†äº† {sample_count} ä¸ªæ ·æœ¬")
    
    def _train_single_fold(self, train_data, val_data, fold_idx, num_epochs, save_model=True, min_epochs=10):
        """è®­ç»ƒå•ä¸ªæŠ˜ï¼ŒåŒ…å«æ—©åœæœºåˆ¶ï¼ˆç®€åŒ–è¾“å‡ºç‰ˆæœ¬ï¼‰"""
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = FoldDataset(
            train_data, self.base_path, is_train=True, augment=self.augment
        )
        val_dataset = FoldDataset(
            val_data, self.base_path, is_train=False, augment=False
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset, batch_size=self.batch_size, 
            shuffle=True, num_workers=0
        )
        val_loader = DataLoader(
            val_dataset, batch_size=self.batch_size,  # ä½¿ç”¨ç›¸åŒæ‰¹æ¬¡å¤§å°
            shuffle=False, num_workers=0
        )
        
        # è®¡ç®—æ­£ç±»æƒé‡
        pos_weight = self._calculate_pos_weight(train_loader)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = Mscnn(
            INPUT_CHANNELS,
            OUTPUT_CLASSES,
            use_stream2=self.use_stream2,
            stream1_kernel=self.kernel_config['stream1_kernel'],
            stream2_first_kernel=self.kernel_config['stream2_first_kernel']
        ).to(device)
        
        criterion = self._create_criterion(pos_weight, device)
        optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = self._create_lr_scheduler(optimizer, num_epochs, train_loader)
        
        # è®­ç»ƒçŠ¶æ€
        best_val_acc = 0
        best_val_auc = 0
        best_val_f1 = 0
        best_composite_score = 0
        best_epoch = 0
        best_model_state = None
        early_stop_counter = 0
        
        # åˆ›å»ºç®€æ´çš„epochè¿›åº¦æ¡
        epoch_pbar = tqdm(
            range(1, num_epochs + 1), 
            desc=f"æŠ˜ {fold_idx+1} è®­ç»ƒè¿›åº¦",
            bar_format='{desc}: {percentage:3.0f}%|{bar:20}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
            position=0,
            leave=False
        )
        
        # è®­ç»ƒå¾ªç¯
        for epoch in epoch_pbar:
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            
            # æ‰¹æ¬¡è®­ç»ƒï¼ˆä¸æ˜¾ç¤ºå†…éƒ¨æ‰¹æ¬¡ä¿¡æ¯ï¼‰
            for x, y in train_loader:
                x = x.to(device).float()
                x = x.view(-1, 1, FIXED_LENGTH)
                y = y.to(device).float()
                
                optimizer.zero_grad()
                logits = model(x)
                loss = criterion(logits, y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            # è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡ï¼ˆé™é»˜æ¨¡å¼ï¼‰
            train_metrics = self._calculate_training_metrics(model, train_loader, criterion, device)
            avg_train_loss = train_loss / len(train_loader)
            
            # éªŒè¯é˜¶æ®µï¼ˆé™é»˜æ¨¡å¼ï¼‰
            val_res = self._validate_model(model, val_loader, criterion, device)
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†
            val_metrics_dict = {
                'accuracy': val_res['acc'],
                'auc': val_res['auc'],
                'f1': val_res['f1']
            }
            composite_score, _ = CompositeScoreCalculator.calculate_composite_score(val_metrics_dict)
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            if scheduler is not None:
                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    scheduler.step(val_res['loss'])  # ä½¿ç”¨éªŒè¯æŸå¤±
                else:
                    scheduler.step()
            
            # è®°å½•å½“å‰å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = False
            if composite_score > best_composite_score:
                if composite_score >= best_composite_score + MIN_DELTA:
                    is_best = True
                    best_composite_score = composite_score
                    best_val_acc = val_res['acc']
                    best_val_auc = val_res['auc']
                    best_val_f1 = val_res['f1']
                    best_epoch = epoch
                    best_model_state = model.state_dict().copy()
                    early_stop_counter = 0
                else:
                    early_stop_counter += 1
            else:
                early_stop_counter += 1
            
            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
            epoch_pbar.set_postfix({
                'loss': f"{avg_train_loss:.3f}",
                'val_acc': f"{val_res['acc']:.3f}",
                'val_f1': f"{val_res['f1']:.3f}",
                'lr': f"{current_lr:.1e}"
            })
            
            # æ¯5ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            if epoch % 5 == 0 or epoch == 1 or epoch == num_epochs:
                print(f"    Epoch {epoch:3d}/{num_epochs}: "
                    f"è®­ç»ƒæŸå¤±={avg_train_loss:.4f}, å‡†ç¡®ç‡={train_metrics['acc']:.4f} | "
                    f"éªŒè¯å‡†ç¡®ç‡={val_res['acc']:.4f}, F1={val_res['f1']:.4f} | "
                    f"å­¦ä¹ ç‡={current_lr:.2e}" + (" â˜…" if is_best else ""))
            
            # æ—©åœæ£€æŸ¥ï¼ˆè‡³å°‘è®­ç»ƒmin_epochsä¸ªepochï¼‰
            if epoch >= min_epochs and early_stop_counter >= EARLY_STOP_PATIENCE:
                print(f"    â¹ï¸  æ—©åœè§¦å‘äºepoch {epoch}ï¼Œè¿ç»­{EARLY_STOP_PATIENCE}ä¸ªepochéªŒè¯é›†æ— æ˜¾è‘—æå‡")
                break
        
        epoch_pbar.close()
        
        # åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€
        if best_model_state:
            model.load_state_dict(best_model_state)
        
        # ä¿å­˜å½“å‰æŠ˜çš„æœ€ä½³æ¨¡å‹
        if save_model and self.experiment_dir:
            model_name = self.file_manager.generate_model_name(
                self._get_config_dict(),
                fold=fold_idx,
                epoch=best_epoch,
                composite_score=best_composite_score
            )
            
            model_path = os.path.join(self.experiment_dir, "models", f"fold{fold_idx}_" + model_name)
            metadata = {
                'fold': fold_idx,
                'val_acc': best_val_acc,
                'val_auc': best_val_auc,
                'val_f1': best_val_f1,
                'composite_score': best_composite_score,
                'epoch': best_epoch,
                'early_stopped': early_stop_counter >= EARLY_STOP_PATIENCE,
                'pos_weight': pos_weight.item(),
                'final_lr': optimizer.param_groups[0]['lr'],
                'lr_scheduler_type': self.lr_scheduler_config.get('scheduler_type', 'plateau')
            }
            self.file_manager.save_model(model, model_path, metadata)
        
        fold_result = {
            'fold': fold_idx,
            'best_val_acc': float(best_val_acc),
            'best_val_auc': float(best_val_auc),
            'best_val_f1': float(best_val_f1),
            'best_composite_score': float(best_composite_score),
            'best_epoch': best_epoch,
            'early_stopped': early_stop_counter >= EARLY_STOP_PATIENCE,
            'total_epochs': epoch
        }
        
        return fold_result, model

    
    def _calculate_pos_weight(self, dataloader):
        """è®¡ç®—æ­£ç±»æƒé‡"""
        if self.use_focal_loss:
            # Focal Lossä¸ä½¿ç”¨pos_weightï¼Œè¿”å›Noneæˆ–é»˜è®¤å€¼
            print(f"âš ï¸ Focal Lossä¸ä½¿ç”¨pos_weightï¼Œå°†å¿½ç•¥æ­¤å‚æ•°")
            return torch.tensor([1.0], dtype=torch.float32).to(device)


        all_labels = []
        for _, y in dataloader:
            all_labels.extend(y.numpy().flatten())
        
        all_labels = np.array(all_labels, dtype=int)
        class_counts = np.bincount(all_labels, minlength=2)
        
        # è®¡ç®—æ­£ç±»æ¯”ä¾‹
        total_samples = np.sum(class_counts)
        positive_ratio = class_counts[1] / total_samples
        negative_ratio = class_counts[0] / total_samples
        
        print(f"ç±»åˆ«åˆ†å¸ƒ: è´Ÿç±»={class_counts[0]}, æ­£ç±»={class_counts[1]}, æ­£ç±»æ¯”ä¾‹={positive_ratio:.2%}")

        # if positive_ratio < 0.2:  # æ­£ç±»æ¯”ä¾‹ä½äº20%
        #     # ä½¿ç”¨2-5ä¹‹é—´çš„æƒé‡ï¼Œè€Œä¸æ˜¯10.39
        #     adjusted_weight = min(5.0, max(2.0, 1.0 / positive_ratio))
        # else:
        #     adjusted_weight = 1.0

        # æ–¹æ³•1: åŸºäºé€†é¢‘ç‡ï¼ˆå½“å‰1:10.6æ¯”ä¾‹ï¼‰
        raw_weight = class_counts[0] / class_counts[1]  # çº¦10.6
        adjusted_weight = min(8.0, max(3.0, raw_weight * 0.6))
    
        
        pos_weight = torch.tensor([adjusted_weight], dtype=torch.float32)
        
        print(f"ä½¿ç”¨æ­£ç±»æƒé‡: {adjusted_weight:.2f} (åŸå§‹æƒé‡: {class_counts[0]/class_counts[1] if class_counts[1] > 0 else 0:.2f})")
        
        return pos_weight.to(device)
    
    def _validate_model(self, model, val_loader, criterion, device):
        """éªŒè¯æ¨¡å‹"""
        model.eval()
        val_loss = 0.0
        all_probs = []
        all_labels = []
        all_logits = []  # æ–°å¢ï¼šç”¨äºè°ƒè¯•
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                
                # ç¡®ä¿è¾“å…¥å½¢çŠ¶æ­£ç¡®
                if inputs.dim() == 2:
                    inputs = inputs.unsqueeze(1)  # [batch, 1, length]
                
                # æ¨¡å‹è¾“å‡ºlogits
                logits = model(inputs)
                
                # ç»Ÿä¸€ç»´åº¦
                if labels.dim() == 1:
                    labels = labels.unsqueeze(1)
                
                # è®¡ç®—æŸå¤±ï¼ˆè¾“å…¥logitsï¼‰
                loss = criterion(logits, labels.float())
                val_loss += loss.item()
                
                # è·å–æ¦‚ç‡ï¼ˆæ‰‹åŠ¨åº”ç”¨sigmoidï¼‰
                probs = torch.sigmoid(logits)
                
                # è°ƒè¯•ä¿¡æ¯
                all_logits.extend(logits.cpu().numpy().flatten().tolist())
                all_probs.extend(probs.cpu().numpy().flatten().tolist())
                all_labels.extend(labels.cpu().numpy().flatten().tolist())
        
        avg_val_loss = val_loss / len(val_loader)
        all_probs = np.array(all_probs)
        all_labels = np.array(all_labels)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if len(all_logits) > 0:
            all_logits = np.array(all_logits)
            print(f"éªŒè¯é›†logitsç»Ÿè®¡: min={all_logits.min():.4f}, max={all_logits.max():.4f}, mean={all_logits.mean():.4f}")
            print(f"éªŒè¯é›†æ¦‚ç‡ç»Ÿè®¡: min={all_probs.min():.4f}, max={all_probs.max():.4f}, mean={all_probs.mean():.4f}")
        
        # é™é»˜å¤„ç†æ ‡ç­¾è½¬æ¢
        if all_labels.dtype != np.int64 and all_labels.dtype != np.int32:
            all_labels = np.round(all_labels).astype(int)
        
        # å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
        best_threshold, best_f1, _, _ = self._find_optimal_threshold(all_labels, all_probs)
        
        # åŸºäºæœ€ä¼˜é˜ˆå€¼è®¡ç®—å‡†ç¡®ç‡
        preds = (all_probs >= best_threshold).astype(int)
        acc = accuracy_score(all_labels, preds)
        
        try:
            auc = roc_auc_score(all_labels, all_probs)
        except:
            auc = 0.5
        
        return {
            'loss': avg_val_loss,
            'acc': acc,
            'auc': auc,
            'f1': best_f1,
            'threshold': best_threshold,
            'probs': all_probs,
            'labels': all_labels,
            'logits': all_logits if 'all_logits' in locals() else None  # å¯é€‰ï¼šè¿”å›logits
        }
    
    def _compute_average_metrics(self, fold_results):
        if not fold_results:
            return {
                'avg_val_acc': 0.0,
                'std_val_acc': 0.0,
                'avg_val_auc': 0.0,
                'std_val_auc': 0.0,
                'avg_val_f1': 0.0,
                'std_val_f1': 0.0,
                'num_folds': 0
            }
    
        val_accs = [r['best_val_acc'] for r in fold_results]
        val_aucs = [r['best_val_auc'] for r in fold_results]
        val_f1s = [r['best_val_f1'] for r in fold_results]
        
        return {
            'avg_val_acc': float(np.mean(val_accs)),
            'std_val_acc': float(np.std(val_accs)),
            'avg_val_auc': float(np.mean(val_aucs)),
            'std_val_auc': float(np.std(val_aucs)),
            'avg_val_f1': float(np.mean(val_f1s)),
            'std_val_f1': float(np.std(val_f1s)),
            'num_folds': len(fold_results)
        }
    
    def evaluate_on_test_set(self, test_cv_indices, model, save_results=True, 
                        optimize_threshold=True, use_two_stage=True,
                        stage1_threshold=0.4, stage2_threshold=None):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå¯é€‰é˜ˆå€¼ä¼˜åŒ–"""
        print(f"\nåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹")
        print(f"æµ‹è¯•é›†: CV{', '.join(map(str, test_cv_indices))}")

        # ==================== å‚æ•°éªŒè¯å’Œé»˜è®¤å€¼è®¾ç½® ====================
        # è®¾ç½®é»˜è®¤å€¼
        if stage1_threshold is None:
            stage1_threshold = 0.4
        if stage2_threshold is None:
            stage2_threshold = 0.76
        
        print(f"åˆå§‹é˜ˆå€¼è®¾ç½®: stage1={stage1_threshold:.2f}, stage2={stage2_threshold:.2f}")
    # ===========================================================
        
        # åŠ è½½æµ‹è¯•é›†æ•°æ®
        test_data = self.data_manager.load_cv_files(test_cv_indices)
        if len(test_data) == 0:
            print("é”™è¯¯: æµ‹è¯•é›†æ•°æ®ä¸ºç©º")   
            return {}
        
        # åˆ›å»ºæ•°æ®é›†
        test_dataset = FoldDataset(
            test_data, self.base_path, is_train=False, augment=False
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        test_loader = DataLoader(
            test_dataset, batch_size=1, shuffle=False, num_workers=0
        )
        
        # ==================== åˆ›å»ºä¸€è‡´çš„æŸå¤±å‡½æ•° ====================
        if self.use_focal_loss:
            if self.focal_alpha is not None:
                criterion = FocalLoss(
                    alpha=self.focal_alpha,
                    gamma=self.focal_gamma,
                    reduction='mean',
                    logits=True
                ).to(device)
                print(f"è¯„ä¼°ä½¿ç”¨Focal Loss: alpha={self.focal_alpha}, gamma={self.focal_gamma}")
            else:
                criterion = WeightedFocalLoss(
                    pos_weight=None,
                    gamma=self.focal_gamma,
                    reduction='mean',
                    logits=True
                ).to(device)
                print(f"è¯„ä¼°ä½¿ç”¨WeightedFocalLoss: gamma={self.focal_gamma}")
        else:
            criterion = torch.nn.BCEWithLogitsLoss().to(device)
            print(f"è¯„ä¼°ä½¿ç”¨BCEWithLogitsLoss")

        # ==================== æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥å‰å‡ ä¸ªæ ·æœ¬ ====================
        print(f"\nå‰5ä¸ªæ ·æœ¬çš„æ¨¡å‹è¾“å‡ºæ£€æŸ¥:")
        model.eval()
        sample_count = 0
        with torch.no_grad():
            for x, y in test_loader:
                x = x.to(device).float()
                x = x.view(-1, 1, FIXED_LENGTH)
                y = y.to(device).float()
                
                logits = model(x)
                probs = torch.sigmoid(logits)
                
                print(f"æ ·æœ¬{sample_count}: logits={logits.cpu().numpy()[0][0]:.6f}, "
                    f"prob={probs.cpu().numpy()[0][0]:.6f}, label={y.cpu().numpy()[0][0]:.0f}")
                
                sample_count += 1
                if sample_count >= 5:
                    break
        
        # è¯„ä¼° - è·å–åŸå§‹æ¦‚ç‡
        test_res = self._validate_model(model, test_loader, criterion, device)

        test_probs = test_res['probs']
        test_labels = test_res['labels']
        test_loss = test_res['loss']
        test_auc = test_res['auc']
        
        # ==================== æ·»åŠ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ ====================
        print(f"\næµ‹è¯•é›†è¯¦ç»†ç»Ÿè®¡:")
        print(f"æ€»æ ·æœ¬æ•°: {len(test_probs)}")
        print(f"æ­£æ ·æœ¬æ•°: {np.sum(test_labels)}")
        print(f"è´Ÿæ ·æœ¬æ•°: {len(test_labels) - np.sum(test_labels)}")
        print(f"æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(test_labels):.2%}")
        
        print(f"\næ¨¡å‹è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ:")
        print(f"æœ€å°å€¼: {np.min(test_probs):.6f}")
        print(f"æœ€å¤§å€¼: {np.max(test_probs):.6f}")
        print(f"å¹³å‡å€¼: {np.mean(test_probs):.6f}")
        print(f"ä¸­ä½æ•°: {np.median(test_probs):.6f}")
        print(f"æ ‡å‡†å·®: {np.std(test_probs):.6f}")
        
        # æ¦‚ç‡åˆ†å¸ƒç›´æ–¹å›¾
        print(f"\næ¦‚ç‡åˆ†å¸ƒç›´æ–¹å›¾:")
        bins = np.linspace(0, 1, 21)
        hist, bin_edges = np.histogram(test_probs, bins=bins)
        for i in range(len(hist)):
            print(f"  {bin_edges[i]:.2f}-{bin_edges[i+1]:.2f}: {hist[i]} samples")
        
        # æŸ¥çœ‹å„ä¸ªé˜ˆå€¼ä¸‹çš„é¢„æµ‹æƒ…å†µ
        print(f"\nä¸åŒé˜ˆå€¼ä¸‹çš„é¢„æµ‹ç»“æœ:")
        thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
        for th in thresholds:
            preds = (test_probs >= th).astype(int)
            pos_preds = np.sum(preds)
            print(f"  é˜ˆå€¼={th:.1f}: é¢„æµ‹æ­£æ ·æœ¬æ•°={pos_preds}")

        # ==================== ä¼˜åŒ–é˜¶æ®µï¼šæé«˜ç²¾ç¡®ç‡å’ŒF1 ====================
        if use_two_stage:
            print("\nğŸ¯ ä½¿ç”¨ä¸¤é˜¶æ®µé˜ˆå€¼ç­–ç•¥ï¼ˆä¼˜åŒ–ç²¾ç¡®ç‡å’ŒF1ï¼‰...")
            
            # æ–¹æ¡ˆ1ï¼šæé«˜ç¬¬äºŒé˜¶æ®µé˜ˆå€¼ä»¥æé«˜ç²¾ç¡®ç‡
            if stage2_threshold is None:
                # å¯»æ‰¾æ›´æ³¨é‡ç²¾ç¡®ç‡çš„é˜ˆå€¼
                stage2_threshold = self._find_precision_optimized_threshold(test_labels, test_probs)
            
            # æ–¹æ¡ˆ2ï¼šå°è¯•å¤šä¸ªé˜ˆå€¼ç»„åˆï¼Œé€‰æ‹©F1æœ€é«˜çš„
            best_metrics = self._find_best_two_stage_combo(test_labels, test_probs)
            
            if best_metrics['f1'] > 0.82:  # å¦‚æœæ‰¾åˆ°æ›´å¥½çš„ç»„åˆ
                stage1_threshold = best_metrics['stage1_threshold']
                stage2_threshold = best_metrics['stage2_threshold']
                print(f"ä½¿ç”¨ä¼˜åŒ–åçš„é˜ˆå€¼ç»„åˆ: stage1={stage1_threshold:.2f}, stage2={stage2_threshold:.2f}")
            else:
                print(f"ä½¿ç”¨é»˜è®¤/æŒ‡å®šé˜ˆå€¼: stage1={stage1_threshold:.2f}, stage2={stage2_threshold:.2f}")
            
            # è°ƒç”¨ä¸¤é˜¶æ®µè¯„ä¼°æ–¹æ³•
            two_stage_results = self._two_stage_evaluate(
                test_probs, test_labels, 
                stage1_th=stage1_threshold, 
                stage2_th=stage2_threshold
            )
            
            test_acc = two_stage_results['acc']
            test_precision = two_stage_results['precision']
            test_recall = two_stage_results['recall']
            test_f1 = two_stage_results['f1']
            best_threshold = two_stage_results['stage2_threshold']
            stage1_recall = two_stage_results['stage1_recall']
            
            print(f"ä¼˜åŒ–åçš„ä¸¤é˜¶æ®µç»“æœ:")
            print(f"  ç¬¬ä¸€é˜¶æ®µå¬å›ç‡: {stage1_recall:.4f}")
            print(f"  æœ€ç»ˆç²¾ç¡®ç‡: {test_precision:.4f} (ç›®æ ‡: 80%+)")
            print(f"  æœ€ç»ˆå¬å›ç‡: {test_recall:.4f}")
            print(f"  æœ€ç»ˆF1: {test_f1:.4f} (ç›®æ ‡: 83%+)")
            
        else:
            # å•é˜¶æ®µé˜ˆå€¼ä¼˜åŒ–
            print("\nä½¿ç”¨å•é˜¶æ®µé˜ˆå€¼ç­–ç•¥...")
            
            if optimize_threshold:
                # ä½¿ç”¨æ›´æ³¨é‡ç²¾ç¡®ç‡çš„é˜ˆå€¼å¯»æ‰¾æ–¹æ³•
                best_threshold, test_f1, test_precision, test_recall = self._find_precision_optimized_threshold(
                    test_labels, test_probs, return_all=True
                )
                
                best_preds = (test_probs >= best_threshold).astype(int)
                test_acc = accuracy_score(test_labels, best_preds)
                
                print(f"ç²¾ç¡®ç‡ä¼˜åŒ–é˜ˆå€¼: {best_threshold:.3f}")
                print(f"ç²¾ç¡®ç‡: {test_precision:.4f}, F1: {test_f1:.4f}")
            else:
                best_threshold = 0.5
                best_preds = (test_probs >= 0.5).astype(int)
                test_acc = accuracy_score(test_labels, best_preds)
                test_precision = precision_score(test_labels, best_preds, zero_division=0)
                test_recall = recall_score(test_labels, best_preds, zero_division=0)
                test_f1 = f1_score(test_labels, best_preds, zero_division=0)
                print(f"ä½¿ç”¨é»˜è®¤é˜ˆå€¼: {best_threshold}")
        
        # ==================== è®¡ç®—ç»¼åˆè¯„åˆ† ====================
        test_metrics = {
            'accuracy': test_acc,
            'auc': test_auc,
            'f1': test_f1
        }
        test_composite_score, breakdown = CompositeScoreCalculator.calculate_composite_score(test_metrics)
        
        # ==================== æ„å»ºç»“æœå­—å…¸ ====================
        test_results = {
            'test_acc': test_acc,
            'test_auc': test_auc,
            'test_precision': test_precision,
            'test_recall': test_recall,
            'test_f1': test_f1,
            'test_composite_score': test_composite_score,
            'test_loss': test_loss,
            'evaluation_time': datetime.now().isoformat(),
            'score_breakdown': breakdown,
            'optimal_threshold': best_threshold,
            'threshold_strategy': 'two_stage_optimized' if use_two_stage else ('single_optimized' if optimize_threshold else 'single_default'),
            'class_distribution': self._get_class_distribution(test_labels)
        }
        
        # æ·»åŠ ä¸¤é˜¶æ®µç‰¹å®šä¿¡æ¯
        if use_two_stage:
            test_results.update({
                'two_stage_used': True,
                'stage1_threshold': stage1_threshold,
                'stage2_threshold': stage2_threshold,
                'stage1_recall': stage1_recall
            })
        else:
            test_results['two_stage_used'] = False
        
        # ==================== æ‰“å°ç»“æœ ====================
        print(f"\nğŸ“Š æµ‹è¯•é›†æœ€ç»ˆç»“æœ:")
        print(f"  é˜ˆå€¼ç­–ç•¥: {'ä¸¤é˜¶æ®µä¼˜åŒ–' if use_two_stage else 'å•é˜¶æ®µ' + (' (ä¼˜åŒ–)' if optimize_threshold else ' (é»˜è®¤)')}")
        if use_two_stage:
            print(f"  ç¬¬ä¸€é˜¶æ®µé˜ˆå€¼: {stage1_threshold:.3f}")
            print(f"  ç¬¬äºŒé˜¶æ®µé˜ˆå€¼: {stage2_threshold:.3f}")
        else:
            print(f"  é˜ˆå€¼: {best_threshold:.3f}")
        print(f"  å‡†ç¡®ç‡: {test_acc:.4f}")
        print(f"  AUC: {test_auc:.4f}")
        print(f"  ç²¾ç¡®ç‡: {test_precision:.4f}")
        print(f"  å¬å›ç‡: {test_recall:.4f}")
        print(f"  F1åˆ†æ•°: {test_f1:.4f}")
        print(f"  ç»¼åˆè¯„åˆ†: {test_composite_score:.4f}")
        print(f"  æŸå¤±: {test_loss:.4f}")
        
        # ==================== ä¿å­˜è¯„ä¼°ç»“æœ ====================
        if save_results and self.experiment_dir:
            results_path = os.path.join(self.experiment_dir, "metrics", "test_evaluation.json")
            self.file_manager.save_metrics(test_results, results_path)
            print(f"ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜: {results_path}")
        
        return test_results

    def _find_optimal_threshold(self, labels, logits_or_probs, metric='f1',is_logits=True):
        """å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼"""
        # å¦‚æœè¾“å…¥æ˜¯logitsï¼Œå…ˆè½¬æ¢ä¸ºæ¦‚ç‡
        if is_logits:
            probs = 1 / (1 + np.exp(-logits_or_probs))  # æ‰‹åŠ¨sigmoid
        else:
            probs = logits_or_probs

        # ç¡®ä¿ labels æ˜¯æ•´æ•°ç±»å‹
        labels = np.array(labels)
        probs = np.array(probs)
        
        # é™é»˜å¤„ç†æ ‡ç­¾è½¬æ¢
        if labels.dtype != np.int64 and labels.dtype != np.int32:
            # é€šè¿‡å››èˆäº”å…¥å°†æµ®ç‚¹æ•°è½¬æ¢ä¸º0/1
            labels = np.round(labels).astype(int)
        
        # ç¡®ä¿ probs åœ¨0-1èŒƒå›´å†…
        if np.min(probs) < 0 or np.max(probs) > 1:
            probs = np.clip(probs, 0, 1)
        
        best_threshold = 0.5
        best_score = 0
        best_precision = 0
        best_recall = 0
        
        # å°è¯•å¤šä¸ªé˜ˆå€¼
        thresholds = np.linspace(0.1, 0.9, 81)
        
        for threshold in thresholds:
            preds = (probs >= threshold).astype(int)
            
            try:
                if metric == 'f1':
                    score = f1_score(labels, preds, zero_division=0)
                elif metric == 'balanced_accuracy':
                    score = balanced_accuracy_score(labels, preds)
                else:
                    score = f1_score(labels, preds, zero_division=0)
            except:
                score = 0
            
            if score > best_score:
                best_score = score
                best_threshold = threshold
        
        # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        best_preds = (probs >= best_threshold).astype(int)
        best_precision = precision_score(labels, best_preds, zero_division=0)
        best_recall = recall_score(labels, best_preds, zero_division=0)
        best_f1 = f1_score(labels, best_preds, zero_division=0)
        
        return best_threshold, best_f1, best_precision, best_recall


    def _calculate_training_metrics(self, model, train_loader, criterion, device):
        """è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡"""
        model.eval()
        train_loss = 0.0
        all_probs = []
        all_labels = []
        
        with torch.no_grad():
            for x, y in train_loader:
                x = x.to(device).float()
                x = x.view(-1, 1, FIXED_LENGTH)
                y = y.to(device).float()
                
                # æ¨¡å‹è¾“å‡ºlogits
                logits = model(x)
                
                # è®¡ç®—æŸå¤±
                loss = criterion(logits, y)
                train_loss += loss.item()
                
                # æ‰‹åŠ¨åº”ç”¨sigmoidè·å–æ¦‚ç‡
                probs = torch.sigmoid(logits)
                all_probs.extend(probs.cpu().numpy().flatten().tolist())
                all_labels.extend(y.cpu().numpy().flatten().tolist())
        
        avg_train_loss = train_loss / len(train_loader)
        all_probs = np.array(all_probs)
        all_labels = np.array(all_labels)
        
        # ä½¿ç”¨ä¸éªŒè¯é›†ç›¸åŒçš„é˜ˆå€¼è®¡ç®—æ–¹æ³•
        best_threshold, best_f1, _, _ = self._find_optimal_threshold(all_labels, all_probs)
        
        # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è®¡ç®—é¢„æµ‹
        preds = (all_probs >= best_threshold).astype(int)
        train_acc = accuracy_score(all_labels, preds)
        
        try:
            train_auc = roc_auc_score(all_labels, all_probs)
        except:
            train_auc = 0.5
        
        return {
            'loss': avg_train_loss,
            'acc': train_acc,
            'auc': train_auc,
            'f1': best_f1,
            'threshold': best_threshold,
            'probs': all_probs,
            'labels': all_labels
        }
    
    def _get_class_distribution(self, labels):
        """è·å–ç±»åˆ«åˆ†å¸ƒ"""
        labels = np.array(labels)
        total = len(labels)
        positive = np.sum(labels)
        negative = total - positive
        
        return {
            'total': int(total),
            'positive': int(positive),
            'negative': int(negative),
            'positive_ratio': float(positive / total),
            'negative_ratio': float(negative / total)
        }
        
    def _visualize_cv_results(self, fold_results, avg_metrics):
        """Visualize cross-validation results (English labels)"""
        if not self.experiment_dir:
            return
        
        # Extract fold performance metrics
        fold_accs = [r['best_val_acc'] for r in fold_results]
        fold_aucs = [r['best_val_auc'] for r in fold_results]
        fold_f1s = [r['best_val_f1'] for r in fold_results]
        
        # Create subplots
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('Cross-Validation Performance Analysis', fontsize=16, fontweight='bold')
        
        # 1. Bar chart of fold accuracies
        axes[0, 0].bar(range(1, len(fold_accs) + 1), fold_accs, 
                    color='skyblue', edgecolor='black', alpha=0.8)
        axes[0, 0].axhline(y=avg_metrics['avg_val_acc'], color='red', linestyle='--', 
                        linewidth=2, label=f'Mean: {avg_metrics["avg_val_acc"]:.4f}')
        axes[0, 0].set_title('Validation Accuracy per Fold', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Fold Number', fontsize=12)
        axes[0, 0].set_ylabel('Accuracy', fontsize=12)
        axes[0, 0].set_xticks(range(1, len(fold_accs) + 1))
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3, axis='y')
        
        # Add value labels on bars
        for i, acc in enumerate(fold_accs):
            axes[0, 0].text(i + 1, acc + 0.005, f'{acc:.3f}', 
                        ha='center', va='bottom', fontsize=9)
        
        # 2. Bar chart of fold AUC scores
        axes[0, 1].bar(range(1, len(fold_aucs) + 1), fold_aucs, 
                    color='lightgreen', edgecolor='black', alpha=0.8)
        axes[0, 1].axhline(y=avg_metrics['avg_val_auc'], color='red', linestyle='--', 
                        linewidth=2, label=f'Mean: {avg_metrics["avg_val_auc"]:.4f}')
        axes[0, 1].set_title('AUC Score per Fold', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Fold Number', fontsize=12)
        axes[0, 1].set_ylabel('AUC Score', fontsize=12)
        axes[0, 1].set_xticks(range(1, len(fold_aucs) + 1))
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3, axis='y')
        
        # Add value labels on bars
        for i, auc in enumerate(fold_aucs):
            axes[0, 1].text(i + 1, auc + 0.005, f'{auc:.3f}', 
                        ha='center', va='bottom', fontsize=9)
        
        # 3. Bar chart of fold F1 scores
        axes[1, 0].bar(range(1, len(fold_f1s) + 1), fold_f1s, 
                    color='lightcoral', edgecolor='black', alpha=0.8)
        axes[1, 0].axhline(y=avg_metrics['avg_val_f1'], color='red', linestyle='--', 
                        linewidth=2, label=f'Mean: {avg_metrics["avg_val_f1"]:.4f}')
        axes[1, 0].set_title('F1 Score per Fold', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Fold Number', fontsize=12)
        axes[1, 0].set_ylabel('F1 Score', fontsize=12)
        axes[1, 0].set_xticks(range(1, len(fold_f1s) + 1))
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3, axis='y')
        
        # Add value labels on bars
        for i, f1 in enumerate(fold_f1s):
            axes[1, 0].text(i + 1, f1 + 0.005, f'{f1:.3f}', 
                        ha='center', va='bottom', fontsize=9)
        
        # 4. Performance metrics summary table
        axes[1, 1].axis('off')
        
        # Create table data
        table_data = []
        for i in range(len(fold_results)):
            table_data.append([
                f'Fold {i+1}',
                f'{fold_accs[i]:.4f}',
                f'{fold_aucs[i]:.4f}',
                f'{fold_f1s[i]:.4f}'
            ])
        
        # Add average row
        table_data.append([
            'Average Â± Std',
            f'{avg_metrics["avg_val_acc"]:.4f} Â± {avg_metrics["std_val_acc"]:.4f}',
            f'{avg_metrics["avg_val_auc"]:.4f} Â± {avg_metrics["std_val_auc"]:.4f}',
            f'{avg_metrics["avg_val_f1"]:.4f} Â± {avg_metrics["std_val_f1"]:.4f}'
        ])
        
        # Create table
        table = axes[1, 1].table(
            cellText=table_data,
            colLabels=['Fold', 'Accuracy', 'AUC', 'F1 Score'],
            colWidths=[0.15, 0.25, 0.25, 0.25],
            cellLoc='center',
            loc='center',
            fontsize=11
        )
        
        # Style the table
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1.2, 1.5)
        
        # Style header row
        for i in range(4):
            table[(0, i)].set_facecolor('#40466e')
            table[(0, i)].set_text_props(weight='bold', color='white')
        
        # Style average row
        for i in range(4):
            table[(len(fold_results), i)].set_facecolor('#f2f2f2')
            table[(len(fold_results), i)].set_text_props(weight='bold')
        
        # Style alternating rows
        for i in range(1, len(fold_results)):
            for j in range(4):
                if i % 2 == 0:
                    table[(i, j)].set_facecolor('#f9f9f9')
        
        axes[1, 1].set_title('Cross-Validation Performance Summary', 
                            fontsize=14, fontweight='bold', y=1.05)
        
        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout for suptitle
        
        # Save visualization
        vis_path = os.path.join(self.experiment_dir, "visualizations", "cv_results_comparison.png")
        plt.savefig(vis_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        # Also create a performance trend visualization
        self._create_performance_trend_visualization(fold_results)
        
        print(f"ğŸ“Š Cross-validation visualization saved: {vis_path}")

    def _create_performance_trend_visualization(self, fold_results):
        """Create a line chart showing performance trends across folds"""
        if not self.experiment_dir:
            return
        
        # Extract metrics for trend analysis
        fold_numbers = list(range(1, len(fold_results) + 1))
        accuracies = [r['best_val_acc'] for r in fold_results]
        aucs = [r['best_val_auc'] for r in fold_results]
        f1_scores = [r['best_val_f1'] for r in fold_results]
        
        # Create trend visualization
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Plot lines with markers
        ax.plot(fold_numbers, accuracies, 'o-', color='skyblue', linewidth=2, markersize=8, 
                label='Accuracy', alpha=0.8)
        ax.plot(fold_numbers, aucs, 's-', color='lightgreen', linewidth=2, markersize=8, 
                label='AUC', alpha=0.8)
        ax.plot(fold_numbers, f1_scores, '^-', color='lightcoral', linewidth=2, markersize=8, 
                label='F1 Score', alpha=0.8)
        
        # Calculate and plot trend lines
        if len(fold_numbers) >= 3:
            # Linear regression for accuracy trend
            z_acc = np.polyfit(fold_numbers, accuracies, 1)
            p_acc = np.poly1d(z_acc)
            ax.plot(fold_numbers, p_acc(fold_numbers), '--', color='skyblue', alpha=0.5, 
                    label='Accuracy Trend')
            
            # Linear regression for AUC trend
            z_auc = np.polyfit(fold_numbers, aucs, 1)
            p_auc = np.poly1d(z_auc)
            ax.plot(fold_numbers, p_auc(fold_numbers), '--', color='lightgreen', alpha=0.5, 
                    label='AUC Trend')
            
            # Linear regression for F1 trend
            z_f1 = np.polyfit(fold_numbers, f1_scores, 1)
            p_f1 = np.poly1d(z_f1)
            ax.plot(fold_numbers, p_f1(fold_numbers), '--', color='lightcoral', alpha=0.5, 
                    label='F1 Trend')
        
        # Style the plot
        ax.set_title('Performance Trends Across Cross-Validation Folds', 
                    fontsize=16, fontweight='bold')
        ax.set_xlabel('Fold Number', fontsize=12)
        ax.set_ylabel('Score Value', fontsize=12)
        ax.set_xticks(fold_numbers)
        ax.set_ylim([0.5, 1.0])  # Set reasonable y-limits for classification metrics
        ax.grid(True, alpha=0.3, linestyle='--')
        ax.legend(loc='best', fontsize=10)
        
        # Add value annotations
        for i, (acc, auc, f1) in enumerate(zip(accuracies, aucs, f1_scores)):
            ax.annotate(f'{acc:.3f}', (fold_numbers[i], acc), 
                    textcoords="offset points", xytext=(0,5), 
                    ha='center', fontsize=8, color='skyblue')
            ax.annotate(f'{auc:.3f}', (fold_numbers[i], auc), 
                    textcoords="offset points", xytext=(0,5), 
                    ha='center', fontsize=8, color='lightgreen')
            ax.annotate(f'{f1:.3f}', (fold_numbers[i], f1), 
                    textcoords="offset points", xytext=(0,5), 
                    ha='center', fontsize=8, color='lightcoral')
        
        # Add statistics text box
        stats_text = f"""Statistics:
    Mean Accuracy: {np.mean(accuracies):.4f} Â± {np.std(accuracies):.4f}
    Mean AUC: {np.mean(aucs):.4f} Â± {np.std(aucs):.4f}
    Mean F1: {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}

    Total Folds: {len(fold_results)}
    """
        
        # Place text box in upper left
        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=10,
                verticalalignment='top', bbox=props)
        
        plt.tight_layout()
        
        # Save trend visualization
        trend_path = os.path.join(self.experiment_dir, "visualizations", "cv_performance_trend.png")
        plt.savefig(trend_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ Performance trend visualization saved: {trend_path}")

    def _get_config_dict(self):
        """è·å–å½“å‰è®­ç»ƒé…ç½®"""
        return {
            'kernel_config': self.kernel_config,
            'batch_size': self.batch_size,
            'lr': self.lr,
            'use_stream2': self.use_stream2,
            'augment': self.augment,
            'config_name': self.config_name,
            'composite_weights': self.composite_weights
        }


# ==================== æ”¹è¿›çš„è¶…å‚æ•°æœç´¢æ¨¡å— ====================
class HyperparameterSearcher:
    """è¶…å‚æ•°æœç´¢å™¨ï¼Œä½¿ç”¨ç»¼åˆè¯„åˆ†é€‰æ‹©æœ€ä½³é…ç½®"""
    
    # æœç´¢é…ç½®
    KERNEL_CONFIGS = [
        {'name': 'MS-CNN(3,9)', 'stream1_kernel': 3, 'stream2_first_kernel': 9},
        {'name': 'MS-CNN(3,7)', 'stream1_kernel': 3, 'stream2_first_kernel': 7},
        {'name': 'MS-CNN(3,5)', 'stream1_kernel': 3, 'stream2_first_kernel': 5},
        {'name': 'MS-CNN(3,3)', 'stream1_kernel': 3, 'stream2_first_kernel': 3},
    ]
    
    BATCH_SIZES = [32, 64, 128]
    
    def __init__(self, base_path, composite_weights=None,
                 use_focal_loss=False, focal_alpha=None, focal_gamma=2.0, adjusted_lr=None):
        self.base_path = base_path
        self.file_manager = ModelFileManager()
        self.composite_weights = composite_weights
        self.visualizer = TrainingVisualizer()
        self.use_focal_loss = use_focal_loss
        self.focal_alpha = focal_alpha
        self.focal_gamma = focal_gamma
        self.adjusted_lr = adjusted_lr or LEARNING_RATE
    
    def search(self, num_epochs_search=20):
        """æ‰§è¡Œè¶…å‚æ•°æœç´¢ï¼Œä½¿ç”¨ç»¼åˆè¯„åˆ†è¯„ä¼°é…ç½®"""
        print("=" * 80)
        print("è¶…å‚æ•°æœç´¢æ¨¡å¼ï¼ˆä½¿ç”¨ç»¼åˆè¯„åˆ†å’Œæ—©åœæœºåˆ¶ï¼‰")
        print("=" * 80)
        
        # åˆ›å»ºæœç´¢ç›®å½•
        search_dir = self.file_manager.create_experiment_dir(
            "/home/xusi/EE5046_Projects/Task1_Results/HyperparamSearch",
            "HyperparamSearch"
        )
        
        # ä¿å­˜æœç´¢é…ç½®
        search_config = {
            'kernel_configs': self.KERNEL_CONFIGS,
            'batch_sizes': self.BATCH_SIZES,
            'num_epochs': num_epochs_search,
            'learning_rate': LEARNING_RATE,
            'use_stream2': USE_STREAM2_SETTING,
            'augment': AUGMENT_SETTING,
            'composite_weights': self.composite_weights,
            'search_time': datetime.now().isoformat()
        }
        
        config_path = os.path.join(search_dir, "configs", "search_config.json")
        self.file_manager.save_config(search_config, config_path)
        
        best_composite_score = 0
        best_config = None
        all_results = {}
        
        # è®¡ç®—æ€»é…ç½®æ•°
        total_configs = len(self.KERNEL_CONFIGS) * len(self.BATCH_SIZES)
        
        # åˆ›å»ºæ€»ä½“æœç´¢è¿›åº¦æ¡
        config_pbar = tqdm(
            total=total_configs, 
            desc="è¶…å‚æ•°æœç´¢è¿›åº¦",
            bar_format='{desc}: {percentage:3.0f}%|{bar:20}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'
        )
        config_idx = 0

        # éå†æ‰€æœ‰è¶…å‚æ•°ç»„åˆ
        for kernel_config in self.KERNEL_CONFIGS:
            for batch_size in self.BATCH_SIZES:
                config_name = f"{kernel_config['name']}_BS{batch_size}"
                print(f"\n{'='*50}")
                print(f"æµ‹è¯•é…ç½®: {config_name}")
                print(f"{'='*50}")
                
                # ä¸ºæ¯ä¸ªé…ç½®åˆ›å»ºå­ç›®å½•
                config_dir = os.path.join(search_dir, "configs", config_name)
                subdirs = [
                    "models",           # ä¿å­˜æ¨¡å‹æ–‡ä»¶
                    "logs",            # è®­ç»ƒæ—¥å¿—
                    "configs",         # é…ç½®æ–‡ä»¶
                    "metrics",         # æ€§èƒ½æŒ‡æ ‡
                    "visualizations"   # å¯è§†åŒ–å›¾è¡¨
                ]
                for subdir in subdirs:
                    os.makedirs(os.path.join(config_dir, subdir), exist_ok=True)
                
                # åˆ›å»ºè®­ç»ƒå™¨
                trainer = ModelTrainer(
                    base_path=self.base_path,
                    kernel_config=kernel_config,
                    batch_size=batch_size,
                    lr=self.adjusted_lr,  # ä½¿ç”¨è°ƒæ•´åçš„å­¦ä¹ ç‡
                    use_stream2=USE_STREAM2_SETTING,
                    augment=AUGMENT_SETTING,
                    experiment_dir=config_dir,
                    config_name=config_name,
                    composite_weights=self.composite_weights,
                    use_focal_loss=self.use_focal_loss,  # æ–°å¢
                    focal_alpha=self.focal_alpha,        # æ–°å¢
                    focal_gamma=self.focal_gamma         # æ–°å¢
                )
                
                # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯
                train_indices = [0, 1, 2, 3]
                avg_metrics, fold_results = trainer.cross_validate_on_train_set(
                    train_indices, num_epochs_search, k_folds=5
                )

                avg_composite_score = avg_metrics.get('avg_composite_score', 0)
                
                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                config_pbar.set_postfix({
                    'config': config_name,
                    'score': f"{avg_composite_score:.4f}",
                    'best': 'â˜…' if avg_composite_score > best_composite_score else ''
                })

                
                # ä¿å­˜é…ç½®ç»“æœ
                config_result = {
                    'config_name': config_name,
                    'kernel_config': kernel_config,
                    'batch_size': batch_size,
                    'avg_accuracy': avg_metrics['avg_val_acc'],
                    'avg_auc': avg_metrics['avg_val_auc'],
                    'avg_f1': avg_metrics['avg_val_f1'],
                    'avg_composite_score': avg_composite_score,
                    'std_accuracy': avg_metrics['std_val_acc'],
                    'std_auc': avg_metrics['std_val_auc'],
                    'std_f1': avg_metrics['std_val_f1'],
                    'fold_results': fold_results,
                    'directory': config_dir
                }
                
                result_path = os.path.join(config_dir, f"{config_name}_results.json")
                self.file_manager.save_metrics(config_result, result_path)
                
                all_results[config_name] = config_result
                
                # æ›´æ–°æœ€ä½³é…ç½®ï¼ˆåŸºäºç»¼åˆè¯„åˆ†ï¼‰
                if avg_composite_score > best_composite_score + MIN_DELTA:
                    best_composite_score = avg_composite_score
                    best_config = config_result
                    print(f"  ğŸ¯ æ–°çš„æœ€ä½³é…ç½®!")

                # æ›´æ–°è¿›åº¦æ¡
                config_pbar.update(1)

        config_pbar.close()
        
        # ä¿å­˜æ‰€æœ‰ç»“æœå’Œæœ€ä½³é…ç½®
        self._save_search_results(all_results, best_config, search_dir)

        # å¯è§†åŒ–æœç´¢ç»“æœ
        self._visualize_search_results(all_results, search_dir)
        
        return best_config, search_dir
    
    def _save_search_results(self, all_results, best_config, search_dir):
        """ä¿å­˜æœç´¢ç»“æœ"""
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        all_results_path = os.path.join(search_dir, "metrics", "all_search_results.json")
        self.file_manager.save_metrics(all_results, all_results_path)
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        sorted_configs = sorted(
            all_results.items(),
            key=lambda x: x[1]['avg_composite_score'],
            reverse=True
        )
        
        # åˆ›å»ºæ’åæŠ¥å‘Š
        ranking_report = {
            'search_timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'total_configs_tested': len(all_results),
            'ranking': []
        }
        
        print(f"\n{'='*80}")
        print("è¶…å‚æ•°æœç´¢æ’åç»“æœï¼ˆæŒ‰ç»¼åˆè¯„åˆ†ï¼‰:")
        print(f"{'='*80}")
        
        for rank, (config_name, data) in enumerate(sorted_configs, 1):
            marker = " ğŸ†" if rank == 1 else ""
            print(f"{rank:2d}. {config_name:25s} "
                  f"ç»¼åˆè¯„åˆ†: {data['avg_composite_score']:.4f} | "
                  f"å‡†ç¡®ç‡: {data['avg_accuracy']:.4f} | "
                  f"AUC: {data['avg_auc']:.4f} | "
                  f"F1: {data['avg_f1']:.4f}{marker}")
            
            ranking_report['ranking'].append({
                'rank': rank,
                'config_name': config_name,
                'avg_composite_score': data['avg_composite_score'],
                'avg_accuracy': data['avg_accuracy'],
                'avg_auc': data['avg_auc'],
                'avg_f1': data['avg_f1']
            })
        
        # ä¿å­˜æœ€ä½³é…ç½®
        best_config_data = {
            'best_config_name': best_config['config_name'],
            'best_config_data': best_config,
            'search_timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'selection_criteria': 'composite_score',
            'composite_weights': self.composite_weights
        }
        
        best_config_path = os.path.join(search_dir, "configs", "best_config.json")
        self.file_manager.save_config(best_config_data, best_config_path)
        
        # åŒæ—¶ä¿å­˜åˆ°æ ‡å‡†ä½ç½®ä»¥ä¾¿å®Œæ•´è®­ç»ƒä½¿ç”¨
        latest_best_path = "/home/xusi/EE5046_Projects/Task1_Results/HyperparamSearch/latest_best_config.json"
        self.file_manager.save_config(best_config_data, latest_best_path)
        
        # ä¿å­˜æ’åæŠ¥å‘Š
        ranking_path = os.path.join(search_dir, "metrics", "ranking_report.json")
        self.file_manager.save_metrics(ranking_report, ranking_path)
        
        print(f"\n{'='*80}")
        print("è¶…å‚æ•°æœç´¢å®Œæˆ!")
        print(f"{'='*80}")
        print(f"æœ€ä½³é…ç½®: {best_config['config_name']}")
        print(f"ç»¼åˆè¯„åˆ†: {best_config['avg_composite_score']:.4f}")
        print(f"å¹³å‡å‡†ç¡®ç‡: {best_config['avg_accuracy']:.4f}")
        print(f"å¹³å‡AUC: {best_config['avg_auc']:.4f}")
        print(f"å¹³å‡F1åˆ†æ•°: {best_config['avg_f1']:.4f}")
        print(f"ç»“æœç›®å½•: {search_dir}")
        print(f"æœ€ä½³é…ç½®å·²ä¿å­˜: {latest_best_path}")


    def _visualize_search_results(self, all_results, search_dir):
        """å¯è§†åŒ–è¶…å‚æ•°æœç´¢ç»“æœ"""
        if not all_results:
            return
        
        # æå–é…ç½®åç§°å’ŒæŒ‡æ ‡
        config_names = []
        composite_scores = []
        accuracies = []
        aucs = []
        f1s = []
        
        for config_name, data in all_results.items():
            config_names.append(config_name)
            composite_scores.append(data['avg_composite_score'])
            accuracies.append(data['avg_accuracy'])
            aucs.append(data['avg_auc'])
            f1s.append(data['avg_f1'])
        
        # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # ç»¼åˆè¯„åˆ†æ’å
        sorted_indices = np.argsort(composite_scores)[::-1]
        sorted_names = [config_names[i] for i in sorted_indices]
        sorted_scores = [composite_scores[i] for i in sorted_indices]
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(sorted_scores)))
        bars1 = axes[0, 0].bar(range(len(sorted_scores)), sorted_scores, color=colors)
        axes[0, 0].set_title('è¶…å‚æ•°é…ç½®ç»¼åˆè¯„åˆ†æ’å', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('é…ç½®æ’å', fontsize=12)
        axes[0, 0].set_ylabel('ç»¼åˆè¯„åˆ†', fontsize=12)
        axes[0, 0].set_xticks(range(len(sorted_scores)))
        axes[0, 0].set_xticklabels([f'#{i+1}' for i in range(len(sorted_scores))], rotation=45)
        axes[0, 0].grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars1, sorted_scores):
            height = bar.get_height()
            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                          f'{score:.3f}', ha='center', va='bottom', fontsize=9)
        
        # å‡†ç¡®ç‡å¯¹æ¯”
        axes[0, 1].bar(range(len(config_names)), accuracies, color='skyblue', edgecolor='black')
        axes[0, 1].set_title('å„é…ç½®å¹³å‡å‡†ç¡®ç‡', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('é…ç½®', fontsize=12)
        axes[0, 1].set_ylabel('å‡†ç¡®ç‡', fontsize=12)
        axes[0, 1].set_xticks(range(len(config_names)))
        axes[0, 1].set_xticklabels(config_names, rotation=45, ha='right')
        axes[0, 1].grid(True, alpha=0.3, axis='y')
        
        # AUCå¯¹æ¯”
        axes[1, 0].bar(range(len(config_names)), aucs, color='lightgreen', edgecolor='black')
        axes[1, 0].set_title('å„é…ç½®å¹³å‡AUC', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('é…ç½®', fontsize=12)
        axes[1, 0].set_ylabel('AUC', fontsize=12)
        axes[1, 0].set_xticks(range(len(config_names)))
        axes[1, 0].set_xticklabels(config_names, rotation=45, ha='right')
        axes[1, 0].grid(True, alpha=0.3, axis='y')
        
        # F1åˆ†æ•°å¯¹æ¯”
        axes[1, 1].bar(range(len(config_names)), f1s, color='lightcoral', edgecolor='black')
        axes[1, 1].set_title('å„é…ç½®å¹³å‡F1åˆ†æ•°', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('é…ç½®', fontsize=12)
        axes[1, 1].set_ylabel('F1åˆ†æ•°', fontsize=12)
        axes[1, 1].set_xticks(range(len(config_names)))
        axes[1, 1].set_xticklabels(config_names, rotation=45, ha='right')
        axes[1, 1].grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        vis_path = os.path.join(search_dir, "visualizations", "search_results_comparison.png")
        plt.savefig(vis_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š è¶…å‚æ•°æœç´¢ç»“æœå¯è§†åŒ–å·²ä¿å­˜: {vis_path}")


# ==================== å®Œæ•´è®­ç»ƒæ¨¡å—ï¼ˆä¿®æ”¹ç‰ˆï¼‰ ====================
class CompleteTrainer:
    """ä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ"""
    
    @staticmethod
    def train_with_best_config(base_path, best_config_data, lr_scheduler_config=None,
                               use_focal_loss=False, focal_alpha=None, focal_gamma=2.0, 
                               adjusted_lr=None):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹ï¼š
        1. åœ¨è®­ç»ƒé›†ï¼ˆCV0~CV3ï¼‰ä¸Šä½¿ç”¨æœ€ä½³è¶…å‚æ•°è¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯
        2. ä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆåŒ…å«éªŒè¯é›†å’Œæ—©åœï¼‰
        3. åœ¨æµ‹è¯•é›†ï¼ˆCV4ï¼‰ä¸Šæœ€ç»ˆè¯„ä¼°
        
        Args:
            base_path: æ•°æ®é›†è·¯å¾„
            best_config_data: æœ€ä½³é…ç½®æ•°æ®
            lr_scheduler_config: å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
        """
        print("=" * 80)
        print("å®Œæ•´è®­ç»ƒæ¨¡å¼ï¼ˆä½¿ç”¨ç»¼åˆè¯„åˆ†å’Œæ—©åœæœºåˆ¶ï¼‰")
        print("=" * 80)

        if adjusted_lr is None:
            adjusted_lr = LEARNING_RATE
        
        # åˆ›å»ºå®éªŒç›®å½•
        file_manager = ModelFileManager()
        experiment_dir = file_manager.create_experiment_dir(
            "/home/xusi/EE5046_Projects/Task1_Results/CompleteTraining",
            "CompleteTraining",
            f"{best_config_data['kernel_config']['name']}_BS{best_config_data['batch_size']}"
        )
        
        # 1. åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯
        print("\næ­¥éª¤1: åœ¨è®­ç»ƒé›†ï¼ˆCV0~CV3ï¼‰ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯")
        
        # ä½¿ç”¨æœ€ä½³é…ç½®ä¸­çš„ç»¼åˆè¯„åˆ†æƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
        composite_weights = best_config_data.get('composite_weights')

        # å¦‚æœæœªæä¾›è°ƒåº¦å™¨é…ç½®ï¼Œä½¿ç”¨ Config.py ä¸­çš„é»˜è®¤é…ç½®
        if lr_scheduler_config is None:
            lr_scheduler_config = LR_SCHEDULER_CONFIG
        
        trainer = ModelTrainer(
            base_path=base_path,
            kernel_config=best_config_data['kernel_config'],
            batch_size=best_config_data['batch_size'],
            lr=adjusted_lr,  # ä½¿ç”¨è°ƒæ•´åçš„å­¦ä¹ ç‡
            use_stream2=USE_STREAM2_SETTING,
            augment=AUGMENT_SETTING,
            experiment_dir=experiment_dir,
            config_name="BestConfig",
            composite_weights=composite_weights,
            lr_scheduler_config=lr_scheduler_config,
            use_focal_loss=use_focal_loss,  # æ–°å¢
            focal_alpha=focal_alpha,        # æ–°å¢
            focal_gamma=focal_gamma         # æ–°å¢
        )
        
        train_indices = [0, 1, 2, 3]
        cv_metrics, cv_results = trainer.cross_validate_on_train_set(
            train_indices, NUM_EPOCHS, k_folds=5
        )
        
        print(f"\n5æŠ˜äº¤å‰éªŒè¯ç»“æœ:")
        print(f"å¹³å‡å‡†ç¡®ç‡: {cv_metrics['avg_val_acc']:.4f} Â± {cv_metrics['std_val_acc']:.4f}")
        print(f"å¹³å‡AUC: {cv_metrics['avg_val_auc']:.4f} Â± {cv_metrics['std_val_auc']:.4f}")
        print(f"å¹³å‡F1åˆ†æ•°: {cv_metrics['avg_val_f1']:.4f} Â± {cv_metrics['std_val_f1']:.4f}")
        print(f"å¹³å‡ç»¼åˆè¯„åˆ†: {cv_metrics.get('avg_composite_score', 0):.4f}")
        
        # 2. ä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆåŒ…å«éªŒè¯é›†å’Œæ—©åœï¼‰
        print("\næ­¥éª¤2: ä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†ï¼ˆCV0~CV3ï¼‰è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆåŒ…å«éªŒè¯é›†ï¼‰")
        final_model, train_metrics = trainer.train_final_model(train_indices, NUM_EPOCHS)
        
        # 3. åœ¨æµ‹è¯•é›†ï¼ˆCV4ï¼‰ä¸Šè¯„ä¼°
        print("\næ­¥éª¤3: åœ¨æµ‹è¯•é›†ï¼ˆCV4ï¼‰ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹")
        test_results = trainer.evaluate_on_test_set([4], final_model,use_two_stage=True,
                                                    optimize_threshold=True,  # ä¸ä½¿ç”¨è‡ªåŠ¨ä¼˜åŒ–ï¼Œä½¿ç”¨æˆ‘ä»¬æŒ‡å®šçš„
                                                    stage1_threshold=0.3,  # æé«˜ç¬¬ä¸€é˜¶æ®µé—¨æ§›
                                                    stage2_threshold=0.5   # æé«˜ç¬¬äºŒé˜¶æ®µé—¨æ§›ä»¥æé«˜ç²¾ç¡®ç‡
                                                    )
        
        print(f"\næœ€ç»ˆæµ‹è¯•ç»“æœ:")
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_results['test_acc']:.4f}")
        print(f"æµ‹è¯•é›†AUC: {test_results['test_auc']:.4f}")
        print(f"æµ‹è¯•é›†F1åˆ†æ•°: {test_results['test_f1']:.4f}")
        print(f"æµ‹è¯•é›†ç»¼åˆè¯„åˆ†: {test_results['test_composite_score']:.4f}")
        
        # ä¿å­˜å®Œæ•´è®­ç»ƒæ‘˜è¦
        summary = {
            'cross_validation': cv_metrics,
            'final_training': train_metrics,
            'test_evaluation': test_results,
            'best_config': best_config_data,
            'complete_training_time': datetime.now().isoformat(),
            'early_stop_patience': EARLY_STOP_PATIENCE,
            'min_delta': MIN_DELTA,
            'composite_weights': composite_weights,
            'lr_scheduler_config':LR_SCHEDULER_CONFIG  # ä¿å­˜è°ƒåº¦å™¨é…ç½®
        }
        
        summary_path = os.path.join(experiment_dir, "metrics", "complete_training_summary.json")
        file_manager.save_metrics(summary, summary_path)
        
        return cv_metrics, test_results, experiment_dir


# ==================== ä¸»ç¨‹åºæ¨¡å—ï¼ˆä¿®æ”¹ç‰ˆï¼‰ ====================
class TrainingPipeline:
    """è®­ç»ƒç®¡é“ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        # æ•°æ®é›†è·¯å¾„
        self.base_path = "/home/xusi/EE5046_Projects/Dataset"
        self.loss_config = get_loss_config()
        self.use_focal_loss = USE_FOCAL_LOSS
        self.focal_alpha = self.loss_config.get('focal_alpha')
        self.focal_gamma = self.loss_config.get('focal_gamma')
        self.adjusted_lr = self.loss_config.get('adjusted_lr', LEARNING_RATE)

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.base_path):
            print(f"âŒ Datasetç›®å½•ä¸å­˜åœ¨: {self.base_path}")
            print("è¯·ç¡®ä¿Datasetç›®å½•åœ¨æŒ‡å®šä½ç½®")
            sys.exit(1)
        
        # æ£€æŸ¥cvç›®å½•
        cv_dir = os.path.join(self.base_path, "cv")
        if not os.path.exists(cv_dir):
            print(f"âŒ cvç›®å½•ä¸å­˜åœ¨: {cv_dir}")
            print("è¯·ç¡®ä¿Datasetç›®å½•åŒ…å«cvå­ç›®å½•")
            sys.exit(1)
        
        # æ£€æŸ¥training2017ç›®å½•
        training_dir = os.path.join(self.base_path, "training2017")
        if not os.path.exists(training_dir):
            print(f"âŒ training2017ç›®å½•ä¸å­˜åœ¨: {training_dir}")
            print("è¯·ç¡®ä¿Datasetç›®å½•åŒ…å«training2017å­ç›®å½•")
            sys.exit(1)
        
        print(f"âœ… ä½¿ç”¨æ•°æ®é›†ç›®å½•: {self.base_path}")
        print(f"âœ… cvç›®å½•: {cv_dir}")
        print(f"âœ… training2017ç›®å½•: {training_dir}")
        
        # æ£€æŸ¥CSVæ–‡ä»¶
        for i in range(5):
            csv_file = os.path.join(cv_dir, f"cv{i}.csv")
            if os.path.exists(csv_file):
                print(f"âœ… æ‰¾åˆ°æ–‡ä»¶: cv{i}.csv")
            else:
                print(f"âš ï¸ è­¦å‘Š: æ–‡ä»¶ cv{i}.csv ä¸å­˜åœ¨")

        self.train_indices = [0, 1, 2, 3]
        self.test_indices = [4]
        
        # è°ƒæ•´åçš„ç»¼åˆè¯„åˆ†æƒé‡ - æ›´é‡è§†F1åˆ†æ•°
        self.custom_weights = {
            'accuracy': 0.30,  # é™ä½å‡†ç¡®ç‡æƒé‡
            'auc': 0.30,       # AUCæƒé‡ä¿æŒä¸å˜
            'f1': 0.35,        # æé«˜F1åˆ†æ•°æƒé‡
            'stability': 0.05  # ç¨³å®šæ€§æƒé‡
        }

        # åˆå§‹åŒ–è®­ç»ƒå™¨é…ç½®ç¼“å­˜ - è¿™æ˜¯å…³é”®ä¿®å¤ï¼
        self.trainer_config = None
        
        # æ‰“å°åˆå§‹åŒ–å®Œæˆä¿¡æ¯
        print(f"âœ… TrainingPipeline åˆå§‹åŒ–å®Œæˆ")
        print(f"  è®­ç»ƒé›†: CV{', '.join(map(str, self.train_indices))}")
        print(f"  æµ‹è¯•é›†: CV{', '.join(map(str, self.test_indices))}")

    def _create_trainer(self, **kwargs):
        """åˆ›å»ºè®­ç»ƒå™¨çš„ç»Ÿä¸€æ–¹æ³•"""
        default_kwargs = {
            'base_path': self.base_path,
            'use_focal_loss': self.use_focal_loss,
            'focal_alpha': self.focal_alpha,
            'focal_gamma': self.focal_gamma,
            'lr': self.adjusted_lr,  # ä½¿ç”¨è°ƒæ•´åçš„å­¦ä¹ ç‡
        }
        default_kwargs.update(kwargs)
        return ModelTrainer(**default_kwargs)
    
    def run(self):
        """è¿è¡Œè®­ç»ƒç®¡é“"""
        parser = argparse.ArgumentParser(description='ECGæˆ¿é¢¤æ£€æµ‹è®­ç»ƒè„šæœ¬ï¼ˆæ”¹è¿›ç‰ˆï¼‰')
        parser.add_argument('--mode', type=str, default=EXPERIMENT_MODE,
                            choices=['search', 'train', 'full', 'compare'],
                            help='è¿è¡Œæ¨¡å¼')
        parser.add_argument('--compare_mode', type=str, default=COMPARISON_MODE,
                            choices=['stream', 'augment'],
                            help='å¯¹æ¯”å®éªŒæ¨¡å¼')
        parser.add_argument('--weights', type=str, default='default',
                            choices=['default', 'accuracy_focus', 'balanced', 'auc_focus'],
                            help='ç»¼åˆè¯„åˆ†æƒé‡ç­–ç•¥')

        parser.add_argument('--loss', type=str, default='focal',
                        choices=['bce', 'focal'],
                        help='æŸå¤±å‡½æ•°ç±»å‹: bceæˆ–focal')
        parser.add_argument('--focal_alpha', type=float, default=None,
                            help='Focal Lossçš„alphaå‚æ•° (0-1)ã€‚é»˜è®¤Noneæ—¶ä¼šè‡ªåŠ¨è®¡ç®—')
        parser.add_argument('--focal_gamma', type=float, default=2.0,
                            help='Focal Lossçš„gammaå‚æ•° (é»˜è®¤2.0)')
        parser.add_argument('--focal_config', type=str, default='focus_positive',
                            choices=['default', 'balanced', 'focus_positive', 'focus_hard'],
                            help='é¢„è®¾çš„Focal Lossé…ç½®')
        parser.add_argument('--lr', type=float, default=LEARNING_RATE,
                            help='åˆå§‹å­¦ä¹ ç‡')
        
        parser.add_argument('--no_progress', action='store_true',
                            help='ç¦ç”¨è¿›åº¦æ¡æ˜¾ç¤º')
        
        args = parser.parse_args()

        if args.no_progress:
            from tqdm import tqdm
            tqdm.__init__ = lambda self, *args, **kwargs: None
        
        print(f"\næ•°æ®é›†åˆ’åˆ†ç­–ç•¥:")
        print(f"è®­ç»ƒé›†: CV{', '.join(map(str, self.train_indices))}")
        print(f"æµ‹è¯•é›†: CV{', '.join(map(str, self.test_indices))}")
        print(f"æ—©åœè€å¿ƒå€¼: {EARLY_STOP_PATIENCE}, æœ€å°æå‡: {MIN_DELTA}")
        
        # æ ¹æ®æƒé‡ç­–ç•¥é€‰æ‹©æƒé‡
        if args.weights == 'accuracy_focus':
            weights = {'accuracy': 0.50, 'auc': 0.30, 'f1': 0.15, 'stability': 0.05}
        elif args.weights == 'auc_focus':
            weights = {'accuracy': 0.30, 'auc': 0.50, 'f1': 0.15, 'stability': 0.05}
        elif args.weights == 'balanced':
            weights = {'accuracy': 0.35, 'auc': 0.35, 'f1': 0.25, 'stability': 0.05}
        else:
            weights = None  # ä½¿ç”¨é»˜è®¤æƒé‡
        
        if weights:
            print(f"ç»¼åˆè¯„åˆ†æƒé‡: {weights}")
        
        if args.mode == 'search':
            self._run_search_mode(args,weights)
        elif args.mode == 'train':
            self._run_train_mode(args,weights)
        elif args.mode == 'full':
            self._run_data_diagnostic()
            self._run_full_mode(args, weights)
        elif args.mode == 'compare':
            self._run_compare_mode(args, weights)

    def _setup_focal_loss_config(self, args):
        """è®¾ç½®Focal Lossé…ç½® - ç»Ÿä¸€ç‰ˆæœ¬"""
        # ä½¿ç”¨Config.pyä¸­çš„é»˜è®¤é…ç½®
        default_config = LOSS_FUNCTION_CONFIG.copy()
        
        # å¦‚æœæœ‰é¢„è®¾é…ç½®åç§°ï¼Œä½¿ç”¨é¢„è®¾é…ç½®
        if args.focal_config in FOCAL_PRESET_CONFIGS:
            preset = FOCAL_PRESET_CONFIGS[args.focal_config]
            
            # ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é¢„è®¾é…ç½®
            focal_alpha = args.focal_alpha or preset.get('alpha', default_config.get('focal_alpha', 0.25))
            focal_gamma = args.focal_gamma or preset.get('gamma', default_config.get('focal_gamma', 2.0))
            lr_factor = preset.get('lr_factor', 0.5)
            
            config_source = f"é¢„è®¾é…ç½®: {args.focal_config}"
        else:
            # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æˆ–Config.pyä¸­çš„é»˜è®¤å€¼
            focal_alpha = args.focal_alpha or default_config.get('focal_alpha', 0.25)
            focal_gamma = args.focal_gamma or default_config.get('focal_gamma', 2.0)
            lr_factor = default_config.get('lr_factor', 0.5)
            
            config_source = f"å‘½ä»¤è¡Œå‚æ•° + Config.pyé»˜è®¤å€¼"
        
        # è®¡ç®—è°ƒæ•´åçš„å­¦ä¹ ç‡
        adjusted_lr = args.lr * lr_factor
        
        print(f"\nğŸ¯ Focal Lossé…ç½® ({config_source}):")
        print(f"  Alpha: {focal_alpha}")
        print(f"  Gamma: {focal_gamma}")
        print(f"  å­¦ä¹ ç‡è°ƒæ•´å› å­: {lr_factor}")
        print(f"  è°ƒæ•´åå­¦ä¹ ç‡: {adjusted_lr:.6f}")
        
        return focal_alpha, focal_gamma, adjusted_lr
    
    def _get_trainer_config(self, args):
        """è·å–ç»Ÿä¸€çš„è®­ç»ƒå™¨é…ç½®"""
        if self.trainer_config is None:
            # è®¡ç®—Focal Losså‚æ•°
            if args.loss == 'focal':
                focal_alpha, focal_gamma, adjusted_lr = self._setup_focal_loss_config(args)
                use_focal_loss = True
            else:
                focal_alpha, focal_gamma, adjusted_lr = None, None, args.lr
                use_focal_loss = False
            
            # ç¼“å­˜é…ç½®
            self.trainer_config = {
                'use_focal_loss': use_focal_loss,
                'focal_alpha': focal_alpha,
                'focal_gamma': focal_gamma,
                'adjusted_lr': adjusted_lr
            }
            
            print(f"\nğŸ¯ ç»Ÿä¸€è®­ç»ƒå™¨é…ç½®:")
            print(f"  ä½¿ç”¨Focal Loss: {self.trainer_config['use_focal_loss']}")
            if self.trainer_config['use_focal_loss']:
                print(f"  Focal Alpha: {self.trainer_config['focal_alpha']}")
                print(f"  Focal Gamma: {self.trainer_config['focal_gamma']}")
            print(f"  å­¦ä¹ ç‡: {self.trainer_config['adjusted_lr']:.6f}")
        
        return self.trainer_config
    
    def _run_search_mode(self,args,weights):
        """è¿è¡Œè¶…å‚æ•°æœç´¢æ¨¡å¼"""
        print("\næ¨¡å¼: è¶…å‚æ•°æœç´¢ï¼ˆä½¿ç”¨ç»¼åˆè¯„åˆ†ï¼‰")
        # è·å–ç»Ÿä¸€çš„è®­ç»ƒå™¨é…ç½®
        trainer_config = self._get_trainer_config(args)
        
        searcher = HyperparameterSearcher(
            self.base_path, 
            composite_weights=weights,
            use_focal_loss=trainer_config['use_focal_loss'],
            focal_alpha=trainer_config['focal_alpha'],
            focal_gamma=trainer_config['focal_gamma'],
            adjusted_lr=trainer_config['adjusted_lr']
        )
        best_config, search_dir = searcher.search(num_epochs_search=30)
        
        print(f"\nè¶…å‚æ•°æœç´¢å®Œæˆ!")
        print(f"æœ€ä½³é…ç½®: {best_config['config_name']}")
        print(f"ç»¼åˆè¯„åˆ†: {best_config['avg_composite_score']:.4f}")
        print(f"ç»“æœç›®å½•: {search_dir}")
    
    def _run_train_mode(self,args,weights):
        """è¿è¡Œé»˜è®¤è®­ç»ƒæ¨¡å¼"""
        print("\næ¨¡å¼: é»˜è®¤é…ç½®è®­ç»ƒï¼ˆä½¿ç”¨ç»¼åˆè¯„åˆ†å’Œæ—©åœï¼‰")
        # è·å–ç»Ÿä¸€çš„è®­ç»ƒå™¨é…ç½®
        trainer_config = self._get_trainer_config(args)
        
        # åˆ›å»ºå®éªŒç›®å½•
        file_manager = ModelFileManager()
        experiment_dir = file_manager.create_experiment_dir(
            "/home/xusi/EE5046_Projects/Task1_Results/DefaultTraining",
            "DefaultTraining",
            f"{DEFAULT_KERNEL_CONFIG['name']}_BS{BATCH_SIZE}"
        )
        
        trainer = ModelTrainer(
            base_path=self.base_path,
            kernel_config=DEFAULT_KERNEL_CONFIG,
            batch_size=BATCH_SIZE,
            lr=trainer_config['adjusted_lr'],  # ä½¿ç”¨ç»Ÿä¸€çš„å­¦ä¹ ç‡
            use_stream2=USE_STREAM2_SETTING,
            augment=AUGMENT_SETTING,
            experiment_dir=experiment_dir,
            config_name="DefaultConfig",
            composite_weights=weights,
            use_focal_loss=trainer_config['use_focal_loss'],
            focal_alpha=trainer_config['focal_alpha'],
            focal_gamma=trainer_config['focal_gamma']
    )
        
        # åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯
        cv_metrics, cv_results = trainer.cross_validate_on_train_set(
            self.train_indices, NUM_EPOCHS, k_folds=5
        )
        
        print(f"\né»˜è®¤è®­ç»ƒå®Œæˆ!")
        print(f"å¹³å‡ç»¼åˆè¯„åˆ†: {cv_metrics.get('avg_composite_score', 0):.4f}")
        print(f"ç»“æœç›®å½•: {experiment_dir}")

    def _run_data_diagnostic(self):
        """è¿è¡Œæ•°æ®è¯Šæ–­"""
        print("\n=== æ•°æ®è¯Šæ–­æ¨¡å¼ ===")
        
        # åˆ›å»ºä¸´æ—¶è®­ç»ƒå™¨
        trainer = ModelTrainer(
            base_path=self.base_path,
            kernel_config=DEFAULT_KERNEL_CONFIG,
            batch_size=32,
            lr=0.001,
            use_stream2=True,
            augment=True,
            experiment_dir=None,
            config_name="Diagnostic"
        )
        
        # è¿è¡ŒåŸºæœ¬åŠŸèƒ½æµ‹è¯•
        trainer.test_basic_functionality()
        
        # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
        print("\n=== æ£€æŸ¥CVæ–‡ä»¶åˆ†å¸ƒ ===")
        for cv_idx in range(5):
            data = trainer.data_manager.load_cv_files([cv_idx])
            if len(data) > 0:
                labels = [label for _, label in data[:1000]]  # åªæ£€æŸ¥å‰1000ä¸ª
                labels_np = np.array(labels)
                print(f"CV{cv_idx}: æ ·æœ¬æ•°={len(data)}, æ­£æ ·æœ¬æ¯”ä¾‹={np.mean(labels_np):.2%}")
    
    def _run_full_mode(self, args, weights):
        """è¿è¡Œå®Œæ•´è®­ç»ƒæ¨¡å¼"""
        print("\næ¨¡å¼: ä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ")
        
        best_config_path = "/home/xusi/EE5046_Projects/Task1_Results/HyperparamSearch/latest_best_config.json"
        
        if os.path.exists(best_config_path):
            with open(best_config_path, 'r') as f:
                best_config_data = json.load(f)
            
            print(f"åŠ è½½æœ€ä½³é…ç½®: {best_config_data['best_config_name']}")
            print(f"é€‰æ‹©æ ‡å‡†: {best_config_data.get('selection_criteria', 'accuracy')}")

            # è·å–ç»Ÿä¸€çš„è®­ç»ƒå™¨é…ç½®
            trainer_config = self._get_trainer_config(args)
            
            # ä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒï¼Œå¹¶ä¼ é€’å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
            cv_metrics, test_results, experiment_dir = CompleteTrainer.train_with_best_config(
                self.base_path, 
                best_config_data['best_config_data'],
                lr_scheduler_config=LR_SCHEDULER_CONFIG,
                use_focal_loss=trainer_config['use_focal_loss'],
                focal_alpha=trainer_config['focal_alpha'],
                focal_gamma=trainer_config['focal_gamma'],
                adjusted_lr=trainer_config['adjusted_lr']
        )
            
            print(f"\nå®Œæ•´è®­ç»ƒå®Œæˆ!")
            print(f"äº¤å‰éªŒè¯å¹³å‡ç»¼åˆè¯„åˆ†: {cv_metrics.get('avg_composite_score', 0):.4f}")
            print(f"æµ‹è¯•é›†ç»¼åˆè¯„åˆ†: {test_results['test_composite_score']:.4f}")
            print(f"ç»“æœç›®å½•: {experiment_dir}")
        else:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æœ€ä½³é…ç½®æ–‡ä»¶ {best_config_path}")
            print("è¯·å…ˆè¿è¡Œè¶…å‚æ•°æœç´¢æ¨¡å¼: python TrainProcess.py --mode search")

    def _run_compare_mode(self, args, weights):
        """è¿è¡Œå¯¹æ¯”å®éªŒæ¨¡å¼"""
        print("\næ¨¡å¼: å¯¹æ¯”å®éªŒ")
        print("æ³¨æ„: å¯¹æ¯”å®éªŒæ¨¡å¼æš‚æœªå®ç°ç»¼åˆè¯„åˆ†ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘")
        
        # å¯¹æ¯”å®éªŒæ¨¡å—éœ€è¦ç›¸åº”ä¿®æ”¹ï¼Œè¿™é‡Œæš‚æ—¶è·³è¿‡
        print("å¯¹æ¯”å®éªŒæ¨¡å¼æš‚æœªæ›´æ–°ï¼Œè¯·ä½¿ç”¨åŸæœ‰ç‰ˆæœ¬")
        return


# ==================== ç¨‹åºå…¥å£ ====================
if __name__ == '__main__':
    # è®¾å¤‡è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“å’Œæ ·å¼
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # è¿è¡Œè®­ç»ƒç®¡é“
    try:
        pipeline = TrainingPipeline()
        pipeline.run()
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()